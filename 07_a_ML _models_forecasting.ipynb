{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 486114 entries, (Timestamp('2014-12-10 00:00:00'), 'AA') to (Timestamp('2022-08-02 00:00:00'), 'ZTS')\n",
      "Columns: 622 entries, FEATURE_open to TARGET_ret_fwd_252d_rank_quantiled\n",
      "dtypes: float32(360), float64(43), int32(198), int64(12), int8(9)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from utils import rank_stocks_and_quantile\n",
    "\n",
    "top = 250  # parameters -> papermill\n",
    "\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_keys = [\n",
    "    '/data/YEAR_20200930_20220802',\n",
    "    '/data/YEAR_20181024_20200929',\n",
    "    '/data/YEAR_20161116_20181023',\n",
    "    '/data/YEAR_20141210_20161115'\n",
    "]\n",
    "target = 'TARGET_ret_fwd'  # no longer a parameter\n",
    "\n",
    "# Initialize empty dataset\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    for key in dataset_keys:\n",
    "        df = store[key]\n",
    "        dataset = pd.concat([dataset, df], ignore_index=False)\n",
    "        del df\n",
    "        gc.collect()  # Explicitly call garbage collector\n",
    "\n",
    "# Rank stocks and quantile\n",
    "dataset = rank_stocks_and_quantile(dataset, target_substring=target)\n",
    "\n",
    "# Adjust timezone\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of business days between 2014-12-10 00:00:00 and 2022-08-02 00:00:00: 1995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the first and last dates from the MultiIndex\n",
    "start_date = dataset.index.get_level_values(0).min()\n",
    "end_date = dataset.index.get_level_values(0).max()\n",
    "\n",
    "# Generate business dates between the start and end date\n",
    "business_dates = pd.bdate_range(start_date, end_date)\n",
    "\n",
    "# Count the number of business days\n",
    "num_business_days = len(business_dates)\n",
    "\n",
    "print(f\"Number of business days between {start_date} and {end_date}: {num_business_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fi(model):\n",
    "#     fi = model.feature_importance(importance_type='gain')\n",
    "#     return pd.Series(fi / fi.sum(), index=model.feature_name())\n",
    "\n",
    "# def ic_lgbm(preds, train_data):\n",
    "#     \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "#     return 'ic', spearmanr(preds, train_data.get_label())[0], True\n",
    "\n",
    "\n",
    "# def custom_eval_metrics(preds, train_data):\n",
    "#     from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "#     labels = train_data.get_label()\n",
    "#     mae = mean_absolute_error(labels, preds)\n",
    "#     mse = mean_squared_error(labels, preds)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(labels, preds)\n",
    "#     return [('MAE', mae, False), \n",
    "#             ('MSE', mse, False), \n",
    "#             ('RMSE', rmse, False), \n",
    "#             ('R2', r2, True)]\n",
    "\n",
    "# def combined_eval_metrics(preds, train_data):\n",
    "#     ic_result = ic_lgbm(preds, train_data)\n",
    "#     sharpe_ratio_result = sharpe_ratio_lgbm(preds, train_data)\n",
    "#     custom_metrics_results = custom_eval_metrics(preds, train_data)\n",
    "#     return [ic_result, sharpe_ratio_result] + custom_metrics_results\n",
    "\n",
    "def sharpe_ratio_lgbm(preds, train_data):\n",
    "    \"\"\"Custom Sharpe ratio eval metric for lightgbm.\"\"\"\n",
    "    labels = train_data.get_label()\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    grouped_labels = pd.Series(labels, index=train_data.data.index).groupby(level=0)\n",
    "    grouped_preds = pd.Series(preds, index=train_data.data.index).groupby(level=0)\n",
    "\n",
    "    daily_scores = []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        score_for_day = spearmanr(actuals_for_day, preds_for_day)[0]\n",
    "        daily_scores.append(score_for_day if not np.isnan(score_for_day) else 0)\n",
    "\n",
    "    sharpe_ratio = np.mean(daily_scores) / (np.std(daily_scores) + 1e-9)\n",
    "    return 'sharpe_ratio', sharpe_ratio, True\n",
    "\n",
    "\n",
    "def mean_ic_for_fold(preds, train_data):\n",
    "    \"\"\"Compute average IC for the entire fold.\"\"\"\n",
    "    labels = train_data.get_label()\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    grouped_labels = pd.Series(labels, index=train_data.data.index).groupby(level=0)\n",
    "    grouped_preds = pd.Series(preds, index=train_data.data.index).groupby(level=0)\n",
    "\n",
    "    daily_ic_scores = []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        ic_score_for_day = spearmanr(actuals_for_day, preds_for_day)[0]\n",
    "        daily_ic_scores.append(ic_score_for_day if not np.isnan(ic_score_for_day) else 0)\n",
    "\n",
    "    return 'IC', np.mean(daily_ic_scores), True\n",
    "\n",
    "def mean_custom_metrics_for_fold(preds, train_data):\n",
    "    \"\"\"Compute average MAE, MSE, RMSE, and R^2 for the entire fold.\"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    labels = train_data.get_label()\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    grouped_labels = pd.Series(labels, index=train_data.data.index).groupby(level=0)\n",
    "    grouped_preds = pd.Series(preds, index=train_data.data.index).groupby(level=0)\n",
    "\n",
    "    mae_scores, mse_scores, rmse_scores, r2_scores = [], [], [], []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        mae_scores.append(mean_absolute_error(actuals_for_day, preds_for_day))\n",
    "        mse_scores.append(mean_squared_error(actuals_for_day, preds_for_day))\n",
    "        rmse_scores.append(np.sqrt(mse_scores[-1]))\n",
    "        r2_scores.append(r2_score(actuals_for_day, preds_for_day))\n",
    "\n",
    "    return [('MAE', np.mean(mae_scores), False),\n",
    "            ('MSE', np.mean(mse_scores), False),\n",
    "            ('RMSE', np.mean(rmse_scores), False),\n",
    "            ('R2', np.mean(r2_scores), True)]\n",
    "\n",
    "def combined_fold_metrics(preds, train_data):\n",
    "    ic_result = mean_ic_for_fold(preds, train_data)\n",
    "    sharpe_ratio_result = sharpe_ratio_lgbm(preds, train_data)\n",
    "    custom_metrics_results = mean_custom_metrics_for_fold(preds, train_data)\n",
    "    \n",
    "    return [ic_result, sharpe_ratio_result] + custom_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_fold(era_scores, weights=None):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max()\n",
    "\n",
    "    # Smart Sharpe\n",
    "    smart_sharpe = mean_correlation \\\n",
    "        / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd,\n",
    "        'min_correlation': era_scores.min(),\n",
    "        'max_correlation': era_scores.max(),\n",
    "    })\n",
    "\n",
    "    if weights:\n",
    "        normalized_metrics = (metrics - metrics.min()) / (metrics.max() - metrics.min())\n",
    "        weighted_values = normalized_metrics.multiply(pd.Series(weights))\n",
    "        metrics[\"weighted_score\"] = weighted_values.sum()\n",
    "\n",
    "    _ = gc.collect()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_loss(preds, dataset):\n",
    "    y = dataset.get_label()\n",
    "    mse_gradient = 2 * (preds - y)\n",
    "    corr_gradient = - (np.mean(y) - preds)\n",
    "    \n",
    "    gradient = mse_gradient + corr_gradient\n",
    "\n",
    "    # For simplicity, setting hessian to ones.\n",
    "    hessian = np.ones_like(y)\n",
    "\n",
    "    return gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "def get_categoricals(dataset, threshold):\n",
    "    return [col for col in dataset.columns if \\\n",
    "            dataset[col].nunique() < threshold and \\\n",
    "            dataset[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "def objective(trial, data, features):\n",
    "    # Dynamic categoricals based on the trial's suggested threshold\n",
    "    cat_threshold = trial.suggest_int('cat_threshold', 5, 50)\n",
    "    categoricals = get_categoricals(data, cat_threshold)\n",
    "    # Define the features and label columns\n",
    "    # features = [col for col in dataset.columns if col.startswith('FEATURE_')]\n",
    "    params = {\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': custom_loss,\n",
    "        'verbose': -1,\n",
    "        'metric': 'None',\n",
    "        'device': 'gpu',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        # 'lookahead': trial.suggest_int('lookahead', [1, 5, 21])  # Suggest lookahead as a parameter\n",
    "        'lookahead': trial.suggest_categorical('lookahead', [1, 5, 21])\n",
    "\n",
    "    }\n",
    "\n",
    "    early_stopping = lgb.early_stopping(stopping_rounds=500, verbose=True, first_metric_only=True)\n",
    "    daily_scores_in_fold = []\n",
    "\n",
    "    # Instantiate the CV object\n",
    "    cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length=142, \n",
    "                                            test_period_length=21, \n",
    "                                            lookahead=1,  # Starting value; we'll adjust it next.\n",
    "                                            date_idx='date')\n",
    "\n",
    "    # Update the CV's lookahead based on the trial's suggestion\n",
    "    cv.update_lookahead(params[\"lookahead\"])\n",
    "\n",
    "    for train_idx, val_idx in cv:\n",
    "        # Dynamic target based on the suggested lookahead\n",
    "        label = f'TARGET_ret_fwd_{params[\"lookahead\"]:02d}d_rank_quantiled'\n",
    "\n",
    "        train_features = data.loc[train_idx, features]\n",
    "        train_labels = data.loc[train_idx, label]\n",
    "        lgb_train = lgb.Dataset(data=train_features, label=train_labels, \n",
    "                        categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        val_features = data.loc[val_idx, features]\n",
    "        val_labels = data.loc[val_idx, label]\n",
    "        lgb_val = lgb.Dataset(data=val_features, label=val_labels, \n",
    "                categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(params=params,\n",
    "                          train_set=lgb_train,\n",
    "                          num_boost_round=5000,\n",
    "                          valid_sets=[lgb_train, lgb_val],\n",
    "                          valid_names=['train', 'valid_0'],\n",
    "                          feval=combined_fold_metrics, \n",
    "                          callbacks=[lgb.record_evaluation(evals_result),\n",
    "                                     early_stopping,\n",
    "                                     LightGBMPruningCallback(trial, 'sharpe_ratio')])\n",
    "\n",
    "        daily_scores_in_fold.extend(evals_result['valid_0']['sharpe_ratio'])\n",
    "\n",
    "\n",
    "\n",
    "    weights = {\n",
    "        'sharpe_ratio': 0.95,       # Primary objective, so highest weight\n",
    "        'max_dd': -0.1,             # Major risk metric, negative to penalize higher drawdowns\n",
    "        'autocorrelation': -0.1,    # Penalize strategies showing signs of overfitting\n",
    "        'std_deviation': -0.025,    # Mild penalty for higher volatility \n",
    "        'smart_sharpe': 0.075       # Supplementary to Sharpe Ratio but considering autocorrelation\n",
    "    }\n",
    "\n",
    "\n",
    "    metrics = metrics_on_fold(daily_scores_in_fold, weights=weights)\n",
    "    # score = metrics['sharpe_ratio']\n",
    "    score = metrics[\"weighted_score\"]\n",
    "    # Log parameters, metrics, and evaluation results to MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log metrics from metrics dictionary\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Logging each score and metric in evals_result\n",
    "        for valid_set, metrics_dict in evals_result.items():\n",
    "            for metric, values in metrics_dict.items():\n",
    "                for idx, value in enumerate(values):\n",
    "                    metric_name = f\"{valid_set}_{metric}_{idx}\"\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "\n",
    "        # Log the average sharpe ratio\n",
    "        mlflow.log_metric(\"avg_score_across_folds\", score)\n",
    "\n",
    "        # mlflow.lightgbm.log_model(model, \"lightgbm_model\")\n",
    "\n",
    "    # score = metrics['sharpe_ratio']\n",
    "    # print(score)\n",
    "    return score if not np.isnan(score) else 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-04 14:06:56,118] A new study created in RDB with name: lgbm_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "\n",
    "def progress_bar(study, trial, n_trials):\n",
    "    progress = (trial.number + 1) / n_trials\n",
    "    best_trial_msg = \"\"\n",
    "    if study.best_trial is not None:\n",
    "        best_trial_msg = f\"Best is trial {study.best_trial.number} \\\n",
    "            with value: {study.best_trial.value}.\"\n",
    "    print(f'Trial {trial.number + 1}/{n_trials} finished with value: \\\n",
    "        {trial.value} and parameters: {trial.params}. {best_trial_msg}')\n",
    "\n",
    "# Check if 'study' directory exists, if not, create it.\n",
    "if not os.path.exists(\"study\"):\n",
    "    os.makedirs(\"study\")\n",
    "\n",
    "# Use SQLite to store optimization results.\n",
    "# The study results are stored in the \"study\" folder as \"study.db\".\n",
    "storage_name = \"sqlite:///study/study.db\"\n",
    "\n",
    "# Name of the study. This should be consistent for resuming the study later.\n",
    "study_name = \"lgbm_optimization\"\n",
    "\n",
    "# Try to load the study. If it doesn't exist, create a new one.\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            storage=storage_name,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True, \n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, \\\n",
    "                            n_warmup_steps=5))\n",
    "\n",
    "n_trials = 5\n",
    "features = [col for col in dataset.columns if col.startswith('FEATURE_')]\n",
    "study.optimize(lambda trial: objective(trial, dataset, features), \n",
    "               n_trials=n_trials, \n",
    "               callbacks=[lambda study, trial: progress_bar(study, trial, n_trials)])\n",
    "\n",
    "# Printing the optimization results\n",
    "print(f'Best trial score: {study.best_trial.value}')\n",
    "print('Best hyperparameters:')\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters found by Optuna:\")\n",
    "print(best_params)\n",
    "\n",
    "# Remove the study database\n",
    "os.remove(\"study/study.db\")\n",
    "print(\"Database has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming evals_result contains MAE, MSE, RMSE, R2 as well\n",
    "metrics = ['IC', 'sharpe_ratio', 'MAE', 'MSE', 'RMSE', 'R2']\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4 * len(metrics)))\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    cv_result = pd.DataFrame({'Train Set': evals_result['train'][metric], \n",
    "                              'Validation Set': evals_result['valid_0'][metric]})\n",
    "    \n",
    "    ax1 = ax\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes sharing the same x-axis\n",
    "    \n",
    "    ax1.plot(cv_result.index, cv_result['Train Set'], 'g-', label=f'Train Set {metric}')\n",
    "    ax2.plot(cv_result.index, cv_result['Validation Set'], 'b-', label=f'Validation Set {metric}')\n",
    "    \n",
    "    ax1.set_ylabel(f'Train Set {metric}', color='g')\n",
    "    ax2.set_ylabel(f'Validation Set {metric}', color='b')\n",
    "    \n",
    "    if metric != 'R2':\n",
    "        ax1.axvline(cv_result['Validation Set'].idxmin(), c='k', ls='--', lw=1)\n",
    "    else:\n",
    "        ax1.axvline(cv_result['Validation Set'].idxmax(), c='k', ls='--', lw=1)\n",
    "\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract cat_threshold from best_params\n",
    "# cat_threshold = best_params.get('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "# Extract cat_threshold from best_params\n",
    "cat_threshold = best_params.pop('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "\n",
    "# Determine the categorical columns based on cat_threshold\n",
    "categoricals = [col for col in dataset.columns if dataset[col].nunique() < cat_threshold \n",
    "                and dataset[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "# Create the training dataset\n",
    "lgb_train_all = lgb.Dataset(data=dataset[features], label=dataset[label], \n",
    "                            categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "best_params['force_col_wise'] = True\n",
    "# Get the best iteration from your previous training\n",
    "# Get the best iteration from your previous training\n",
    "optimal_boosting_rounds = evals_result['valid_0']['sharpe_ratio'].index(max(evals_result['valid_0']['sharpe_ratio']))\n",
    "\n",
    "if optimal_boosting_rounds == 0:\n",
    "    optimal_boosting_rounds = 1\n",
    "    print(optimal_boosting_rounds)\n",
    "\n",
    "best_model = lgb.train(params=best_params,\n",
    "                       train_set=lgb_train_all,\n",
    "                       num_boost_round=optimal_boosting_rounds,  # Use the optimal number of rounds\n",
    "                       feval=combined_eval_metrics,  # Updated feval\n",
    "                       callbacks=[lgb.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the models folder path\n",
    "models = Path(\"./models\")\n",
    "\n",
    "# Ensure the folder exists\n",
    "models.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure that dataset_key doesn't contain invalid characters like slashes\n",
    "clean_dataset_key = dataset_key.replace(\"/\", \"_\")\n",
    "\n",
    "# Formulate the clean save path\n",
    "save_path = models / f\"{top}{clean_dataset_key}_best_model_{target}.txt\"\n",
    "\n",
    "# Try saving again\n",
    "best_model.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from utils import rank_stocks_and_quantile\n",
    "\n",
    "top = 250  # parameters -> papermill\n",
    "UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "UNSEEN_STORE = Path(f'data/{top}_unseen_dataset.h5')\n",
    "with pd.HDFStore(UNSEEN_STORE) as store:\n",
    "    test_data = store[UNSEEN_KEY]\n",
    "    test_data = rank_stocks_and_quantile(test_data, TARGET_col=target)\n",
    "    # print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data[features]\n",
    "test_labels = test_data[label]\n",
    "\n",
    "model_path = f\"/home/sayem/Desktop/Project/{save_path}\"\n",
    "# Load the model\n",
    "best_model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "y_pred = best_model.predict(test_features)\n",
    "\n",
    "preds = test_labels.reset_index(name=\\\n",
    "    'actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "\n",
    "# Rename columns to add 'feature_' prefix\n",
    "cols_to_rename = ['open', 'high', 'low', 'close', 'volume']\n",
    "new_col_names = [\"FEATURE_\" + col for col in cols_to_rename]\n",
    "rename_dict = dict(zip(cols_to_rename, new_col_names))\n",
    "\n",
    "test_data_renamed = test_data.rename(columns=rename_dict)\n",
    "\n",
    "# Using the 'merge' method to join on MultiIndex levels 'date' and 'ticker'\n",
    "preds = preds.reset_index().merge(test_data_renamed[new_col_names].reset_index(), \n",
    "                                  on=['ticker', 'date'], \n",
    "                                  how='left')\n",
    "\n",
    "### Only select columns of interest\n",
    "preds = preds[['date', 'ticker', 'actual', 'predicted'] \\\n",
    "    + new_col_names].set_index(['ticker', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['predicted'].value_counts()\n",
    "# print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_spearman(group):\n",
    "    return spearmanr(group['actual'], group['predicted'])[0]\n",
    "\n",
    "daily_correlations = preds.groupby('date').apply(daily_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_correlations.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_correlations.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of daily correlations\n",
    "mean_daily_correlation = daily_correlations.mean()\n",
    "\n",
    "# Calculate Sharpe ratio for each date\n",
    "fold_sharpe_ratio = papermill_fold_scores = daily_correlations / daily_correlations.std()\n",
    "\n",
    "papermill_fold_scores_df = papermill_fold_scores.to_frame()\n",
    "papermill_fold_scores_df.columns = papermill_fold_scores_df.columns.astype(str)\n",
    "sb.glue(\"papermill_era_scores\", papermill_fold_scores_df, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a list of colors based on the sign of the Sharpe Ratios\n",
    "# colors = ['blue' if value > 0 else 'red' for value in fold_sharpe_ratio]\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# fold_sharpe_ratio.plot(kind='bar', color=colors)\n",
    "# plt.title('Daily Sharpe Ratios')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Sharpe Ratio')\n",
    "# plt.grid(axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.axhline(y=0, color='black', linestyle='-')  # Here's where we add the horizontal line at y=0\n",
    "# plt.xticks(rotation=45)  # rotates the x-axis labels for better visibility\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dir = Path(\"plots\")\n",
    "# plot_dir.mkdir(exist_ok=True)\n",
    "# plot_path = plot_dir / f\"sharpe_ratios_{key}.png\"\n",
    "# plt.savefig(plot_path)\n",
    "# plt.close()\n",
    "\n",
    "# papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "# sb.glue(\"papermill_plot_path\", papermill_plot_path_str, display=True)  # Glue the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_r, lr_p = spearmanr(preds.actual, preds.predicted)\n",
    "print(f'Information Coefficient (overall): {lr_r:.3%} (p-value: {lr_p:.8%})')\n",
    "\n",
    "# Return the Information Coefficient and its p-value\n",
    "information_coefficient = lr_r\n",
    "p_value = lr_p\n",
    "\n",
    "# information_coefficient = papermill_information_coefficient, p_value = papermill_p_value\n",
    "sb.glue(\"information_coefficient\", information_coefficient, display=True)\n",
    "sb.glue(\"p_value\", p_value, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "# sb.glue(\"papermill_plot_path\", papermill_plot_path_str)  # Glue the string\n",
    "\n",
    "# sb.glue(\"information_coefficient\", information_coefficient)\n",
    "# sb.glue(\"p_value\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import alphalens as al\n",
    "\n",
    "# def generate_alphalens_tearsheet(df: pd.DataFrame, label_col: str, price_col: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate the Alphalens full tearsheet given the input dataframe, \n",
    "#     label column, and price data column.\n",
    "#     \"\"\"\n",
    "#     # Extract the factor and trade_prices series\n",
    "#     factor = df[label_col]\n",
    "#     trade_prices = df[price_col]\n",
    "    \n",
    "#     # Remove duplicated indices from factor and trade_prices\n",
    "#     factor = factor[~factor.index.duplicated(keep='first')]\n",
    "#     trade_prices = trade_prices[~trade_prices.index.duplicated(keep='first')]\n",
    "    \n",
    "#     # Convert the trade_prices series into unstacked format\n",
    "#     trade_prices_unstacked = trade_prices.unstack(level='ticker')\n",
    "    \n",
    "#     # Ensure the factor's DatetimeIndex level has the same timezone as trade_prices_unstacked\n",
    "#     if isinstance(factor.index, pd.MultiIndex):\n",
    "#         level_0 = factor.index.get_level_values(0)\n",
    "#         if hasattr(level_0, 'tz') and level_0.tz is not None:\n",
    "#             if level_0.tz != trade_prices_unstacked.index.tz:\n",
    "#                 factor.index.set_levels(level_0.tz_convert(trade_prices_unstacked.index.tz), \\\n",
    "#                     level=0, inplace=True)\n",
    "#         factor = factor.swaplevel().sort_index()\n",
    "#     else:\n",
    "#         if hasattr(factor.index, 'tz') and factor.index.tz is not None:\n",
    "#             if factor.index.tz != trade_prices_unstacked.index.tz:\n",
    "#                 factor.index = factor.index.tz_convert(trade_prices_unstacked.index.tz)\n",
    "\n",
    "#     # Create the factor_data dataframe with forward returns\n",
    "#     alphalen_analysis = al.utils.get_clean_factor_and_forward_returns(\n",
    "#         factor=factor,\n",
    "#         prices=trade_prices_unstacked,\n",
    "#         periods=[1, 5, 10],\n",
    "#         max_loss=0.6\n",
    "#     )\n",
    "#     factor_returns = al.performance.factor_returns(alphalen_analysis)\n",
    "#     sharpe_ratios = factor_returns.mean() / factor_returns.std()\n",
    "\n",
    "#     print(\"\\nSharpe Ratios:\\n\", sharpe_ratios)\n",
    "#     alphalen_analysis = alphalen_analysis[~alphalen_analysis.index.duplicated(keep='first')]\n",
    "#     alphalen_analysis = alphalen_analysis.groupby(level=[0, 1]).mean()\n",
    "#     return alphalen_analysis\n",
    "\n",
    "# # target = 'actual'\n",
    "# alphalens_analysis = generate_alphalens_tearsheet(preds, \\\n",
    "#     label_col='actual', price_col='FEATURE_close')\n",
    "\n",
    "# import alphalens as al\n",
    "# al.tears.create_full_tear_sheet(alphalens_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
