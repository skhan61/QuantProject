{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import rank_stocks_and_quantile\n",
    "\n",
    "top = 250 # parameters -> papermill\n",
    "\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_key = '/data/YEAR_20200930_20220802'\n",
    "# dataset_key = None\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    dataset = store[dataset_key]\n",
    "    # dataset = store['/data/YEAR_20161115_20181022']\n",
    "    dataset = rank_stocks_and_quantile(dataset, TARGET_col='TARGET_ret_fwd_frac_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the features and label columns\n",
    "features = [col for col in dataset.columns if col.startswith('FEATURE_')]\n",
    "label = 'TARGET_ret_fwd_frac_order_rank_quantiled'\n",
    "\n",
    "# Print the number of features and the label\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "# Remove timezone information from the date index\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)\n",
    "\n",
    "### Since we have unseen dataset\n",
    "# # Get unique dates and sort them\n",
    "# unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "\n",
    "# # Define the look-ahead gap\n",
    "# look_ahead = 1\n",
    "\n",
    "# # Split dates for training and testing with a gap\n",
    "# train_dates = unique_dates[:-21-look_ahead]\n",
    "# test_dates = unique_dates[-21:]\n",
    "\n",
    "# # Split the dataset using the train and test dates\n",
    "# train_data = dataset[dataset.index.get_level_values('date').isin(train_dates)]\n",
    "# test_data = dataset[dataset.index.get_level_values('date').isin(test_dates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fi(model):\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(),\n",
    "                index=model.feature_name()))\n",
    "                \n",
    "def ic_lgbm(preds, train_data):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    is_higher_better = True\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], \\\n",
    "        is_higher_better\n",
    "\n",
    "def sharpe_ratio_lgbm(preds, train_data):\n",
    "    \"\"\"Custom Sharpe ratio eval metric for lightgbm that calculates daily Spearman correlations.\"\"\"\n",
    "    labels = train_data.get_label()\n",
    "    # print(type(preds))\n",
    "    # print(type(train_data))\n",
    "    # print(labels)\n",
    "    # Assuming the data index is a MultiIndex with date as the first level\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    # Group by the first level of the MultiIndex (date) and compute the Spearman correlation for each group\n",
    "    grouped_labels = pd.Series(labels, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_labels))\n",
    "    grouped_preds = pd.Series(preds, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_preds))\n",
    "\n",
    "    daily_scores = []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        score_for_day = spearmanr(actuals_for_day, preds_for_day)[0]\n",
    "        if np.isnan(score_for_day):\n",
    "            score_for_day = 0\n",
    "        daily_scores.append(score_for_day)\n",
    "\n",
    "    # Calculate the Sharpe ratio\n",
    "    sharpe_ratio = np.mean(daily_scores) / (np.std(daily_scores) + 1e-9)  \n",
    "    # added epsilon to avoid division by zero\n",
    "\n",
    "    return 'sharpe_ratio', sharpe_ratio, True\n",
    "\n",
    "\n",
    "def custom_eval_metrics(preds, train_data):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    labels = train_data.get_label()\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(labels, preds)\n",
    "\n",
    "    return [(\"MAE\", mae, False), \n",
    "            (\"MSE\", mse, False), \n",
    "            (\"RMSE\", rmse, False), \n",
    "            (\"R2\", r2, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_fold(era_scores):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max()\n",
    "\n",
    "    # Smart Sharpe\n",
    "    smart_sharpe = mean_correlation / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd,\n",
    "        'min_correlation': era_scores.min(),\n",
    "        'max_correlation': era_scores.max(),\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from scipy.stats import spearmanr\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "def get_categoricals(dataset, threshold):\n",
    "    return [col for col in dataset.columns if \\\n",
    "            dataset[col].nunique() < threshold and \\\n",
    "            dataset[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "def objective(trial, data, features, cv):\n",
    "    # Dynamic categoricals based on the trial's suggested threshold\n",
    "    cat_threshold = trial.suggest_int('cat_threshold', 5, 50)\n",
    "    categoricals = [col for col in data.columns if data[col].nunique() < cat_threshold \n",
    "                    and data[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "    params = {\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'verbose': -1,\n",
    "        'metric': 'None',\n",
    "        'device': 'gpu',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "    }\n",
    "\n",
    "    early_stopping = lgb.early_stopping(stopping_rounds=500, \\\n",
    "        verbose=True, first_metric_only=True)\n",
    "    daily_scores_in_fold = []\n",
    "\n",
    "    for train_idx, val_idx in cv:\n",
    "        train_features = data.loc[train_idx, features]\n",
    "        train_labels = data.loc[train_idx, label]\n",
    "        lgb_train = lgb.Dataset(data=train_features, label=train_labels, \n",
    "                        categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        val_features = data.loc[val_idx, features]\n",
    "        val_labels = data.loc[val_idx, label]\n",
    "        lgb_val = lgb.Dataset(data=val_features, label=val_labels, \n",
    "                categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(params=params,\n",
    "                          train_set=lgb_train,\n",
    "                          num_boost_round=5000,\n",
    "                          valid_sets=[lgb_train, lgb_val],\n",
    "                          valid_names=['train', 'valid_0'],\n",
    "                          feval=[sharpe_ratio_lgbm, custom_eval_metrics],\n",
    "                          callbacks=[lgb.record_evaluation(evals_result),\n",
    "                                     early_stopping,\n",
    "                                     LightGBMPruningCallback(trial, 'sharpe_ratio')])\n",
    "        \n",
    "        daily_scores_in_fold.extend(evals_result['valid_0']['sharpe_ratio'])\n",
    "\n",
    "    metrics = metrics_on_fold(daily_scores_in_fold)\n",
    "    score = metrics['sharpe_ratio']\n",
    "\n",
    "    # Log parameters, metrics, and evaluation results to MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log metrics from metrics dictionary\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Logging each score and metric in evals_result\n",
    "        for valid_set, metrics_dict in evals_result.items():\n",
    "            for metric, values in metrics_dict.items():\n",
    "                for idx, value in enumerate(values):\n",
    "                    metric_name = f\"{valid_set}_{metric}_{idx}\"\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "\n",
    "        # Log the average sharpe ratio\n",
    "        mlflow.log_metric(\"avg_sharpe_ratio_across_folds\", score)\n",
    "\n",
    "        # mlflow.lightgbm.log_model(model, \"lightgbm_model\")\n",
    "\n",
    "    score = metrics['sharpe_ratio']\n",
    "    # print(score)\n",
    "    return score if not np.isnan(score) else 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length=142, \n",
    "                                        test_period_length=5, \n",
    "                                        lookahead=1, \n",
    "                                        date_idx='date')\n",
    "\n",
    "def progress_bar(study, trial, n_trials):\n",
    "    progress = (trial.number + 1) / n_trials\n",
    "    best_trial_msg = \"\"\n",
    "    if study.best_trial is not None:\n",
    "        best_trial_msg = f\"Best is trial {study.best_trial.number} \\\n",
    "            with value: {study.best_trial.value}.\"\n",
    "    print(f'Trial {trial.number + 1}/{n_trials} finished with value: \\\n",
    "        {trial.value} and parameters: {trial.params}. {best_trial_msg}')\n",
    "\n",
    "# Check if 'study' directory exists, if not, create it.\n",
    "if not os.path.exists(\"study\"):\n",
    "    os.makedirs(\"study\")\n",
    "\n",
    "# Use SQLite to store optimization results.\n",
    "# The study results are stored in the \"study\" folder as \"study.db\".\n",
    "storage_name = \"sqlite:///study/study.db\"\n",
    "\n",
    "# Name of the study. This should be consistent for resuming the study later.\n",
    "study_name = \"lgbm_optimization\"\n",
    "\n",
    "# Try to load the study. If it doesn't exist, create a new one.\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            storage=storage_name,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True, \n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, \\\n",
    "                            n_warmup_steps=5))\n",
    "\n",
    "n_trials = 5\n",
    "study.optimize(lambda trial: objective(trial, dataset, features, cv), \n",
    "               n_trials=n_trials, \n",
    "               callbacks=[lambda study, trial: progress_bar(study, trial, n_trials)])\n",
    "\n",
    "# Printing the optimization results\n",
    "print(f'Best trial score: {study.best_trial.value}')\n",
    "print('Best hyperparameters:')\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters found by Optuna:\")\n",
    "print(best_params)\n",
    "\n",
    "# Remove the study database\n",
    "os.remove(\"study/study.db\")\n",
    "print(\"Database has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result['valid_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming evals_result contains MAE, MSE, RMSE, R2 as well\n",
    "metrics = ['sharpe_ratio', 'MAE', 'MSE', 'RMSE', 'R2']\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4 * len(metrics)))\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    cv_result = pd.DataFrame({'Train Set': evals_result['train'][metric], \n",
    "                              'Validation Set': evals_result['valid_0'][metric]})\n",
    "    \n",
    "    ax1 = ax\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes sharing the same x-axis\n",
    "    \n",
    "    ax1.plot(cv_result.index, cv_result['Train Set'], 'g-', label=f'Train Set {metric}')\n",
    "    ax2.plot(cv_result.index, cv_result['Validation Set'], 'b-', label=f'Validation Set {metric}')\n",
    "    \n",
    "    ax1.set_ylabel(f'Train Set {metric}', color='g')\n",
    "    ax2.set_ylabel(f'Validation Set {metric}', color='b')\n",
    "    \n",
    "    if metric != 'R2':\n",
    "        ax1.axvline(cv_result['Validation Set'].idxmin(), c='k', ls='--', lw=1)\n",
    "    else:\n",
    "        ax1.axvline(cv_result['Validation Set'].idxmax(), c='k', ls='--', lw=1)\n",
    "\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'valid_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_a_ML _models_forecasting.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m best_params[\u001b[39m'\u001b[39m\u001b[39mforce_col_wise\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Get the best iteration from your previous training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimal_boosting_rounds \u001b[39m=\u001b[39m evals_result[\u001b[39m'\u001b[39m\u001b[39mvalid_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msharpe_ratio\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(evals_result[\u001b[39m'\u001b[39m\u001b[39mvalid_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msharpe_ratio\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m best_model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mtrain(params\u001b[39m=\u001b[39mbest_params,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                        train_set\u001b[39m=\u001b[39mlgb_train_all,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                        num_boost_round\u001b[39m=\u001b[39moptimal_boosting_rounds,  \u001b[39m# Use the optimal number of rounds\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                        feval\u001b[39m=\u001b[39msharpe_ratio_lgbm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_a_ML%20_models_forecasting.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                        callbacks\u001b[39m=\u001b[39m[lgb\u001b[39m.\u001b[39mrecord_evaluation(evals_result)])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'valid_0'"
     ]
    }
   ],
   "source": [
    "# # Extract cat_threshold from best_params\n",
    "# cat_threshold = best_params.get('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "# Extract cat_threshold from best_params\n",
    "cat_threshold = best_params.pop('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "\n",
    "# Determine the categorical columns based on cat_threshold\n",
    "categoricals = [col for col in dataset.columns if dataset[col].nunique() < cat_threshold \n",
    "                and dataset[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "# Create the training dataset\n",
    "lgb_train_all = lgb.Dataset(data=dataset[features], label=dataset[label], \n",
    "                            categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "best_params['force_col_wise'] = True\n",
    "# Get the best iteration from your previous training\n",
    "optimal_boosting_rounds = evals_result['valid_0']['sharpe_ratio'].index(max(evals_result['valid_0']['sharpe_ratio']))\n",
    "\n",
    "best_model = lgb.train(params=best_params,\n",
    "                       train_set=lgb_train_all,\n",
    "                       num_boost_round=optimal_boosting_rounds,  # Use the optimal number of rounds\n",
    "                       feval=sharpe_ratio_lgbm,\n",
    "                       callbacks=[lgb.record_evaluation(evals_result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the models folder path\n",
    "models = Path(\"./models\")\n",
    "\n",
    "# Ensure the folder exists\n",
    "models.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure that dataset_key doesn't contain invalid characters like slashes\n",
    "clean_dataset_key = dataset_key.replace(\"/\", \"_\")\n",
    "\n",
    "# Formulate the clean save path\n",
    "save_path = models / f\"{clean_dataset_key}_best_model.txt\"\n",
    "\n",
    "# Try saving again\n",
    "best_model.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data[features]\n",
    "test_labels = test_data[label]\n",
    "\n",
    "model_path = \"/home/sayem/Desktop/Project/models/_data_YEAR_20200929_20220902_best_model.txt\"\n",
    "# Load the model\n",
    "best_model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "y_pred = best_model.predict(test_features)\n",
    "\n",
    "preds = test_labels.reset_index(name=\\\n",
    "    'actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "\n",
    "# Rename columns to add 'feature_' prefix\n",
    "cols_to_rename = ['open', 'high', 'low', 'close', 'volume']\n",
    "new_col_names = [\"FEATURE_\" + col for col in cols_to_rename]\n",
    "rename_dict = dict(zip(cols_to_rename, new_col_names))\n",
    "\n",
    "test_data_renamed = test_data.rename(columns=rename_dict)\n",
    "\n",
    "# Using the 'merge' method to join on MultiIndex levels 'date' and 'ticker'\n",
    "preds = preds.reset_index().merge(test_data_renamed[new_col_names].reset_index(), \n",
    "                                  on=['ticker', 'date'], \n",
    "                                  how='left')\n",
    "\n",
    "### Only select columns of interest\n",
    "preds = preds[['date', 'ticker', 'actual', 'predicted'] \\\n",
    "    + new_col_names].set_index(['ticker', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import alphalens as al\n",
    "\n",
    "def generate_alphalens_tearsheet(df: pd.DataFrame, label_col: str, price_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate the Alphalens full tearsheet given the input dataframe, \n",
    "    label column, and price data column.\n",
    "    \"\"\"\n",
    "    # Extract the factor and trade_prices series\n",
    "    factor = df[label_col]\n",
    "    trade_prices = df[price_col]\n",
    "    \n",
    "    # Remove duplicated indices from factor and trade_prices\n",
    "    factor = factor[~factor.index.duplicated(keep='first')]\n",
    "    trade_prices = trade_prices[~trade_prices.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Convert the trade_prices series into unstacked format\n",
    "    trade_prices_unstacked = trade_prices.unstack(level='ticker')\n",
    "    \n",
    "    # Ensure the factor's DatetimeIndex level has the same timezone as trade_prices_unstacked\n",
    "    if isinstance(factor.index, pd.MultiIndex):\n",
    "        level_0 = factor.index.get_level_values(0)\n",
    "        if hasattr(level_0, 'tz') and level_0.tz is not None:\n",
    "            if level_0.tz != trade_prices_unstacked.index.tz:\n",
    "                factor.index.set_levels(level_0.tz_convert(trade_prices_unstacked.index.tz), \\\n",
    "                    level=0, inplace=True)\n",
    "        factor = factor.swaplevel().sort_index()\n",
    "    else:\n",
    "        if hasattr(factor.index, 'tz') and factor.index.tz is not None:\n",
    "            if factor.index.tz != trade_prices_unstacked.index.tz:\n",
    "                factor.index = factor.index.tz_convert(trade_prices_unstacked.index.tz)\n",
    "\n",
    "    # Create the factor_data dataframe with forward returns\n",
    "    alphalen_analysis = al.utils.get_clean_factor_and_forward_returns(\n",
    "        factor=factor,\n",
    "        prices=trade_prices_unstacked,\n",
    "        periods=[1, 5, 10],\n",
    "        max_loss=0.9\n",
    "    )\n",
    "\n",
    "    factor_returns = al.performance.factor_returns(alphalen_analysis)\n",
    "    sharpe_ratios = factor_returns.mean() / factor_returns.std()\n",
    "\n",
    "    print(\"\\nSharpe Ratios:\\n\", sharpe_ratios)\n",
    "    alphalen_analysis = alphalen_analysis[~alphalen_analysis.index.duplicated(keep='first')]\n",
    "    alphalen_analysis = alphalen_analysis.groupby(level=[0, 1]).mean()\n",
    "    return alphalen_analysis\n",
    "\n",
    "target = 'actual'\n",
    "alphalens_analysis = generate_alphalens_tearsheet(preds, \\\n",
    "    label_col=target, price_col='FEATURE_close')\n",
    "\n",
    "import alphalens as al\n",
    "al.tears.create_full_tear_sheet(alphalens_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import alphalens as al\n",
    "\n",
    "# def generate_alphalens_tearsheet(df: pd.DataFrame, \\\n",
    "#     label_col: str, price_col: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate the Alphalens full tearsheet given the input dataframe, \n",
    "#     label column, and price data column.\n",
    "#     \"\"\"\n",
    "#     # Extract the factor and trade_prices series\n",
    "#     factor = df[label_col]\n",
    "#     trade_prices = df[price_col]\n",
    "    \n",
    "#     # Handle duplicate indices in trade_prices\n",
    "#     trade_prices = trade_prices[~trade_prices.index.duplicated(keep='first')]\n",
    "    \n",
    "#     # Convert the trade_prices series into unstacked format\n",
    "#     trade_prices_unstacked = trade_prices.unstack(level='ticker')\n",
    "    \n",
    "#     # Ensure the factor's DatetimeIndex level has the same timezone as trade_prices_unstacked\n",
    "#     if isinstance(factor.index, pd.MultiIndex):\n",
    "#         level_0 = factor.index.get_level_values(0)\n",
    "#         if hasattr(level_0, 'tz') and level_0.tz is not None:\n",
    "#             if level_0.tz != trade_prices_unstacked.index.tz:\n",
    "#                 factor.index.set_levels(level_0.tz_convert(trade_prices_unstacked.index.tz), level=0, inplace=True)\n",
    "#         factor = factor.swaplevel().sort_index()\n",
    "#     else:\n",
    "#         if hasattr(factor.index, 'tz') and factor.index.tz is not None:\n",
    "#             if factor.index.tz != trade_prices_unstacked.index.tz:\n",
    "#                 factor.index = factor.index.tz_convert(trade_prices_unstacked.index.tz)\n",
    "\n",
    "#     # Create the factor_data dataframe with forward returns\n",
    "#     alphalen_analysis = al.utils.get_clean_factor_and_forward_returns(\n",
    "#         factor=factor,\n",
    "#         prices=trade_prices_unstacked,\n",
    "#         periods=[1, 5, 10],\n",
    "#         max_loss=0.5\n",
    "#     )\n",
    "\n",
    "#     factor_returns = al.performance.factor_returns(alphalen_analysis)\n",
    "#     sharpe_ratios = factor_returns.mean() / factor_returns.std()\n",
    "\n",
    "#     print(\"\\nSharpe Ratios:\\n\", sharpe_ratios)\n",
    "#     alphalen_analysis = alphalen_analysis[~alphalen_analysis.index.duplicated(keep='first')]\n",
    "#     alphalen_analysis = alphalen_analysis.groupby(level=[0, 1]).mean()\n",
    "#     return alphalen_analysis\n",
    "\n",
    "# target = 'predicted'\n",
    "# alphalens_analysis = generate_alphalens_tearsheet(preds, label_col=target, price_col='FEATURE_close')\n",
    "\n",
    "# import alphalens as al\n",
    "# al.tears.create_full_tear_sheet(alphalens_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_spearman(group):\n",
    "    return spearmanr(group['actual'], group['predicted'])[0]\n",
    "\n",
    "daily_correlations = preds.groupby('date').apply(daily_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of daily correlations\n",
    "mean_daily_correlation = daily_correlations.mean()\n",
    "std_daily_correlation = daily_correlations.std()\n",
    "\n",
    "# Calculate Sharpe ratio for each date\n",
    "papermill_era_scores = daily_sharpe_ratios = (daily_correlations - \\\n",
    "    mean_daily_correlation) / std_daily_correlation\n",
    "\n",
    "papermill_era_scores_df = papermill_era_scores.to_frame()\n",
    "papermill_era_scores_df.columns = papermill_era_scores_df.columns.astype(str)\n",
    "sb.glue(\"papermill_era_scores\", papermill_era_scores_df, display=True)\n",
    "\n",
    "# papermill_era_scores_list = papermill_era_scores.tolist()\n",
    "# sb.glue(\"papermill_era_scores\", papermill_era_scores_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of colors based on the sign of the Sharpe Ratios\n",
    "colors = ['blue' if value > 0 else 'red' for value in daily_sharpe_ratios]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_sharpe_ratios.plot(kind='bar', color=colors)\n",
    "plt.title('Daily Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.axhline(y=0, color='black', linestyle='-')  # Here's where we add the horizontal line at y=0\n",
    "plt.xticks(rotation=45)  # rotates the x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path(\"plots\")\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "plot_path = plot_dir / f\"sharpe_ratios_{key}.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "sb.glue(\"papermill_plot_path\", papermill_plot_path_str, display=True)  # Glue the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_r, lr_p = spearmanr(preds.actual, preds.predicted)\n",
    "print(f'Information Coefficient (overall): {lr_r:.3%} (p-value: {lr_p:.8%})')\n",
    "\n",
    "# Return the Information Coefficient and its p-value\n",
    "information_coefficient = lr_r\n",
    "p_value = lr_p\n",
    "\n",
    "# information_coefficient = papermill_information_coefficient, p_value = papermill_p_value\n",
    "sb.glue(\"information_coefficient\", information_coefficient, display=True)\n",
    "sb.glue(\"p_value\", p_value, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "# sb.glue(\"papermill_plot_path\", papermill_plot_path_str)  # Glue the string\n",
    "\n",
    "# sb.glue(\"information_coefficient\", information_coefficient)\n",
    "# sb.glue(\"p_value\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
