{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in HDF5 file: ['/data/2020-01-02_to_2022-12-30', '/data/corr_IC_2020-01-02_to_2022-12-30', '/data/corr_IC_2022-07-21_to_2022-07-29', '/data/reduced_2022-07-21_to_2022-07-29', '/data/corr_IC_/data/reduced_2022-07-21_to_2022-07-29']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "FILE_PATH = \"/home/sayem/Desktop/Project/data/dataset.h5\"\n",
    "\n",
    "# Open the HDF5 file\n",
    "with pd.HDFStore(FILE_PATH) as store:\n",
    "    keys = store.keys()\n",
    "\n",
    "# Print the keys\n",
    "print(\"Keys in HDF5 file:\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define file path\n",
    "FILE_PATH = \"/home/sayem/Desktop/Project/data/dataset.h5\"\n",
    "\n",
    "\n",
    "FILE_NAME = '/data/corr_IC_2020-01-02_to_2022-12-30'\n",
    "\n",
    "# FILE_NAME = '/data/2020-01-02_to_2022-12-30'\n",
    "\n",
    "# Read the dataset using Dask\n",
    "data = dd.read_hdf(FILE_PATH, FILE_NAME)\n",
    "\n",
    "# Compute the result (this will load data into memory)\n",
    "result = data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4191516 entries, ('AA', Timestamp('2020-01-02 00:00:00')) to ('ZTS', Timestamp('2022-12-30 00:00:00'))\n",
      "Columns: 129 entries, MARKET_CAP to CLOSE\n",
      "dtypes: float32(70), float64(4), int32(48), int8(7)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Assuming you want to add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "idx = pd.IndexSlice\n",
    "deciles = np.arange(.1, 1, .1).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_READ = Path('data/dataset.h5')\n",
    "\n",
    "dataset = (pd.concat([pd.read_hdf(DATA_READ, 'raw')],\n",
    "                     axis=1)\n",
    "                    .dropna(axis=1, thresh=100000)\n",
    "                    .sort_index())\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "cols = dataset.columns.tolist()\n",
    "\n",
    "# Populate the features list with column names starting with 'feature_'\n",
    "features = [col for col in cols if col.startswith('feature_')]\n",
    "\n",
    "# Find the first column starting with 'target_' and set it as the label\n",
    "label_cols = [col for col in cols if col.startswith('target_')]\n",
    "target = label_cols[0] if label_cols else None\n",
    "\n",
    "print(len(features))  # This will show all the columns starting with 'feature_'\n",
    "print(target)  # This will show the first column starting with 'target_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique dates and sort them\n",
    "unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "\n",
    "# Adjust for the look-ahead gap\n",
    "look_ahead = 1\n",
    "\n",
    "# Split dates for training and testing with a gap\n",
    "train_dates = unique_dates[:-21-look_ahead]\n",
    "test_dates = unique_dates[-21:]\n",
    "\n",
    "# Split the dataset\n",
    "train_data = dataset.loc[train_dates] ## train + val\n",
    "test_data = dataset.loc[test_dates] # backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(train_period_length=142, \n",
    "                                        test_period_length=21, \n",
    "                                        lookahead=1, \n",
    "                                        date_idx='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from models import SimpleNN\n",
    "from utils import get_era2data, train_model, metrics_on_batch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FEATURE_DIM = 225\n",
    "OUTPUT_DIM = 1\n",
    "MAX_LEN = 500\n",
    "\n",
    "# Define paths for saving models and hyperparameters\n",
    "model_dir = \"./saved_models\"\n",
    "best_hyperparams_dir = \"./best_hyperparams\"\n",
    "study_dir = \"./optuna_studies\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir in [model_dir, best_hyperparams_dir, study_dir]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def save_best_model(study, trial):\n",
    "    # If the trial is better than the current best, save its model weights\n",
    "    if study.best_trial.number == trial.number:\n",
    "        # Retrieve model's parameters from the trial\n",
    "        trial_params = trial.params\n",
    "        input_dim = FEATURE_DIM\n",
    "        hidden_dim = trial_params[\"HIDDEN_DIM\"]\n",
    "        num_output = OUTPUT_DIM\n",
    "        \n",
    "        # Initialize the model with the trial's parameters\n",
    "        model = SimpleNN(input_dim, hidden_dim, num_output)\n",
    "        \n",
    "        # Load the model's state_dict from the saved path\n",
    "        saved_model_path = trial.user_attrs[\"model_path\"]\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "        \n",
    "        # Save the model's state_dict as the best model\n",
    "        best_model_path = os.path.join(model_dir, \\\n",
    "            f\"best_model_trial_{trial.number}.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        # Delete previous best model file if exists and is different from the current one\n",
    "        previous_best_model = os.path.join(model_dir, \\\n",
    "            f\"best_model_trial_{study.best_trial.number - 1}.pt\")\n",
    "        if os.path.exists(previous_best_model):\n",
    "            os.remove(previous_best_model)\n",
    "        \n",
    "    # Remove the saved model of the current trial since it's not the best\n",
    "    current_trial_model_path = os.path.join(study_dir, f\"model_{trial.number}.pt\")\n",
    "    if os.path.exists(current_trial_model_path):\n",
    "        os.remove(current_trial_model_path)\n",
    "\n",
    "# Your objective function remains untouched\n",
    "def objective(trial):\n",
    "    # 1. Define hyperparameters to optimize\n",
    "    HIDDEN_DIM = trial.suggest_int(\"HIDDEN_DIM\", 32, 512, log=True)  # Log scale search for hidden dimensions\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)  # Log scale search for learning rate\n",
    "    step_size = trial.suggest_int(\"step_size\", 50, 200)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.01, 0.5, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 3, 10)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "    # 2. Instantiate model and other components using hyperparameters\n",
    "    model = SimpleNN(FEATURE_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "    model.to(device=device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    val_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(train_data):\n",
    "        train = train_data.loc[train_idx, :]\n",
    "        val = train_data.loc[val_idx, :]\n",
    "\n",
    "        # Convert your data to appropriate loaders here\n",
    "        train_loader = get_era2data(train, features, target)\n",
    "        val_loader = get_era2data(val, features, target)\n",
    "\n",
    "        _, batch_matric, _ = train_model(model, criterion, optimizer, \\\n",
    "                scheduler, num_epochs, patience, train_loader, val_loader)\n",
    "\n",
    "        metrics = metrics_on_batch(batch_matric)\n",
    "        val_scores.append(metrics[2])\n",
    "\n",
    "    model_path = os.path.join(study_dir, f\"model_{trial.number}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Set the file path as a user attribute (optional)\n",
    "    trial.set_user_attr(\"model_path\", model_path)\n",
    "\n",
    "    return -np.mean(val_scores)\n",
    "\n",
    "# Callback to provide feedback about each trial's end and the current best trial\n",
    "def callback(study, trial):\n",
    "    print(f\"\\n--- Trial {trial.number} finished ---\")\n",
    "    print(f\"Value: {trial.value} and parameters: {trial.params}\")\n",
    "    print(f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}\\n\")\n",
    "\n",
    "# Initialize the study\n",
    "study = optuna.create_study(study_name='Maximizing the Sharpe', direction='minimize',\n",
    "                            storage=f'sqlite:///{study_dir}/study.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=1, callbacks=[save_best_model, callback])\n",
    "\n",
    "# # Print best trial's parameters and value\n",
    "# print(f\"Best trial: {study.best_trial.params}\")\n",
    "# print(f\"Best value: {study.best_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(train_data, features, target):\n",
    "    # 1. Get the best parameters from the study\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # 2. Initialize model with best parameters\n",
    "    HIDDEN_DIM = best_params[\"HIDDEN_DIM\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    step_size = best_params[\"step_size\"]\n",
    "    gamma = best_params[\"gamma\"]\n",
    "    num_epochs = best_params[\"num_epochs\"]\n",
    "\n",
    "    model = SimpleNN(FEATURE_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "    model.to(device=device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # 3. Train the model with all the data\n",
    "    train_loader = get_era2data(train_data, features, target)\n",
    "\n",
    "    # Note: Since you're training on all data, there's no validation loader.\n",
    "    # However, the `train_model` function you've provided seems to require one.\n",
    "    # You can create a dummy one or modify the `train_model` to handle cases where \n",
    "    # there's no validation set.\n",
    "\n",
    "    dummy_val_loader = []  # This is a dummy validation loader since we're training on all data\n",
    "\n",
    "    train_loss, _, _ = train_model(model, criterion, optimizer, \\\n",
    "        scheduler, num_epochs, patience=None, train_loader=train_loader, \\\n",
    "        val_loader=dummy_val_loader)\n",
    "\n",
    "    # Save the model after training\n",
    "    final_model_path = os.path.join(model_dir, \"final_best_model.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Now, call the function to train your model with the best parameters on all data\n",
    "final_model = train_best_model(train_data, features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
