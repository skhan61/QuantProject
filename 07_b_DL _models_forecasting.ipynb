{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 117250 entries, (Timestamp('2020-09-30 00:00:00'), 'AA') to (Timestamp('2022-08-02 00:00:00'), 'ZTS')\n",
      "Columns: 622 entries, FEATURE_open to TARGET_ret_fwd_252d_rank_quantiled\n",
      "dtypes: float32(360), float64(43), int32(198), int64(12), int8(9)\n",
      "memory usage: 300.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from utils import rank_stocks_and_quantile\n",
    "\n",
    "top = 250  # parameters -> papermill\n",
    "\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_keys = [\n",
    "    '/data/YEAR_20200930_20220802',\n",
    "    # '/data/YEAR_20181024_20200929',\n",
    "    # '/data/YEAR_20161116_20181023',\n",
    "    # '/data/YEAR_20141210_20161115'\n",
    "]\n",
    "target_string = 'TARGET_ret_fwd'  # no longer a parameter\n",
    "\n",
    "# Initialize empty dataset\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    for key in dataset_keys:\n",
    "        df = store[key]\n",
    "        dataset = pd.concat([dataset, df], ignore_index=False)\n",
    "        del df\n",
    "        gc.collect()  # Explicitly call garbage collector\n",
    "\n",
    "# Rank stocks and quantile\n",
    "dataset = rank_stocks_and_quantile(dataset, target_substring=target_string)\n",
    "\n",
    "# Adjust timezone\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of business days between 2020-09-30 00:00:00 and 2022-08-02 00:00:00: 480\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the first and last dates from the MultiIndex\n",
    "start_date = dataset.index.get_level_values(0).min()\n",
    "end_date = dataset.index.get_level_values(0).max()\n",
    "\n",
    "# Generate business dates between the start and end date\n",
    "business_dates = pd.bdate_range(start_date, end_date)\n",
    "\n",
    "# Count the number of business days\n",
    "num_business_days = len(business_dates)\n",
    "\n",
    "print(f\"Number of business days between {start_date} and {end_date}: {num_business_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique dates and sort them\n",
    "unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "\n",
    "# Adjust for the look-ahead gap\n",
    "look_ahead = 1\n",
    "\n",
    "# Split dates for training and testing with a gap\n",
    "train_dates = unique_dates[:-21-look_ahead]\n",
    "test_dates = unique_dates[-21:]\n",
    "\n",
    "# Split the dataset\n",
    "train_data = dataset.loc[train_dates] ## train + val\n",
    "test_data = dataset.loc[test_dates] # backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(train_period_length=142, \n",
    "                                        test_period_length=21, \n",
    "                                        lookahead=1, \n",
    "                                        date_idx='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from models import SimpleNN\n",
    "from utils import get_era2data, train_model, metrics_on_batch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FEATURE_DIM = 225\n",
    "OUTPUT_DIM = 1\n",
    "MAX_LEN = 500\n",
    "\n",
    "# Define paths for saving models and hyperparameters\n",
    "model_dir = \"./saved_models\"\n",
    "best_hyperparams_dir = \"./best_hyperparams\"\n",
    "study_dir = \"./optuna_studies\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir in [model_dir, best_hyperparams_dir, study_dir]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def save_best_model(study, trial):\n",
    "    # If the trial is better than the current best, save its model weights\n",
    "    if study.best_trial.number == trial.number:\n",
    "        # Retrieve model's parameters from the trial\n",
    "        trial_params = trial.params\n",
    "        input_dim = FEATURE_DIM\n",
    "        hidden_dim = trial_params[\"HIDDEN_DIM\"]\n",
    "        num_output = OUTPUT_DIM\n",
    "        \n",
    "        # Initialize the model with the trial's parameters\n",
    "        model = SimpleNN(input_dim, hidden_dim, num_output)\n",
    "        \n",
    "        # Load the model's state_dict from the saved path\n",
    "        saved_model_path = trial.user_attrs[\"model_path\"]\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "        \n",
    "        # Save the model's state_dict as the best model\n",
    "        best_model_path = os.path.join(model_dir, \\\n",
    "            f\"best_model_trial_{trial.number}.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        # Delete previous best model file if exists and is different from the current one\n",
    "        previous_best_model = os.path.join(model_dir, \\\n",
    "            f\"best_model_trial_{study.best_trial.number - 1}.pt\")\n",
    "        if os.path.exists(previous_best_model):\n",
    "            os.remove(previous_best_model)\n",
    "        \n",
    "    # Remove the saved model of the current trial since it's not the best\n",
    "    current_trial_model_path = os.path.join(study_dir, f\"model_{trial.number}.pt\")\n",
    "    if os.path.exists(current_trial_model_path):\n",
    "        os.remove(current_trial_model_path)\n",
    "\n",
    "# Your objective function remains untouched\n",
    "def objective(trial):\n",
    "    # 1. Define hyperparameters to optimize\n",
    "    HIDDEN_DIM = trial.suggest_int(\"HIDDEN_DIM\", 32, 512, log=True)  # Log scale search for hidden dimensions\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)  # Log scale search for learning rate\n",
    "    step_size = trial.suggest_int(\"step_size\", 50, 200)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.01, 0.5, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 3, 10)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 10)\n",
    "\n",
    "    # 2. Instantiate model and other components using hyperparameters\n",
    "    model = SimpleNN(FEATURE_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "    model.to(device=device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    val_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(train_data):\n",
    "        train = train_data.loc[train_idx, :]\n",
    "        val = train_data.loc[val_idx, :]\n",
    "\n",
    "        # Convert your data to appropriate loaders here\n",
    "        train_loader = get_era2data(train, features, target)\n",
    "        val_loader = get_era2data(val, features, target)\n",
    "\n",
    "        _, batch_matric, _ = train_model(model, criterion, optimizer, \\\n",
    "                scheduler, num_epochs, patience, train_loader, val_loader)\n",
    "\n",
    "        metrics = metrics_on_batch(batch_matric)\n",
    "        val_scores.append(metrics[2])\n",
    "\n",
    "    model_path = os.path.join(study_dir, f\"model_{trial.number}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Set the file path as a user attribute (optional)\n",
    "    trial.set_user_attr(\"model_path\", model_path)\n",
    "\n",
    "    return -np.mean(val_scores)\n",
    "\n",
    "# Callback to provide feedback about each trial's end and the current best trial\n",
    "def callback(study, trial):\n",
    "    print(f\"\\n--- Trial {trial.number} finished ---\")\n",
    "    print(f\"Value: {trial.value} and parameters: {trial.params}\")\n",
    "    print(f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}\\n\")\n",
    "\n",
    "# Initialize the study\n",
    "study = optuna.create_study(study_name='Maximizing the Sharpe', direction='minimize',\n",
    "                            storage=f'sqlite:///{study_dir}/study.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=1, callbacks=[save_best_model, callback])\n",
    "\n",
    "# # Print best trial's parameters and value\n",
    "# print(f\"Best trial: {study.best_trial.params}\")\n",
    "# print(f\"Best value: {study.best_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(train_data, features, target):\n",
    "    # 1. Get the best parameters from the study\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # 2. Initialize model with best parameters\n",
    "    HIDDEN_DIM = best_params[\"HIDDEN_DIM\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    step_size = best_params[\"step_size\"]\n",
    "    gamma = best_params[\"gamma\"]\n",
    "    num_epochs = best_params[\"num_epochs\"]\n",
    "\n",
    "    model = SimpleNN(FEATURE_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "    model.to(device=device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # 3. Train the model with all the data\n",
    "    train_loader = get_era2data(train_data, features, target)\n",
    "\n",
    "    # Note: Since you're training on all data, there's no validation loader.\n",
    "    # However, the `train_model` function you've provided seems to require one.\n",
    "    # You can create a dummy one or modify the `train_model` to handle cases where \n",
    "    # there's no validation set.\n",
    "\n",
    "    dummy_val_loader = []  # This is a dummy validation loader since we're training on all data\n",
    "\n",
    "    train_loss, _, _ = train_model(model, criterion, optimizer, \\\n",
    "        scheduler, num_epochs, patience=None, train_loader=train_loader, \\\n",
    "        val_loader=dummy_val_loader)\n",
    "\n",
    "    # Save the model after training\n",
    "    final_model_path = os.path.join(model_dir, \"final_best_model.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Now, call the function to train your model with the best parameters on all data\n",
    "final_model = train_best_model(train_data, features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
