{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths to the downloaded datasets, model, and hyperparameters\n",
    "data_dir = Path('data/')\n",
    "model_dir = Path('model/')\n",
    "best_hyperparams_dir = Path('best_hyperparams/')\n",
    "study_dir = Path('study/')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_hyperparams_dir.mkdir(parents=True, exist_ok=True)\n",
    "study_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils import rank_stocks_and_quantile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Parameters and data paths\n",
    "top = 250\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_keys = [\n",
    "    '/data/YEAR_20200930_20220802',\n",
    "    # '/data/YEAR_20181024_20200929',\n",
    "    # '/data/YEAR_20161116_20181023',\n",
    "    # '/data/YEAR_20141210_20161115'\n",
    "]\n",
    "target_string = 'TARGET_ret_fwd'\n",
    "CHUNK_SIZE = 50000\n",
    "\n",
    "def convert_dtype(chunk, feature_columns, dtype='float32'):\n",
    "    \"\"\"Converts the datatype of the specified columns.\"\"\"\n",
    "    chunk[feature_columns] = chunk[feature_columns].astype(dtype)\n",
    "    return chunk\n",
    "\n",
    "def handle_infinite_values(chunk, feature_columns):\n",
    "    \"\"\"Handle infinite values by replacing them with the maximum finite value.\"\"\"\n",
    "    max_val = np.finfo('float32').max\n",
    "    chunk[feature_columns] = chunk[feature_columns].replace([np.inf, -np.inf], max_val)\n",
    "    return chunk\n",
    "\n",
    "def process_chunk(chunk, feature_columns, scaler=None):\n",
    "    \"\"\"Process a single chunk of data.\"\"\"\n",
    "    chunk = convert_dtype(chunk, feature_columns)\n",
    "    chunk = handle_infinite_values(chunk, feature_columns)\n",
    "    \n",
    "    # Normalize with scaler if provided\n",
    "    if scaler:\n",
    "        chunk[feature_columns] = scaler.transform(chunk[feature_columns])\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "# Identify features and targets based on the first chunk\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    first_chunk = store.select(dataset_keys[0], stop=CHUNK_SIZE)\n",
    "    features = [col for col in first_chunk.columns if col.startswith('FEATURE_')]\n",
    "    target = [col for col in first_chunk.columns if col.startswith('TARGET_')]\n",
    "\n",
    "# Determine the scaler using the entire dataset for the identified features\n",
    "scaler = MinMaxScaler()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            # Convert dtype and handle infinite values\n",
    "            chunk = convert_dtype(chunk, features)\n",
    "            chunk = handle_infinite_values(chunk, features)\n",
    "            scaler.partial_fit(chunk[features])\n",
    "\n",
    "# Process and concatenate chunks\n",
    "dataset = pd.DataFrame()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            processed_chunk = process_chunk(chunk, features, scaler)\n",
    "            dataset = pd.concat([dataset, processed_chunk], ignore_index=False)\n",
    "            del processed_chunk\n",
    "            gc.collect()\n",
    "\n",
    "# Post-processing steps\n",
    "dataset = rank_stocks_and_quantile(dataset, target_substring=target_string)\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.head(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = None  # If you have a predefined value, set it here; otherwise, it gets calculated automatically.\n",
    "\n",
    "def pad_sequence(inputs, padding_value=-1, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([input.shape[0] for input in inputs])\n",
    "    padded_inputs = []\n",
    "    masks = []\n",
    "    for input in inputs:\n",
    "        pad_len = max_len - input.shape[0]\n",
    "        padded_input = F.pad(input, (0, 0, 0, pad_len), value=padding_value)\n",
    "        mask = torch.ones((input.shape[0], 1), dtype=torch.float)\n",
    "        masks.append(\n",
    "            torch.cat((mask, torch.zeros((pad_len, 1), dtype=torch.float)), dim=0)\n",
    "        )\n",
    "        padded_inputs.append(padded_input)\n",
    "    return torch.stack(padded_inputs), torch.stack(masks)\n",
    "\n",
    "def convert_to_torch(timestamp, data):\n",
    "    feature_names = [col for col in data.columns if col.startswith('FEATURE_')]\n",
    "    target_names = [col for col in data.columns if col.startswith('TARGET_')]\n",
    "    \n",
    "    inputs = torch.from_numpy(\n",
    "                data[feature_names].values.astype(np.float32))\n",
    "    labels = torch.from_numpy(\n",
    "                data[target_names].values.astype(np.float32))\n",
    "\n",
    "    padded_inputs, masks_inputs = pad_sequence(\n",
    "            [inputs], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "    padded_labels, masks_labels = pad_sequence(\n",
    "            [labels], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "\n",
    "    return {\n",
    "        timestamp: (\n",
    "            padded_inputs,\n",
    "            padded_labels,\n",
    "            masks_inputs\n",
    "        )\n",
    "    }\n",
    "\n",
    "def get_era2data(df):\n",
    "    # Group by the Timestamp index (level=0)\n",
    "    res = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "        delayed(convert_to_torch)(timestamp, data)\n",
    "        for timestamp, data in tqdm(df.groupby(level=0)))\n",
    "    \n",
    "    era2data = {}\n",
    "    for r in tqdm(res):\n",
    "        era2data.update(r)\n",
    "    return era2data\n",
    "\n",
    "# # Assuming your DataFrame is named \"dataset\"\n",
    "# timestamp2data_dataset = get_era2data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 6000, 586])\n",
      "Output Shape: torch.Size([2, 6000, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = 6000\n",
    "FEATURE_DIM = len(features)\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(target)\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "from model import Transformer\n",
    "\n",
    "def test_model():\n",
    "\n",
    "    inputs = [\n",
    "        torch.randint(0, 4, (5, FEATURE_DIM)).float(),\n",
    "        torch.randint(0, 4, (3, FEATURE_DIM)).float(),\n",
    "    ]\n",
    "    labels = [\n",
    "        torch.randint(0, 2, (5, OUTPUT_DIM)).float(),\n",
    "        torch.randint(0, 2, (3, OUTPUT_DIM)).float(),\n",
    "    ]\n",
    "\n",
    "    padded_inputs, masks_inputs = pad_sequence(inputs, \\\n",
    "                                               padding_value=0, max_len=MAX_LEN)\n",
    "    padded_labels, masks_labels = pad_sequence(labels, \\\n",
    "                                               padding_value=0, max_len=MAX_LEN)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        input_dim=FEATURE_DIM,\n",
    "        d_model=HIDDEN_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        max_len=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(padded_inputs, masks_inputs)\n",
    "\n",
    "    assert torch.isnan(outputs).sum() == 0\n",
    "    assert outputs.shape[:2] == padded_inputs.shape[:2]\n",
    "    assert outputs.shape[-1] == len(target)\n",
    "\n",
    "    print(\"Input Shape:\", padded_inputs.shape)\n",
    "    print(\"Output Shape:\", outputs.shape)\n",
    "\n",
    "    del transformer\n",
    "    del inputs, labels\n",
    "    del padded_inputs, masks_inputs, padded_labels, masks_labels\n",
    "    del outputs\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearsonr in torch differentiable\n",
    "def pearsonr(x, y):\n",
    "    mx = x.mean()\n",
    "    my = y.mean()\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = torch.sum(xm * ym)\n",
    "    r_den = torch.sqrt(torch.sum(xm ** 2) * torch.sum(ym ** 2))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                padded_inputs=None, target_weight_softmax=None):\n",
    "\n",
    "    # MSE on all targets; additionally, on primary target\n",
    "    if target_weight_softmax is not None:\n",
    "        _mse = criterion(\n",
    "            outputs * masks_inputs * target_weight_softmax,\n",
    "            padded_labels * masks_inputs * target_weight_softmax\n",
    "        ) * 0.1\n",
    "\n",
    "    else:\n",
    "        _mse = criterion(outputs * masks_inputs, padded_labels * masks_inputs) * 0.1\n",
    "\n",
    "    _mse += criterion(outputs[:, 0] * masks_inputs, padded_labels[:, 0] * masks_inputs)\n",
    "\n",
    "    # Corr with only primary target; adjust as needed\n",
    "    corr = pearsonr(\n",
    "        outputs[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "        padded_labels[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "    )\n",
    "\n",
    "    loss = _mse - corr #+ some_complex_constraints\n",
    "    return loss, _mse, corr\n",
    "\n",
    "# Training loop\n",
    "def train_on_batch(transformer, criterion, optimizer, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = transformer(padded_inputs / 4.0, masks_inputs)\n",
    "    # print(outputs)\n",
    "\n",
    "    target_weight_softmax = None\n",
    "    #random_weights = torch.rand(padded_labels.shape[-1], device=device)\n",
    "    #target_weight_softmax = F.softmax(random_weights)\n",
    "\n",
    "    loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                                       target_weight_softmax=target_weight_softmax)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), _mse.item(), _corr.item()\n",
    "\n",
    "\n",
    "def evaluate_on_batch(transformer, criterion, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(padded_inputs / 4.0, masks_inputs)\n",
    "        # print(outputs)\n",
    "        loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs)\n",
    "        \n",
    "        # Convert outputs to numpy\n",
    "        preds = outputs[0][masks_inputs.view(-1).nonzero()].squeeze(1).cpu().numpy()\n",
    "        # print(preds)\n",
    "\n",
    "    return loss.item(), _mse.item(), _corr.item(), preds\n",
    "\n",
    "\n",
    "def metrics_on_batch(era_scores):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max() # from calculate_metrics\n",
    "\n",
    "    # Smart Sharpe: Modified Sharpe ratio that also considers the instability of scores over time,\n",
    "    # penalizing models with high score instability even if their mean score is high\n",
    "    smart_sharpe = mean_correlation / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation: Measure of the correlation of the series with a lagged version of itself\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd, # added from calculate_metrics\n",
    "        'min_correlation': era_scores.min(), # added from calculate_metrics\n",
    "        'max_correlation': era_scores.max(), # added from calculate_metrics\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(transformer, criterion, optimizer, scheduler, \\\n",
    "                num_epochs, patience, train_loader, val_loader, is_lr_scheduler=True):\n",
    "    best_loss = float('inf')\n",
    "    best_corr = None\n",
    "    best_model = None\n",
    "    best_outputs = None\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    epoch_progress = tqdm(range(num_epochs), desc=\"Epochs\", position=0, leave=False)\n",
    "\n",
    "    for epoch in epoch_progress:\n",
    "        total_loss = []\n",
    "        total_corr = []\n",
    "\n",
    "        # Training\n",
    "        for era_num in tqdm(train_loader, desc=\"Training\", leave=False, position=1):\n",
    "            batch = train_loader[era_num]\n",
    "            loss, _mse, _corr = train_on_batch(transformer, criterion, optimizer, batch)\n",
    "            total_loss.append(loss)\n",
    "            total_corr.append(_corr)\n",
    "\n",
    "        # Adjust learning rate if is_lr_scheduler is True\n",
    "        if is_lr_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        transformer.eval()\n",
    "        val_total_loss = []\n",
    "        val_total_corr = []\n",
    "        val_total_outputs = {}\n",
    "        with torch.no_grad():\n",
    "            for era_num in tqdm(val_loader, desc=\"Validation\", leave=False, position=2):\n",
    "                batch = val_loader[era_num]\n",
    "                loss, _mse, _corr, outputs = evaluate_on_batch(transformer, criterion, batch)\n",
    "                # print(outputs)\n",
    "                val_total_loss.append(loss)\n",
    "                val_total_corr.append(_corr)\n",
    "                val_total_outputs[era_num] = outputs\n",
    "\n",
    "        # Early stopping check\n",
    "        val_loss = np.mean(val_total_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_corr = val_total_corr.copy()\n",
    "            best_model = transformer.state_dict().copy()\n",
    "            best_outputs = val_total_outputs.copy()\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            no_improve_epoch += 1\n",
    "            if no_improve_epoch >= patience:\n",
    "                epoch_progress.set_description(f'Early stopping at epoch {epoch+1}')\n",
    "                epoch_progress.refresh()\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        _ = gc.collect()\n",
    "\n",
    "    # Save the best model state\n",
    "    torch.save(best_model, data_dir / \"transformer_best.pth\")\n",
    "\n",
    "    return transformer, best_corr, best_outputs # best_outputs for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-06 17:23:48,281] A new study created in RDB with name: Maximizing the Sharpe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Trial: 1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f0dc6cb4c14cc7bf8de56651f9c01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4abdb432c947e8a14183556718ed99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682df23f712a4125ab960fcb8c91143f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3ec680697460f911078c3291c8b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b51748dcefb40eabc603e78aa01bc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f59787a543a47c0bfdd353f5a98596c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-10-06 17:23:49,184] Trial 0 failed with parameters: {'num_heads': 4, 'hidden_dim': 240, 'num_layers': 5, 'learning_rate': 0.000201614661048267} because of the following error: RuntimeError('The size of tensor a (12) must match the size of tensor b (36) at non-singleton dimension 2').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sayem/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_143574/1737206000.py\", line 60, in objective\n",
      "    transformer, best_corr, outputs = train_model(transformer, criterion, optimizer, scheduler,\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_143574/2781623084.py\", line 20, in train_model\n",
      "    loss, _mse, _corr = train_on_batch(transformer, criterion, optimizer, batch)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_143574/3563533369.py\", line 41, in train_on_batch\n",
      "    loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_143574/3563533369.py\", line 12, in calculate_loss\n",
      "    _mse = criterion(outputs * masks_inputs, padded_labels * masks_inputs) * 0.1\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sayem/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sayem/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py\", line 536, in forward\n",
      "    return F.mse_loss(input, target, reduction=self.reduction)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sayem/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py\", line 3294, in mse_loss\n",
      "    expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sayem/anaconda3/lib/python3.11/site-packages/torch/functional.py\", line 74, in broadcast_tensors\n",
      "    return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: The size of tensor a (12) must match the size of tensor b (36) at non-singleton dimension 2\n",
      "[W 2023-10-06 17:23:49,185] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (36) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_b_DL _models_forecasting.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m             json\u001b[39m.\u001b[39mdump(trial\u001b[39m.\u001b[39mparams, f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMaximizing the Sharpe\u001b[39m\u001b[39m'\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m                             storage\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqlite:///\u001b[39m\u001b[39m{\u001b[39;00mstudy_dir\u001b[39m}\u001b[39;00m\u001b[39m/study.db\u001b[39m\u001b[39m'\u001b[39m, load_if_exists\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(objective, n_trials\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, callbacks\u001b[39m=\u001b[39m[callback])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     _optimize(\n\u001b[1;32m    443\u001b[0m         study\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    444\u001b[0m         func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[1;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[1;32m    448\u001b[0m         catch\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(catch) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(catch, Iterable) \u001b[39melse\u001b[39;00m (catch,),\n\u001b[1;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39mgc_after_trial,\n\u001b[1;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_b_DL _models_forecasting.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m era2data_train \u001b[39m=\u001b[39m get_era2data(train_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m era2data_validation \u001b[39m=\u001b[39m get_era2data(test_data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m transformer, best_corr, outputs \u001b[39m=\u001b[39m train_model(transformer, criterion, optimizer, scheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                                               NUM_EPOCHS, PATIENCE, era2data_train, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                               era2data_validation, is_lr_scheduler\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m metrics \u001b[39m=\u001b[39m metrics_on_batch(best_corr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m sharpe_ratios\u001b[39m.\u001b[39mappend(metrics[\u001b[39m'\u001b[39m\u001b[39msharpe_ratio\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_b_DL _models_forecasting.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m era_num \u001b[39min\u001b[39;00m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, position\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     batch \u001b[39m=\u001b[39m train_loader[era_num]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss, _mse, _corr \u001b[39m=\u001b[39m train_on_batch(transformer, criterion, optimizer, batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     total_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     total_corr\u001b[39m.\u001b[39mappend(_corr)\n",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_b_DL _models_forecasting.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m target_weight_softmax \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#random_weights = torch.rand(padded_labels.shape[-1], device=device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m#target_weight_softmax = F.softmax(random_weights)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss, _mse, _corr \u001b[39m=\u001b[39m calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                                    target_weight_softmax\u001b[39m=\u001b[39mtarget_weight_softmax)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/sayem/Desktop/Project/07_b_DL _models_forecasting.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     _mse \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         outputs \u001b[39m*\u001b[39m masks_inputs \u001b[39m*\u001b[39m target_weight_softmax,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         padded_labels \u001b[39m*\u001b[39m masks_inputs \u001b[39m*\u001b[39m target_weight_softmax\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     ) \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     _mse \u001b[39m=\u001b[39m criterion(outputs \u001b[39m*\u001b[39m masks_inputs, padded_labels \u001b[39m*\u001b[39m masks_inputs) \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m _mse \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(outputs[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m masks_inputs, padded_labels[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m masks_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/07_b_DL%20_models_forecasting.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Corr with only primary target; adjust as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmse_loss(\u001b[39minput\u001b[39m, target, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[1;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (36) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "PATIENCE = 5\n",
    "\n",
    "def objective(trial, dataset=dataset):\n",
    "    print(f\"\\n--- Starting Trial: {trial.number + 1} ---\")\n",
    "    \n",
    "    # Instantiate CV object inside the objective function\n",
    "    cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length=10, \n",
    "                                            test_period_length=21, \n",
    "                                            lookahead=1,  # Starting value\n",
    "                                            date_idx='date')\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 1, 5)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256, step=2)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        input_dim=FEATURE_DIM,\n",
    "        d_model=hidden_dim,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "    )\n",
    "    transformer.to(device=device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "    sharpe_ratios = []\n",
    "\n",
    "    # Update the CV's lookahead based on the trial's suggestion\n",
    "    look_ahead = 1\n",
    "    cv.update_lookahead(look_ahead)\n",
    "\n",
    "    # Loop through train and test splits from the CV\n",
    "    for train_idx, test_idx in cv:\n",
    "        train_data = dataset.iloc[train_idx]\n",
    "        test_data = dataset.iloc[test_idx]\n",
    "\n",
    "        # Convert train_data and test_data to the desired format\n",
    "        era2data_train = get_era2data(train_data)\n",
    "        era2data_validation = get_era2data(test_data)\n",
    "\n",
    "        transformer, best_corr, outputs = train_model(transformer, criterion, optimizer, scheduler,\n",
    "                                                      NUM_EPOCHS, PATIENCE, era2data_train, \n",
    "                                                      era2data_validation, is_lr_scheduler=True)\n",
    "\n",
    "        metrics = metrics_on_batch(best_corr)\n",
    "        sharpe_ratios.append(metrics['sharpe_ratio'])\n",
    "\n",
    "    trial.set_user_attr(\"transformer\", transformer)\n",
    "    \n",
    "    return -np.mean(sharpe_ratios)  # average negative sharpe ratio over all splits\n",
    "\n",
    "# Callback to save the best model and its hyperparameters after each trial\n",
    "def callback(study, trial):\n",
    "    print(f\"\\n--- Trial {trial.number + 1} finished ---\")\n",
    "    print(f\"Value: {trial.value} and parameters: {trial.params}\")\n",
    "    print(f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}\\n\")\n",
    "    \n",
    "    if study.best_trial.number == trial.number:\n",
    "        best_model = trial.user_attrs[\"transformer\"]\n",
    "        torch.save(best_model, model_dir / \"best_model.pth\")\n",
    "        with open(best_hyperparams_dir / \"best_params.json\", 'w') as f:\n",
    "            json.dump(trial.params, f)\n",
    "\n",
    "study = optuna.create_study(study_name='Maximizing the Sharpe', direction='minimize',\n",
    "                            storage=f'sqlite:///{study_dir}/study.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=25, callbacks=[callback])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "look_ahead = 1\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length = 10, \n",
    "                                    test_period_length = 21, \n",
    "                                    lookahead=1,  # Starting value; we'll adjust it next.\n",
    "                                    date_idx='date')\n",
    "\n",
    "# look_ahead = 1\n",
    "\n",
    "# Update the CV's lookahead based on the trial's suggestion\n",
    "cv.update_lookahead(look_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in cv:\n",
    "    # Dynamic target based on the suggested lookahead\n",
    "    label = f'TARGET_ret_fwd_{look_ahead:02d}d_rank_quantiled'\n",
    "\n",
    "    train = dataset.loc[train_idx]\n",
    "    validation = dataset.loc[val_idx]\n",
    "\n",
    "    era2data_train = get_era2data(train)\n",
    "    era2data_validation = get_era2data(validation)\n",
    "    # train_labels = dataset.loc[train_idx, label]\n",
    "\n",
    "    # val_features = dataset.loc[val_idx, features]\n",
    "    # val_labels = dataset.loc[val_idx, label]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
