{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths to the downloaded datasets, model, and hyperparameters\n",
    "data_dir = Path('data/')\n",
    "model_dir = Path('models/')\n",
    "best_hyperparams_dir = Path('best_hyperparams/')\n",
    "study_dir = Path('study/')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_hyperparams_dir.mkdir(parents=True, exist_ok=True)\n",
    "study_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process Large Financial Datasets from HDF5 Format.\n",
    "\n",
    "This script reads, processes, and normalizes financial datasets stored in an HDF5 format.\n",
    "The primary processing steps involve converting data types, handling infinite values, and\n",
    "scaling the dataset. The MinMaxScaler, computed from the entire dataset, is employed for normalization.\n",
    "Once data processing is complete, stocks are ranked, and quantiles are determined in post-processing.\n",
    "\n",
    "Attributes:\n",
    "    - top (int): Number of top stocks to consider.\n",
    "    - DATA_STORE (Path): Path to the HDF5 file containing the datasets.\n",
    "    - dataset_keys (list of str): Keys identifying which datasets to process in the HDF5 store.\n",
    "    - target_string (str): Target column identifier for post-processing.\n",
    "    - CHUNK_SIZE (int): Size of chunks in which data is read and processed.\n",
    "\n",
    "Functions:\n",
    "    - convert_dtype(chunk, feature_columns, dtype='float32'): Converts dtype of specified columns in a chunk.\n",
    "    - handle_infinite_values(chunk, feature_columns): Handles infinite values in a chunk.\n",
    "    - process_chunk(chunk, feature_columns, scaler=None): Process a single chunk with optional normalization.\n",
    "\n",
    "Workflow:\n",
    "    1. Set parameters and paths.\n",
    "    2. Define utility functions.\n",
    "    3. Identify features and target columns from the first chunk.\n",
    "    4. Determine the MinMaxScaler using all chunks in the dataset.\n",
    "    5. Process and concatenate chunks to form the dataset.\n",
    "    6. Rank stocks and compute quantiles in post-processing.\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils import rank_stocks_and_quantile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Parameters and data paths\n",
    "TOP = top = 250\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_keys = [\n",
    "    '/data/YEAR_20200930_20220802',\n",
    "    '/data/YEAR_20181024_20200929',\n",
    "    '/data/YEAR_20161116_20181023',\n",
    "    '/data/YEAR_20141210_20161115'\n",
    "]\n",
    "target_string = 'TARGET_ret_fwd'\n",
    "CHUNK_SIZE = 50000\n",
    "\n",
    "def convert_dtype(chunk, feature_columns, dtype='float32'):\n",
    "    \"\"\"Converts the datatype of the specified columns.\"\"\"\n",
    "    chunk[feature_columns] = chunk[feature_columns].astype(dtype)\n",
    "    return chunk\n",
    "\n",
    "def handle_infinite_values(chunk, feature_columns):\n",
    "    \"\"\"Handle infinite values by replacing them with the maximum finite value.\"\"\"\n",
    "    max_val = np.finfo('float32').max\n",
    "    chunk[feature_columns] = chunk[feature_columns].replace([np.inf, -np.inf], max_val)\n",
    "    return chunk\n",
    "\n",
    "def process_chunk(chunk, feature_columns, scaler=None):\n",
    "    \"\"\"Process a single chunk of data.\"\"\"\n",
    "    chunk = convert_dtype(chunk, feature_columns)\n",
    "    chunk = handle_infinite_values(chunk, feature_columns)\n",
    "    \n",
    "    # Normalize with scaler if provided\n",
    "    if scaler:\n",
    "        chunk[feature_columns] = scaler.transform(chunk[feature_columns])\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "# Identify features and targets based on the first chunk\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    first_chunk = store.select(dataset_keys[0], stop=CHUNK_SIZE)\n",
    "    features = [col for col in first_chunk.columns if col.startswith('FEATURE_')]\n",
    "    target = [col for col in first_chunk.columns if col.startswith('TARGET_')]\n",
    "\n",
    "# Determine the scaler using the entire dataset for the identified features\n",
    "scaler = MinMaxScaler()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            # Convert dtype and handle infinite values\n",
    "            chunk = convert_dtype(chunk, features)\n",
    "            chunk = handle_infinite_values(chunk, features)\n",
    "            scaler.partial_fit(chunk[features])\n",
    "\n",
    "# Process and concatenate chunks\n",
    "dataset = pd.DataFrame()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            processed_chunk = process_chunk(chunk, features, scaler)\n",
    "            dataset = pd.concat([dataset, processed_chunk], ignore_index=False)\n",
    "            del processed_chunk\n",
    "            gc.collect()\n",
    "\n",
    "# Post-processing steps\n",
    "dataset = rank_stocks_and_quantile(dataset, target_substring=target_string)\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = None  # If you have a predefined value, set it here; otherwise, it gets calculated automatically.\n",
    "\n",
    "def pad_sequence(inputs, padding_value=-1, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([input.shape[0] for input in inputs])\n",
    "    padded_inputs = []\n",
    "    masks = []\n",
    "    for input in inputs:\n",
    "        pad_len = max_len - input.shape[0]\n",
    "        padded_input = F.pad(input, (0, 0, 0, pad_len), value=padding_value)\n",
    "        mask = torch.ones((input.shape[0], 1), dtype=torch.float)\n",
    "        masks.append(\n",
    "            torch.cat((mask, torch.zeros((pad_len, 1), dtype=torch.float)), dim=0)\n",
    "        )\n",
    "        padded_inputs.append(padded_input)\n",
    "    return torch.stack(padded_inputs), torch.stack(masks)\n",
    "\n",
    "def convert_to_torch(timestamp, data):\n",
    "    feature_names = [col for col in data.columns if col.startswith('FEATURE_')]\n",
    "    target_names = [col for col in data.columns if col.startswith('TARGET_')]\n",
    "    \n",
    "    inputs = torch.from_numpy(\n",
    "                data[feature_names].values.astype(np.float32))\n",
    "    labels = torch.from_numpy(\n",
    "                data[target_names].values.astype(np.float32))\n",
    "\n",
    "    padded_inputs, masks_inputs = pad_sequence(\n",
    "            [inputs], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "    padded_labels, masks_labels = pad_sequence(\n",
    "            [labels], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "\n",
    "    return {\n",
    "        timestamp: (\n",
    "            padded_inputs,\n",
    "            padded_labels,\n",
    "            masks_inputs\n",
    "        )\n",
    "    }\n",
    "\n",
    "def get_era2data(df):\n",
    "    # Group by the Timestamp index (level=0)\n",
    "    res = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "        delayed(convert_to_torch)(timestamp, data)\n",
    "        for timestamp, data in tqdm(df.groupby(level=0)))\n",
    "    \n",
    "    era2data = {}\n",
    "    for r in tqdm(res):\n",
    "        era2data.update(r)\n",
    "    return era2data\n",
    "\n",
    "# Assuming DataFrame is named \"dataset\": testing the function\n",
    "timestamp2data_dataset = get_era2data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearsonr in torch differentiable\n",
    "def pearsonr(x, y):\n",
    "    mx = x.mean()\n",
    "    my = y.mean()\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = torch.sum(xm * ym)\n",
    "    r_den = torch.sqrt(torch.sum(xm ** 2) * torch.sum(ym ** 2))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                padded_inputs=None, target_weight_softmax=None):\n",
    "    # print(\"Outputs shape:\", outputs.shape)\n",
    "    # print(\"Padded labels shape:\", padded_labels.shape)\n",
    "    # MSE on all targets; additionally, on primary target\n",
    "    if target_weight_softmax is not None:\n",
    "        _mse = criterion(\n",
    "            outputs * masks_inputs * target_weight_softmax,\n",
    "            padded_labels * masks_inputs * target_weight_softmax\n",
    "        ) * 0.1\n",
    "\n",
    "    else:\n",
    "        _mse = criterion(outputs * masks_inputs, padded_labels * masks_inputs) * 0.1\n",
    "\n",
    "    _mse += criterion(outputs[:, 0] * masks_inputs, padded_labels[:, 0] * masks_inputs)\n",
    "\n",
    "    # Corr with only primary target; adjust as needed\n",
    "    corr = pearsonr(\n",
    "        outputs[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "        padded_labels[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "    )\n",
    "\n",
    "    loss = _mse - corr #+ some_complex_constraints\n",
    "    return loss, _mse, corr\n",
    "\n",
    "# Training loop\n",
    "def train_on_batch(model, criterion, optimizer, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    # print(padded_inputs.shape)\n",
    "    # print(padded_labels.shape)\n",
    "    # print(masks_inputs.shape)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(padded_inputs / 4.0, masks_inputs)\n",
    "    # print(\"Outputs shape:\", outputs.shape)\n",
    "    # print(\"Padded labels shape:\", padded_labels.shape)\n",
    "\n",
    "\n",
    "    target_weight_softmax = None\n",
    "    #random_weights = torch.rand(padded_labels.shape[-1], device=device)\n",
    "    #target_weight_softmax = F.softmax(random_weights)\n",
    "\n",
    "    loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                                       target_weight_softmax=target_weight_softmax)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), _mse.item(), _corr.item()\n",
    "\n",
    "\n",
    "def evaluate_on_batch(transformer, criterion, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(padded_inputs / 4.0, masks_inputs)\n",
    "        # print(outputs)\n",
    "        loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs)\n",
    "        \n",
    "        # Convert outputs to numpy\n",
    "        preds = outputs[0][masks_inputs.view(-1).nonzero()].squeeze(1).cpu().numpy()\n",
    "        # print(preds)\n",
    "\n",
    "    return loss.item(), _mse.item(), _corr.item(), preds\n",
    "\n",
    "def compute_fold_metrics(era_scores, weights=None):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max()\n",
    "\n",
    "    # Smart Sharpe\n",
    "    smart_sharpe = mean_correlation \\\n",
    "        / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd,\n",
    "        'min_correlation': era_scores.min(),\n",
    "        'max_correlation': era_scores.max(),\n",
    "    })\n",
    "\n",
    "    if weights:\n",
    "        normalized_metrics = (metrics - metrics.min()) / (metrics.max() - metrics.min())\n",
    "        weighted_values = normalized_metrics.multiply(pd.Series(weights))\n",
    "        metrics[\"weighted_score\"] = weighted_values.sum()\n",
    "\n",
    "    _ = gc.collect()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, \\\n",
    "                num_epochs, patience, train_loader, val_loader=None, is_lr_scheduler=True):\n",
    "    best_score = float('-inf')  # Initialize with negative infinity since we want to maximize Sharpe ratio\n",
    "    best_corr = None\n",
    "    best_model = None\n",
    "    all_val_scores = []\n",
    "    all_val_outputs = {}\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    epoch_progress = tqdm(range(num_epochs), desc=\"Epochs\", leave=False)\n",
    "\n",
    "    for epoch in epoch_progress:\n",
    "        total_loss = []\n",
    "        total_corr = []\n",
    "\n",
    "        # Training\n",
    "        for era_num in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            batch = train_loader[era_num]\n",
    "            loss, _mse, _corr = train_on_batch(model, criterion, optimizer, batch)\n",
    "            total_loss.append(loss)\n",
    "            total_corr.append(_corr)\n",
    "\n",
    "        # Adjust learning rate if is_lr_scheduler is True\n",
    "        if is_lr_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation - Only if val_loader is provided\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_total_loss = []\n",
    "            val_total_corr = []\n",
    "            val_total_outputs = {}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for era_num in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                    batch = val_loader[era_num]\n",
    "                    loss, _mse, _corr, outputs = evaluate_on_batch(model, criterion, batch)\n",
    "                    val_total_loss.append(loss)\n",
    "                    val_total_corr.append(_corr)\n",
    "                    val_total_outputs[era_num] = outputs\n",
    "\n",
    "            all_val_scores.append(val_total_corr) \n",
    "            all_val_outputs.update(val_total_outputs)\n",
    "\n",
    "            # Early stopping check based on Sharpe score\n",
    "            current_score = np.mean(val_total_corr) / np.std(val_total_corr)  # Assuming Sharpe ratio here\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_corr = val_total_corr.copy()\n",
    "                best_model = model.state_dict().copy()\n",
    "                no_improve_epoch = 0\n",
    "            else:\n",
    "                no_improve_epoch += 1\n",
    "                if no_improve_epoch >= patience:\n",
    "                    epoch_progress.set_description(f'Early stopping at epoch {epoch+1}')\n",
    "                    epoch_progress.refresh()\n",
    "                    break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        _ = gc.collect()\n",
    "\n",
    "    if val_loader:  # If validation data was provided\n",
    "        return best_model, best_corr, all_val_scores\n",
    "    else:  # If only training data was used without validation\n",
    "        return model.state_dict(), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "from model import Transformer, SimpleNN\n",
    "\n",
    "# Constants and hyperparameters\n",
    "NUM_EPOCHS = 15\n",
    "PATIENCE = 5\n",
    "FEATURE_DIM = len(features)  # Assuming 'features' is defined elsewhere in your code\n",
    "OUTPUT_DIM = 1\n",
    "NUM_TRAIL = 25\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "weights = {\n",
    "        'mean_correlation': 0.0,\n",
    "        'std_deviation': 0, # Mild penalty for higher volatility\n",
    "        'sharpe_ratio': 1,    # Primary objective, so highest weight\n",
    "        'smart_sharpe': 0,   # Supplementary to Sharpe Ratio but considering autocorrelation\n",
    "        'autocorrelation': 0, # Penalize strategies showing signs of overfitting\n",
    "        'max_dd': 0,          # Major risk metric, negative to penalize higher drawdowns\n",
    "        'min_correlation': 0.0,\n",
    "        'max_correlation': 0.0,\n",
    "    }\n",
    "\n",
    "def objective(trial, dataset=dataset):\n",
    "    print(f\"\\n--- Starting Trial: {trial.number + 1} ---\")\n",
    "\n",
    "    # Suggest parameters for data split\n",
    "    train_length_multiplier = trial.suggest_int('train_length_multiplier', 10, 15)\n",
    "    val_period_length = trial.suggest_categorical('val_period_length', [21, 42, 63])\n",
    "    lookahead = trial.suggest_categorical('lookahead', [1, 5, 21])\n",
    "\n",
    "    # Model-specific hyperparameters\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 1, 5)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256, step=2)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 5)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    cv = CustomBackwardMultipleTimeSeriesCV(dataset,\n",
    "                                            train_period_length=int(21 * train_length_multiplier),\n",
    "                                            test_period_length=val_period_length,\n",
    "                                            lookahead=lookahead, date_idx='date')\n",
    "    cv.update_lookahead(lookahead)\n",
    "\n",
    "    fold_weighted_scores = []\n",
    "\n",
    "    for train_idx, test_idx in cv:\n",
    "        # Choose model\n",
    "        model = Transformer(\n",
    "            input_dim=FEATURE_DIM,\n",
    "            d_model=hidden_dim,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "        ).to(device)\n",
    "        \n",
    "        # Uncomment below lines for SimpleNN\n",
    "        # model = SimpleNN(input_dim=FEATURE_DIM, output_dim=OUTPUT_DIM).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "        train_data = dataset.iloc[train_idx]\n",
    "        test_data = dataset.iloc[test_idx]\n",
    "\n",
    "        train_batches = get_era2data(train_data)  # Assuming this function is defined elsewhere\n",
    "        validation_batches = get_era2data(test_data)\n",
    "\n",
    "        _, val_corr_on_fold, _ = train_model(\n",
    "            model, criterion, optimizer, scheduler, NUM_EPOCHS, PATIENCE, \n",
    "            train_batches, validation_batches, is_lr_scheduler=True\n",
    "        )\n",
    "\n",
    "        scores_on_fold = compute_fold_metrics(val_corr_on_fold)\n",
    "\n",
    "        normalized_scores = (scores_on_fold - scores_on_fold.min()) \\\n",
    "            / (scores_on_fold.max() - scores_on_fold.min())\n",
    "        weighted_scores_on_fold = normalized_scores.multiply(pd.Series(weights))\n",
    "        fold_weighted_scores.append(weighted_scores_on_fold.sum())\n",
    "\n",
    "    overall_score = np.mean(fold_weighted_scores)\n",
    "    print(overall_score)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(trial.params)\n",
    "        mlflow.log_metric(\"avg_score_across_folds\", overall_score)\n",
    "\n",
    "    return -overall_score if not np.isnan(overall_score) else 1e-9\n",
    "\n",
    "def callback(study, trial):\n",
    "    print(f\"\\n--- Trial {trial.number + 1} finished ---\")\n",
    "    print(f\"Value: {trial.value} and parameters: {trial.params}\")\n",
    "    \n",
    "    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    \n",
    "    if completed_trials:\n",
    "        print(f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}\\n\")\n",
    "    else:\n",
    "        print(\"No successful trials yet.\\n\")\n",
    "\n",
    "study_dir = \"/home/sayem/Desktop/Project/study\"\n",
    "study = optuna.create_study(study_name='Maximizing the Sharpe', direction='minimize',\n",
    "                            storage=f'sqlite:///{study_dir}/study.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=NUM_TRAIL, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all trials have finished, retrieve the best trial's parameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Create the best model using the Transformer\n",
    "best_model = Transformer(\n",
    "    input_dim=FEATURE_DIM,\n",
    "    d_model=best_params[\"hidden_dim\"],\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    num_heads=best_params[\"num_heads\"],\n",
    "    num_layers=best_params[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "# Below is the SimpleNN code, commented out:\n",
    "# best_model = SimpleNN(input_dim=FEATURE_DIM, output_dim=OUTPUT_DIM).to(device)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "criterion = nn.MSELoss()\n",
    "lr = best_params['learning_rate']\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Assuming get_era2data() can handle the entire dataset\n",
    "all_batches = get_era2data(dataset)  \n",
    "\n",
    "# You might need to adjust/train_model to handle no validation set or adjust accordingly.\n",
    "_, _, _ = train_model(\n",
    "    best_model, criterion, optimizer, scheduler, NUM_EPOCHS, PATIENCE, \n",
    "    all_batches, None, is_lr_scheduler=True  # Assuming train_model can handle None for validation_batches\n",
    ")\n",
    "\n",
    "# Saving the model\n",
    "model_name = best_model.__class__.__name__\n",
    "lookahead = best_params.get(\"lookahead\", \"NA\")\n",
    "filename = f\"{top}_{model_name}_{target_string}_{lookahead:02d}d_rank_quantiled.pkl\"\n",
    "file_path = os.path.join(model_dir, filename)\n",
    "\n",
    "save_data = {\n",
    "    'model_type': 'Transformer',\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'trial_params': best_params\n",
    "}\n",
    "torch.save(save_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved data\n",
    "loaded_data = torch.load(file_path)\n",
    "\n",
    "# Create the correct model based on the saved type\n",
    "if loaded_data['model_type'] == 'Transformer':\n",
    "    model = Transformer(\n",
    "        input_dim=FEATURE_DIM,\n",
    "        d_model=loaded_data['trial_params'][\"hidden_dim\"],\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        num_heads=loaded_data['trial_params'][\"num_heads\"],\n",
    "        num_layers=loaded_data['trial_params'][\"num_layers\"]\n",
    "    ).to(device)\n",
    "else:\n",
    "    model = SimpleNN(input_dim=FEATURE_DIM, output_dim=OUTPUT_DIM).to(device)\n",
    "\n",
    "# Load the saved parameters into the model\n",
    "model.load_state_dict(loaded_data['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
