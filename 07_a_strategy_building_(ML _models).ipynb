{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import rank_and_quantize\n",
    "\n",
    "top = 250 # parameters -> papermill\n",
    "\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_key = '/data/YEAR_20220906_20230811'\n",
    "# dataset_key = None\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    dataset = store[dataset_key]\n",
    "    # dataset = store['/data/YEAR_20161115_20181022']\n",
    "    dataset = rank_and_quantize(dataset, TARGET_col='TARGET_ret_fwd_frac_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "cols = dataset.columns.tolist()\n",
    "\n",
    "# Populate the features list with column names starting with 'feature_'\n",
    "features = [col for col in cols if col.startswith('FEATURE_')]\n",
    "\n",
    "# Find the first column starting with 'target_' and set it as the label\n",
    "label_cols = [col for col in cols if col.startswith('TARGET_')]\n",
    "# print(label_cols)\n",
    "# label = label_cols[0] if label_cols else None\n",
    "label = 'TARGET_ret_fwd_frac_order_quantiled' # parameters\n",
    "print(len(features))  # This will show all the columns starting with 'feature_'\n",
    "print(label)  # This will show the first column starting with 'target_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique dates and sort them\n",
    "unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "\n",
    "# Adjust for the look-ahead gap\n",
    "look_ahead = 1\n",
    "\n",
    "# Split dates for training and testing with a gap\n",
    "train_dates = unique_dates[:-21-look_ahead]\n",
    "test_dates = unique_dates[-21:]\n",
    "\n",
    "# Split the dataset\n",
    "train_data = dataset.loc[pd.IndexSlice[:, train_dates], :]\n",
    "test_data = dataset.loc[pd.IndexSlice[:, test_dates], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fi(model):\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(),\n",
    "                index=model.feature_name()))\n",
    "                \n",
    "def ic_lgbm(preds, train_data):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    is_higher_better = True\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], \\\n",
    "        is_higher_better\n",
    "\n",
    "def sharpe_ratio_lgbm(preds, train_data):\n",
    "    \"\"\"Custom Sharpe ratio eval metric for lightgbm that calculates daily Spearman correlations.\"\"\"\n",
    "    labels = train_data.get_label()\n",
    "    \n",
    "    # Assuming the data index is a MultiIndex with date as the first level\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    # Group by the first level of the MultiIndex (date) and compute the Spearman correlation for each group\n",
    "    grouped_labels = pd.Series(labels, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_labels))\n",
    "    grouped_preds = pd.Series(preds, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_preds))\n",
    "\n",
    "    daily_scores = []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        score_for_day = spearmanr(actuals_for_day, preds_for_day)[0]\n",
    "        if np.isnan(score_for_day):\n",
    "            score_for_day = 0\n",
    "        daily_scores.append(score_for_day)\n",
    "\n",
    "    # Calculate the Sharpe ratio\n",
    "    sharpe_ratio = np.mean(daily_scores) / (np.std(daily_scores) + 1e-9)  # added epsilon to avoid division by zero\n",
    "\n",
    "    return 'sharpe_ratio', sharpe_ratio, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_fold(era_scores):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max()\n",
    "\n",
    "    # Smart Sharpe\n",
    "    smart_sharpe = mean_correlation / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd,\n",
    "        'min_correlation': era_scores.min(),\n",
    "        'max_correlation': era_scores.max(),\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from scipy.stats import spearmanr\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "def get_categoricals(dataset, threshold):\n",
    "    return [col for col in dataset.columns if \\\n",
    "            dataset[col].nunique() < threshold and \\\n",
    "            dataset[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "def objective(trial, data, features, cv):\n",
    "    # Dynamic categoricals based on the trial's suggested threshold\n",
    "    cat_threshold = trial.suggest_int('cat_threshold', 5, 50)\n",
    "    categoricals = [col for col in data.columns if data[col].nunique() < cat_threshold \n",
    "                    and data[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "    params = {\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'verbose': -1,\n",
    "        'metric': 'None',\n",
    "        'device': 'gpu',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "    }\n",
    "\n",
    "    early_stopping = lgb.early_stopping(stopping_rounds=500, \\\n",
    "        verbose=True, first_metric_only=True)\n",
    "    all_daily_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv:\n",
    "        train_features = data.loc[train_idx, features]\n",
    "        train_labels = data.loc[train_idx, label]\n",
    "        lgb_train = lgb.Dataset(data=train_features, label=train_labels, \n",
    "                                categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        val_features = data.loc[val_idx, features]\n",
    "        val_labels = data.loc[val_idx, label]\n",
    "        lgb_val = lgb.Dataset(data=val_features, label=val_labels, \n",
    "                              categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(params=params,\n",
    "                          train_set=lgb_train,\n",
    "                          num_boost_round=5000,\n",
    "                          valid_sets=[lgb_train, lgb_val],\n",
    "                          valid_names=['train', 'valid_0'],\n",
    "                          feval=sharpe_ratio_lgbm,\n",
    "                          callbacks=[lgb.record_evaluation(evals_result),\n",
    "                                     early_stopping,\n",
    "                                     LightGBMPruningCallback(trial, 'sharpe_ratio')])\n",
    "        \n",
    "        all_daily_scores.extend(evals_result['valid_0']['sharpe_ratio'])\n",
    "\n",
    "    metrics = metrics_on_fold(all_daily_scores)\n",
    "    score = metrics['smart_sharpe']\n",
    "\n",
    "    # Log parameters and metrics to MLflow\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_metric(\"avg_sharpe_ratio\", np.mean(score))\n",
    "        # mlflow.lightgbm.log_model(model, \"lightgbm_model\")\n",
    "\n",
    "\n",
    "    return score if not np.isnan(score) else 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(train_data, train_period_length=21*3, \n",
    "                                        test_period_length=5, \n",
    "                                        lookahead=1, \n",
    "                                        date_idx='date')\n",
    "\n",
    "def progress_bar(study, trial, n_trials):\n",
    "    progress = (trial.number + 1) / n_trials\n",
    "    best_trial_msg = \"\"\n",
    "    if study.best_trial is not None:\n",
    "        best_trial_msg = f\"Best is trial {study.best_trial.number} \\\n",
    "            with value: {study.best_trial.value}.\"\n",
    "    print(f'Trial {trial.number + 1}/{n_trials} finished with value: \\\n",
    "        {trial.value} and parameters: {trial.params}. {best_trial_msg}')\n",
    "\n",
    "# Check if 'study' directory exists, if not, create it.\n",
    "if not os.path.exists(\"study\"):\n",
    "    os.makedirs(\"study\")\n",
    "\n",
    "# Use SQLite to store optimization results.\n",
    "# The study results are stored in the \"study\" folder as \"study.db\".\n",
    "storage_name = \"sqlite:///study/study.db\"\n",
    "\n",
    "# Name of the study. This should be consistent for resuming the study later.\n",
    "study_name = \"lgbm_optimization\"\n",
    "\n",
    "# Try to load the study. If it doesn't exist, create a new one.\n",
    "study = optuna.create_study(study_name=study_name,\n",
    "                            storage=storage_name,\n",
    "                            direction='maximize',\n",
    "                            load_if_exists=True, \n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, \\\n",
    "                            n_warmup_steps=5))\n",
    "\n",
    "n_trials = 15\n",
    "study.optimize(lambda trial: objective(trial, train_data, features, cv), \n",
    "               n_trials=n_trials, \n",
    "               callbacks=[lambda study, trial: progress_bar(study, trial, n_trials)])\n",
    "\n",
    "# Printing the optimization results\n",
    "print(f'Best trial score: {study.best_trial.value}')\n",
    "print('Best hyperparameters:')\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters found by Optuna:\")\n",
    "print(best_params)\n",
    "\n",
    "# Remove the study database\n",
    "os.remove(\"study/study.db\")\n",
    "print(\"Database has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create DataFrame from results\n",
    "cv_result = pd.DataFrame({'Train Set': evals_result['train']['sharpe_ratio'], \n",
    "                          'Validation Set': evals_result['valid_0']['sharpe_ratio']})\n",
    "\n",
    "# Create the plot with separate y-axes for Train and Validation sets\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes sharing the same x-axis\n",
    "ax1.plot(cv_result.index, cv_result['Train Set'], 'g-')\n",
    "ax2.plot(cv_result.index, cv_result['Validation Set'], 'b-')\n",
    "\n",
    "ax1.set_ylabel('Train Set', color='g')\n",
    "ax2.set_ylabel('Validation Set', color='b')\n",
    "ax1.axvline(cv_result['Validation Set'].idxmax(), c='k', ls='--', lw=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract cat_threshold from best_params\n",
    "# cat_threshold = best_params.get('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "# Extract cat_threshold from best_params\n",
    "cat_threshold = best_params.pop('cat_threshold', 50)  # default to 50 if not in best_params\n",
    "\n",
    "\n",
    "# Determine the categorical columns based on cat_threshold\n",
    "categoricals = [col for col in train_data.columns if train_data[col].nunique() < cat_threshold \n",
    "                and train_data[col].ge(0).all() and col.startswith(\"FEATURE_\")]\n",
    "\n",
    "# Create the training dataset\n",
    "lgb_train_all = lgb.Dataset(data=train_data[features], label=train_data[label], \n",
    "                            categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "best_params['force_col_wise'] = True\n",
    "# Train the model with the best parameters\n",
    "# best_model = lgb.train(params=best_params,\n",
    "#                        train_set=lgb_train_all,\n",
    "#                        num_boost_round=5000,  # or some other number of boosting rounds\n",
    "#                        feval=sharpe_ratio_lgbm,\n",
    "#                        callbacks=[lgb.record_evaluation(evals_result)])\n",
    "best_model = lgb.train(params=best_params,\n",
    "                       train_set=lgb_train_all,\n",
    "                       num_boost_round=5000,  # or some other number of boosting rounds\n",
    "                       feval=sharpe_ratio_lgbm,\n",
    "                       callbacks=[lgb.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the models folder path\n",
    "models = Path(\"./models\")\n",
    "\n",
    "# Ensure the folder exists\n",
    "models.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure that dataset_key doesn't contain invalid characters like slashes\n",
    "clean_dataset_key = dataset_key.replace(\"/\", \"_\")\n",
    "\n",
    "# Formulate the clean save path\n",
    "save_path = models / f\"{clean_dataset_key}_best_model.txt\"\n",
    "\n",
    "# Try saving again\n",
    "best_model.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data[features]\n",
    "test_labels = test_data[label]\n",
    "\n",
    "y_pred = best_model.predict(test_features)\n",
    "\n",
    "preds = test_labels.reset_index(name=\\\n",
    "    'actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "\n",
    "# Rename columns to add 'feature_' prefix\n",
    "cols_to_rename = ['open', 'high', 'low', 'close', 'volume']\n",
    "new_col_names = [\"FEATURE_\" + col for col in cols_to_rename]\n",
    "rename_dict = dict(zip(cols_to_rename, new_col_names))\n",
    "\n",
    "test_data_renamed = test_data.rename(columns=rename_dict)\n",
    "\n",
    "# Using the 'merge' method to join on MultiIndex levels 'date' and 'ticker'\n",
    "preds = preds.reset_index().merge(test_data_renamed[new_col_names].reset_index(), \n",
    "                                  on=['ticker', 'date'], \n",
    "                                  how='left')\n",
    "\n",
    "### Only select columns of interest\n",
    "preds = preds[['date', 'ticker', 'actual', 'predicted'] \\\n",
    "    + new_col_names].set_index(['ticker', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_spearman(group):\n",
    "    return spearmanr(group['actual'], group['predicted'])[0]\n",
    "\n",
    "daily_correlations = preds.groupby('date').apply(daily_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of daily correlations\n",
    "mean_daily_correlation = daily_correlations.mean()\n",
    "std_daily_correlation = daily_correlations.std()\n",
    "\n",
    "# Calculate Sharpe ratio for each date\n",
    "papermill_era_scores = daily_sharpe_ratios = (daily_correlations - \\\n",
    "    mean_daily_correlation) / std_daily_correlation\n",
    "\n",
    "papermill_era_scores_df = papermill_era_scores.to_frame()\n",
    "papermill_era_scores_df.columns = papermill_era_scores_df.columns.astype(str)\n",
    "sb.glue(\"papermill_era_scores\", papermill_era_scores_df, display=True)\n",
    "\n",
    "# papermill_era_scores_list = papermill_era_scores.tolist()\n",
    "# sb.glue(\"papermill_era_scores\", papermill_era_scores_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of colors based on the sign of the Sharpe Ratios\n",
    "colors = ['blue' if value > 0 else 'red' for value in daily_sharpe_ratios]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_sharpe_ratios.plot(kind='bar', color=colors)\n",
    "plt.title('Daily Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.axhline(y=0, color='black', linestyle='-')  # Here's where we add the horizontal line at y=0\n",
    "plt.xticks(rotation=45)  # rotates the x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path(\"plots\")\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "plot_path = plot_dir / f\"sharpe_ratios_{key}.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "sb.glue(\"papermill_plot_path\", papermill_plot_path_str, display=True)  # Glue the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_r, lr_p = spearmanr(preds.actual, preds.predicted)\n",
    "print(f'Information Coefficient (overall): {lr_r:.3%} (p-value: {lr_p:.8%})')\n",
    "\n",
    "# Return the Information Coefficient and its p-value\n",
    "information_coefficient = lr_r\n",
    "p_value = lr_p\n",
    "\n",
    "# information_coefficient = papermill_information_coefficient, p_value = papermill_p_value\n",
    "sb.glue(\"information_coefficient\", information_coefficient, display=True)\n",
    "sb.glue(\"p_value\", p_value, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papermill_plot_path_str = str(plot_path)  # Convert to string\n",
    "# sb.glue(\"papermill_plot_path\", papermill_plot_path_str)  # Glue the string\n",
    "\n",
    "# sb.glue(\"information_coefficient\", information_coefficient)\n",
    "# sb.glue(\"p_value\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
