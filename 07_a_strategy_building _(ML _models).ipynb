{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/YEAR_20130102_20141208', '/data/YEAR_20141209_20161114', '/data/YEAR_20161115_20181022', '/data/YEAR_20181023_20200928', '/data/YEAR_20200929_20220902', '/data/YEAR_20220906_20230811', '/data/ic_based_reduced_features_YEAR_20220906_20230811']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_STORE = Path('data/250_dataset.h5')\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    keys = store.keys()\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rank_and_quantize\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    dataset = store['/data/YEAR_20220906_20230811']\n",
    "    dataset = rank_and_quantize(dataset, TARGET_col='TARGET_ret_fwd_frac_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "cols = dataset.columns.tolist()\n",
    "\n",
    "# Populate the features list with column names starting with 'feature_'\n",
    "features = [col for col in cols if col.startswith('FEATURE_')]\n",
    "\n",
    "# Find the first column starting with 'target_' and set it as the label\n",
    "label_cols = [col for col in cols if col.startswith('TARGET_')]\n",
    "# print(label_cols)\n",
    "# label = label_cols[0] if label_cols else None\n",
    "label = 'TARGET_ret_fwd_frac_order_quantiled'\n",
    "print(len(features))  # This will show all the columns starting with 'feature_'\n",
    "print(label)  # This will show the first column starting with 'target_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique dates and sort them\n",
    "unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "\n",
    "# Adjust for the look-ahead gap\n",
    "look_ahead = 1\n",
    "\n",
    "# Split dates for training and testing with a gap\n",
    "train_dates = unique_dates[:-21-look_ahead]\n",
    "test_dates = unique_dates[-21:]\n",
    "\n",
    "# Split the dataset\n",
    "train_data = dataset.loc[pd.IndexSlice[:, train_dates], :]\n",
    "test_data = dataset.loc[pd.IndexSlice[:, test_dates], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fi(model):\n",
    "    fi = model.feature_importance(importance_type='gain')\n",
    "    return (pd.Series(fi / fi.sum(),\n",
    "                index=model.feature_name()))\n",
    "                \n",
    "def ic_lgbm(preds, train_data):\n",
    "    \"\"\"Custom IC eval metric for lightgbm\"\"\"\n",
    "    is_higher_better = True\n",
    "    return 'ic', spearmanr(preds, train_data.get_label())[0], \\\n",
    "        is_higher_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio_lgbm(preds, train_data):\n",
    "    \"\"\"Custom Sharpe ratio eval metric for lightgbm that calculates daily Spearman correlations.\"\"\"\n",
    "    labels = train_data.get_label()\n",
    "    \n",
    "    # Assuming the data index is a MultiIndex with date as the first level\n",
    "    if not isinstance(train_data.data.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Expecting a MultiIndex with date as the first level\")\n",
    "\n",
    "    # Group by the first level of the MultiIndex (date) and compute the Spearman correlation for each group\n",
    "    grouped_labels = pd.Series(labels, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_labels))\n",
    "    grouped_preds = pd.Series(preds, \\\n",
    "        index=train_data.data.index).groupby(level=0)\n",
    "    # print(len(grouped_preds))\n",
    "\n",
    "    daily_scores = []\n",
    "    for (_, actuals_for_day), (_, preds_for_day) in zip(grouped_labels, grouped_preds):\n",
    "        score_for_day = spearmanr(actuals_for_day, preds_for_day)[0]\n",
    "        if np.isnan(score_for_day):\n",
    "            score_for_day = 0\n",
    "        daily_scores.append(score_for_day)\n",
    "\n",
    "    # Calculate the Sharpe ratio\n",
    "    sharpe_ratio = np.mean(daily_scores) / (np.std(daily_scores) + 1e-9)  # added epsilon to avoid division by zero\n",
    "\n",
    "    return 'sharpe_ratio', sharpe_ratio, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_fold(era_scores):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max()\n",
    "\n",
    "    # Smart Sharpe\n",
    "    smart_sharpe = mean_correlation / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd,\n",
    "        'min_correlation': era_scores.min(),\n",
    "        'max_correlation': era_scores.max(),\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT DELETE or MODIFY\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from scipy.stats import spearmanr\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "def objective(trial, data, features, categoricals, cv):\n",
    "    params = {\n",
    "        'boosting': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'verbose': -1,\n",
    "        'metric': 'None',\n",
    "        'device': 'gpu',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "    }\n",
    "\n",
    "    # Create a LightGBM callback for early stopping\n",
    "    early_stopping = lgb.early_stopping(stopping_rounds=500, \\\n",
    "        verbose=True, first_metric_only=True)\n",
    "\n",
    "    all_daily_scores = []\n",
    "    for train_idx, val_idx in cv:\n",
    "        train_features = data.loc[train_idx, features]\n",
    "        train_labels = data.loc[train_idx, label]\n",
    "\n",
    "        lgb_train = lgb.Dataset(data=train_features, label=train_labels, \n",
    "                                categorical_feature=categoricals,\n",
    "                                free_raw_data=False)\n",
    "\n",
    "        val_features = data.loc[val_idx, features]\n",
    "        val_labels = data.loc[val_idx, label]\n",
    "\n",
    "        lgb_val = lgb.Dataset(data=val_features, label=val_labels, \n",
    "                              categorical_feature=categoricals,\n",
    "                              free_raw_data=False)\n",
    "\n",
    "        model = lgb.train(params=params,\n",
    "                          train_set=lgb_train,\n",
    "                          num_boost_round=5000,\n",
    "                          valid_sets=[lgb_train, lgb_val],\n",
    "                          valid_names=['train', 'valid_0'],\n",
    "                          feval=sharpe_ratio_lgbm,\n",
    "                          callbacks=[lgb.record_evaluation(evals_result),\n",
    "                                     early_stopping,\n",
    "                                     LightGBMPruningCallback(trial, 'sharpe_ratio')])\n",
    "\n",
    "        # Ensure your feval function returns daily scores for this to work correctly\n",
    "        all_daily_scores.extend(evals_result['valid_0']['sharpe_ratio'])\n",
    "\n",
    "    metrics = metrics_on_fold(all_daily_scores)\n",
    "\n",
    "    # Now, return the smart_sharpe to be optimized by Optuna\n",
    "    score = metrics['smart_sharpe'] #- 0.1 * metrics['max_dd']\n",
    "\n",
    "    if np.isnan(score):\n",
    "        return 1e-9  # Return a very low value if smart_sharpe is nan\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "uniques = dataset.nunique()\n",
    "\n",
    "# categoricals = [col for col in uniques[uniques < 20].index if dataset[col].ge(0).all()]\n",
    "categoricals = [col for col in uniques[uniques < 20].index if \\\n",
    "    dataset[col].ge(0).all() and col.startswith(\"feature_\")]\n",
    "\n",
    "\n",
    "print(f'Number of categorical vals: {len(categoricals)}')\n",
    "\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(train_data, train_period_length=142, \n",
    "                                        test_period_length=21, \n",
    "                                        lookahead=1, \n",
    "                                        date_idx='date')\n",
    "\n",
    "def progress_bar(study, trial, n_trials):\n",
    "    progress = (trial.number + 1) / n_trials\n",
    "    best_trial_msg = \"\"\n",
    "    if study.best_trial is not None:\n",
    "        best_trial_msg = f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}.\"\n",
    "    print(f'Trial {trial.number + 1}/{n_trials} finished with value: {trial.value} and parameters: {trial.params}. {best_trial_msg}')\n",
    "\n",
    "n_trials = 15\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=5))\n",
    "study.optimize(lambda trial: objective(trial, train_data, features, categoricals, cv), \n",
    "               n_trials=n_trials, \n",
    "               callbacks=[lambda study, \\\n",
    "               trial: progress_bar(study, trial, n_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = pd.DataFrame({'Train Set': evals_result['train']['sharpe_ratio'], \n",
    "                          'Validation Set': evals_result['valid_0']['sharpe_ratio']})\n",
    "\n",
    "ax = cv_result.loc[:300].plot(figsize=(12, 4))\n",
    " \n",
    "ax.axvline(cv_result['Validation Set'].idxmax(), c='k', ls='--', lw=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Best parameters found by Optuna:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_all = lgb.Dataset(data=train_data[features], label=train_data[label], \n",
    "                            categorical_feature=categoricals, free_raw_data=False)\n",
    "\n",
    "best_model = lgb.train(params=best_params,\n",
    "                       train_set=lgb_train_all,\n",
    "                       num_boost_round=5000,  # or some other number of boosting rounds\n",
    "                       feval=sharpe_ratio_lgbm,\n",
    "                       callbacks=[lgb.record_evaluation(evals_result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data[features]\n",
    "test_labels = test_data[label]\n",
    "\n",
    "y_pred = best_model.predict(test_features)\n",
    "# score = spearmanr(test_labels, y_pred)[0]\n",
    "# print(f\"Test set Spearman score: {score}\")\n",
    "\n",
    "preds = test_labels.reset_index(name='actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data[features]\n",
    "test_labels = test_data[label]\n",
    "\n",
    "y_pred = best_model.predict(test_features)\n",
    "\n",
    "preds = test_labels.reset_index(name='actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "\n",
    "# Rename columns to add 'feature_' prefix\n",
    "cols_to_rename = ['open', 'high', 'low', 'close', 'volume']\n",
    "new_col_names = [\"feature_\" + col for col in cols_to_rename]\n",
    "rename_dict = dict(zip(cols_to_rename, new_col_names))\n",
    "\n",
    "test_data_renamed = test_data.rename(columns=rename_dict)\n",
    "\n",
    "# Add ['feature_open', 'feature_high', 'feature_low', 'feature_close', 'feature_volume'] to preds\n",
    "preds = preds.join(test_data_renamed[new_col_names], on=['date', 'ticker'])\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_spearman(group):\n",
    "    return spearmanr(group['actual'], group['predicted'])[0]\n",
    "\n",
    "daily_correlations = preds.groupby('date').apply(daily_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of daily correlations\n",
    "mean_daily_correlation = daily_correlations.mean()\n",
    "std_daily_correlation = daily_correlations.std()\n",
    "\n",
    "# Calculate Sharpe ratio for each date\n",
    "daily_sharpe_ratios = (daily_correlations - mean_daily_correlation) / std_daily_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of colors based on the sign of the Sharpe Ratios\n",
    "colors = ['blue' if value > 0 else 'red' for value in daily_sharpe_ratios]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_sharpe_ratios.plot(kind='bar', color=colors)\n",
    "plt.title('Daily Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.axhline(y=0, color='black', linestyle='-')  # Here's where we add the horizontal line at y=0\n",
    "plt.xticks(rotation=45)  # rotates the x-axis labels for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_r, lr_p = spearmanr(preds.actual, preds.predicted)\n",
    "# print(f'Information Coefficient (overall): {lr_r:.3%} (p-value: {lr_p:.4%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
