{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import rank_and_quantize\n",
    "\n",
    "top = 250\n",
    "DATA_STORE = Path(f'data/total_unseen_data.h5')\n",
    "key = dataset_key = '/data/YEAR_20220906_20230811'\n",
    "label = 'TARGET_ret_fwd_frac_order_quantiled' # parameters\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    dataset = store[dataset_key]\n",
    "    dataset = rank_and_quantize(dataset, TARGET_col='TARGET_ret_fwd_frac_order')\n",
    "\n",
    "# Get unique dates and sort them\n",
    "unique_dates = dataset.index.get_level_values('date').unique().sort_values()\n",
    "cols = dataset.columns.tolist()\n",
    "\n",
    "# Populate the features list with column names starting with 'feature_'\n",
    "features = [col for col in cols if col.startswith('FEATURE_')]\n",
    "\n",
    "# Find the first column starting with 'target_' and set it as the label\n",
    "label_cols = [col for col in cols if col.startswith('TARGET_')]\n",
    "\n",
    "# Adjust for the look-ahead gap and get test dates\n",
    "look_ahead = 1\n",
    "test_dates = unique_dates[-21:]\n",
    "\n",
    "# Create the test_data DataFrame using cross-section (xs) for MultiIndex slicing\n",
    "test_data = dataset[dataset.index.isin(test_dates, level='date')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "def predict_and_format(model_path: str, test_data: pd.DataFrame, features: list, label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a LightGBM model from the specified path, make predictions on the test data, and format the results.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to the saved LightGBM model.\n",
    "    - test_data: Test dataframe containing features and labels.\n",
    "    - features: List of feature column names.\n",
    "    - label: Column name of the label.\n",
    "\n",
    "    Returns:\n",
    "    - preds: Formatted dataframe with predictions and selected feature data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model\n",
    "    best_model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "    # Extract features and labels\n",
    "    test_features = test_data[features]\n",
    "    test_labels = test_data[label]\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(test_features)\n",
    "\n",
    "    # Format the predictions dataframe\n",
    "    preds = test_labels.reset_index(name='actual').assign(predicted=y_pred).set_index(['date', 'ticker'])\n",
    "\n",
    "    # Rename columns to add 'feature_' prefix\n",
    "    cols_to_rename = ['open', 'high', 'low', 'close', 'volume']\n",
    "    new_col_names = [\"FEATURE_\" + col for col in cols_to_rename]\n",
    "    rename_dict = dict(zip(cols_to_rename, new_col_names))\n",
    "\n",
    "    test_data_renamed = test_data.rename(columns=rename_dict)\n",
    "\n",
    "    # Join with selected feature data\n",
    "    preds = preds.reset_index().merge(test_data_renamed[new_col_names].reset_index(), \n",
    "                                      on=['ticker', 'date'], \n",
    "                                      how='left')\n",
    "\n",
    "    # Filter columns of interest\n",
    "    preds = preds[['date', 'ticker', 'actual', 'predicted'] + new_col_names].set_index(['ticker', 'date'])\n",
    "    \n",
    "    return preds\n",
    "\n",
    "model_path = \"/home/sayem/Desktop/Project/models/_data_YEAR_20130102_20141208_best_model.txt\"\n",
    "preds = predict_and_format(model_path, test_data, features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>FEATURE_open</th>\n",
       "      <th>FEATURE_high</th>\n",
       "      <th>FEATURE_low</th>\n",
       "      <th>FEATURE_close</th>\n",
       "      <th>FEATURE_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AA</th>\n",
       "      <th>2023-07-14</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.100522</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>35.520000</td>\n",
       "      <td>34.049999</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>8082529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-17</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137607</td>\n",
       "      <td>34.349998</td>\n",
       "      <td>36.049999</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>35.570000</td>\n",
       "      <td>6405879.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-18</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.094613</td>\n",
       "      <td>35.310001</td>\n",
       "      <td>35.720001</td>\n",
       "      <td>34.619999</td>\n",
       "      <td>35.040001</td>\n",
       "      <td>4661669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-19</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.105579</td>\n",
       "      <td>35.169998</td>\n",
       "      <td>35.340000</td>\n",
       "      <td>34.529999</td>\n",
       "      <td>34.840000</td>\n",
       "      <td>6127124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.029240</td>\n",
       "      <td>35.330002</td>\n",
       "      <td>35.759998</td>\n",
       "      <td>32.669998</td>\n",
       "      <td>32.680000</td>\n",
       "      <td>11962060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZTS</th>\n",
       "      <th>2023-08-07</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>180.190002</td>\n",
       "      <td>181.880005</td>\n",
       "      <td>180.029999</td>\n",
       "      <td>180.690002</td>\n",
       "      <td>1941699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-08</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>182.149994</td>\n",
       "      <td>190.539993</td>\n",
       "      <td>177.250000</td>\n",
       "      <td>189.300003</td>\n",
       "      <td>4117986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-09</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.813457</td>\n",
       "      <td>189.940002</td>\n",
       "      <td>190.949997</td>\n",
       "      <td>186.964996</td>\n",
       "      <td>189.350006</td>\n",
       "      <td>2270030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-10</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.779792</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>191.824997</td>\n",
       "      <td>187.729996</td>\n",
       "      <td>189.100006</td>\n",
       "      <td>2348639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-11</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.786464</td>\n",
       "      <td>188.479996</td>\n",
       "      <td>191.600006</td>\n",
       "      <td>188.179993</td>\n",
       "      <td>190.830002</td>\n",
       "      <td>1742931.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5544 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   actual  predicted  FEATURE_open  FEATURE_high  FEATURE_low  \\\n",
       "ticker date                                                                     \n",
       "AA     2023-07-14    0.25   0.100522     35.500000     35.520000    34.049999   \n",
       "       2023-07-17    0.00   0.137607     34.349998     36.049999    34.250000   \n",
       "       2023-07-18    0.00   0.094613     35.310001     35.720001    34.619999   \n",
       "       2023-07-19    0.00   0.105579     35.169998     35.340000    34.529999   \n",
       "       2023-07-20    0.00   0.029240     35.330002     35.759998    32.669998   \n",
       "...                   ...        ...           ...           ...          ...   \n",
       "ZTS    2023-08-07    0.75   0.734100    180.190002    181.880005   180.029999   \n",
       "       2023-08-08    0.75   0.786845    182.149994    190.539993   177.250000   \n",
       "       2023-08-09    0.75   0.813457    189.940002    190.949997   186.964996   \n",
       "       2023-08-10    0.75   0.779792    190.000000    191.824997   187.729996   \n",
       "       2023-08-11    0.75   0.786464    188.479996    191.600006   188.179993   \n",
       "\n",
       "                   FEATURE_close  FEATURE_volume  \n",
       "ticker date                                       \n",
       "AA     2023-07-14      34.520000       8082529.0  \n",
       "       2023-07-17      35.570000       6405879.0  \n",
       "       2023-07-18      35.040001       4661669.0  \n",
       "       2023-07-19      34.840000       6127124.0  \n",
       "       2023-07-20      32.680000      11962060.0  \n",
       "...                          ...             ...  \n",
       "ZTS    2023-08-07     180.690002       1941699.0  \n",
       "       2023-08-08     189.300003       4117986.0  \n",
       "       2023-08-09     189.350006       2270030.0  \n",
       "       2023-08-10     189.100006       2348639.0  \n",
       "       2023-08-11     190.830002       1742931.0  \n",
       "\n",
       "[5544 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_spearman(group):\n",
    "    return spearmanr(group['actual'], group['predicted'])[0]\n",
    "\n",
    "daily_correlations = preds.groupby('date').apply(daily_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of daily correlations\n",
    "mean_daily_correlation = daily_correlations.mean()\n",
    "std_daily_correlation = daily_correlations.std()\n",
    "\n",
    "# Calculate Sharpe ratio for each date\n",
    "papermill_era_scores = daily_sharpe_ratios = (daily_correlations - \\\n",
    "    mean_daily_correlation) / std_daily_correlation\n",
    "\n",
    "papermill_era_scores_df = papermill_era_scores.to_frame()\n",
    "papermill_era_scores_df.columns = papermill_era_scores_df.columns.astype(str)\n",
    "sb.glue(\"papermill_era_scores\", papermill_era_scores_df, display=True)\n",
    "\n",
    "papermill_era_scores_list = papermill_era_scores.tolist()\n",
    "sb.glue(\"papermill_era_scores\", papermill_era_scores_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a list of colors based on the sign of the Sharpe Ratios\n",
    "colors = ['blue' if value > 0 else 'red' for value in daily_sharpe_ratios]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_sharpe_ratios.plot(kind='bar', color=colors)\n",
    "plt.title('Daily Sharpe Ratios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.axhline(y=0, color='black', linestyle='-')  # Add horizontal line at y=0\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "# Define directory and clean up the dataset_key\n",
    "plot_dir = Path(\"plots\")\n",
    "clean_dataset_key = dataset_key.replace(\"/\", \"_\")\n",
    "\n",
    "# Create the plots directory if it doesn't exist\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the save path for the plot using the cleaned key\n",
    "plot_path = plot_dir / f\"sharpe_ratios_{clean_dataset_key}.png\"\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "\n",
    "# Convert the path to string and glue it\n",
    "papermill_plot_path_str = str(plot_path)\n",
    "sb.glue(\"papermill_plot_path\", papermill_plot_path_str, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_r, lr_p = spearmanr(preds.actual, preds.predicted)\n",
    "print(f'Information Coefficient (overall): {lr_r:.3%} (p-value: {lr_p:.8%})')\n",
    "\n",
    "# Return the Information Coefficient and its p-value\n",
    "information_coefficient = lr_r\n",
    "p_value = lr_p\n",
    "\n",
    "# information_coefficient = papermill_information_coefficient, p_value = papermill_p_value\n",
    "sb.glue(\"information_coefficient\", information_coefficient, display=True)\n",
    "sb.glue(\"p_value\", p_value, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quantile_signals(df, buy_threshold=0.95, sell_threshold=0.05):\n",
    "    buy_cutoff = df['predicted'].quantile(buy_threshold)\n",
    "    sell_cutoff = df['predicted'].quantile(sell_threshold)\n",
    "    \n",
    "    df['signal'] = 0  # Neutral by default\n",
    "    df.loc[df['predicted'] >= buy_cutoff, 'signal'] = 1  # Buy\n",
    "    df.loc[df['predicted'] <= sell_cutoff, 'signal'] = -1  # Sell\n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "preds = add_quantile_signals(preds, \\\n",
    "    buy_threshold=0.95, sell_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import backtrader as bt\n",
    "import backtrader.analyzers as btanalyzers\n",
    "from pypfopt import EfficientFrontier, expected_returns, risk_models\n",
    "import pyfolio as pf\n",
    "\n",
    "class PandasPredictions(bt.feeds.PandasData):\n",
    "    lines = ('signal',)\n",
    "    params = (\n",
    "        ('signal', -1),\n",
    "        ('open', 'FEATURE_open'),\n",
    "        ('high', 'FEATURE_high'),\n",
    "        ('low', 'FEATURE_low'),\n",
    "        ('close', 'FEATURE_close'),\n",
    "        ('volume', 'FEATURE_volume')\n",
    "    )\n",
    "\n",
    "def optimize_weights(datas):\n",
    "    prices = {}\n",
    "    \n",
    "    for data in datas:\n",
    "        # Convert data to pandas series and name it\n",
    "        s = pd.Series(data.close.array, index=data.datetime.array, name=data._name)\n",
    "        prices[data._name] = s\n",
    "\n",
    "    # Create a DataFrame with a common date index\n",
    "    df = pd.DataFrame(index=prices[next(iter(prices))].index)\n",
    "\n",
    "    for ticker, s in prices.items():\n",
    "        df = df.merge(s, left_index=True, right_index=True, how='left').rename(columns={s.name: ticker})\n",
    "\n",
    "    # Handle missing data (can use forward fill, backward fill, or drop NaN, depending on your needs)\n",
    "    df = df.dropna()  # For this example, I'll drop rows with missing data\n",
    "\n",
    "    mu = expected_returns.mean_historical_return(df)\n",
    "    # print(mu)\n",
    "    S = risk_models.sample_cov(df)\n",
    "    # ef = EfficientFrontier(mu, S, solver=\"ECOS\")\n",
    "    ef = EfficientFrontier(mu, S, solver=\"SCS\", verbose=True)\n",
    "    weights = ef.max_sharpe(risk_free_rate=0.005)  # example with 0.5% risk-free rate\n",
    "    return ef.clean_weights()\n",
    "\n",
    "\n",
    "class TradeAndRebalanceStrategy(bt.Strategy):\n",
    "    def __init__(self):\n",
    "        self.rebalance_days = 0\n",
    "        self.max_loss = -0.15\n",
    "        self.start_cash = self.broker.get_cash()\n",
    "        self.benchmark = self.getdatabyname(\"S&P 500\")  # reference to the S&P 500 data\n",
    "\n",
    "    def next(self):\n",
    "        if (self.broker.get_cash() - self.start_cash) / self.start_cash <= self.max_loss:\n",
    "            return\n",
    "        \n",
    "        # Trading signals based on existing signal column in preds dataframe\n",
    "        for data in self.datas:\n",
    "            if data._name == \"S&P 500\":  # Skip the benchmark data\n",
    "                continue\n",
    "            if data.signal[0] == 1:\n",
    "                self.buy(data)\n",
    "            elif data.signal[0] == -1:\n",
    "                self.sell(data)\n",
    "\n",
    "        # Rebalancing\n",
    "        if self.rebalance_days == 0:\n",
    "            weights = optimize_weights([data for data in self.datas if data._name != \"S&P 500\"])  # Exclude benchmark from optimization\n",
    "            for asset, weight in weights.items():\n",
    "                if weight > 0.30:\n",
    "                    weights[asset] = 0.30\n",
    "            \n",
    "            for data in self.datas:\n",
    "                if data._name == \"S&P 500\":  # Skip the benchmark data\n",
    "                    continue\n",
    "                if data._name in weights:\n",
    "                    self.order_target_percent(data, target=weights[data._name])\n",
    "                else:\n",
    "                    self.close(data)\n",
    "            self.rebalance_days = 20\n",
    "        else:\n",
    "            self.rebalance_days -= 1\n",
    "\n",
    "        # Log benchmark performance\n",
    "        benchmark_return = (self.benchmark.close[0] - self.benchmark.close[-1]) / self.benchmark.close[-1]\n",
    "        self.log(f\"Benchmark Return: {benchmark_return * 100:.2f}%\")\n",
    "\n",
    "# # Backtesting Strategy with Trading and Rebalancing Logic\n",
    "# class TradeAndRebalanceStrategy(bt.Strategy):\n",
    "#     def __init__(self):\n",
    "#         self.rebalance_days = 0\n",
    "#         self.max_loss = -0.15\n",
    "#         self.start_cash = self.broker.get_cash()\n",
    "#         self.benchmark = self.getdatabyname(\"S&P 500\")  # reference to the S&P 500 data\n",
    "\n",
    "#     def next(self):\n",
    "#         if (self.broker.get_cash() - self.start_cash) / self.start_cash <= self.max_loss:\n",
    "#             return\n",
    "        \n",
    "#         # Trading signals based on existing signal column in preds dataframe\n",
    "#         for data in self.datas:\n",
    "#             if data.signal[0] == 1:\n",
    "#                 self.buy(data)\n",
    "#             elif data.signal[0] == -1:\n",
    "#                 self.sell(data)\n",
    "\n",
    "#         # Rebalancing\n",
    "#         if self.rebalance_days == 0:\n",
    "#             weights = optimize_weights(self.datas)\n",
    "#             for asset, weight in weights.items():\n",
    "#                 if weight > 0.30:\n",
    "#                     weights[asset] = 0.30\n",
    "            \n",
    "#             for data in self.datas:\n",
    "#                 if data._name in weights:\n",
    "#                     self.order_target_percent(data, target=weights[data._name])\n",
    "#                 else:\n",
    "#                     self.close(data)\n",
    "#             self.rebalance_days = 20\n",
    "#         else:\n",
    "#             self.rebalance_days -= 1\n",
    "\n",
    "#         # Log benchmark performance\n",
    "#         benchmark_return = (self.benchmark.close[0] - self.benchmark.close[-1]) / self.benchmark.close[-1]\n",
    "#         self.log(f\"Benchmark Return: {benchmark_return * 100:.2f}%\")\n",
    "\n",
    "# # Splitting the main dataframe 'preds' into separate dataframes for each ticker\n",
    "# your_data_dict = {ticker: preds.xs(ticker) for ticker in preds.index.get_level_values(0).unique()}\n",
    "\n",
    "# # Extract benchmark returns for Pyfolio\n",
    "# benchmark_returns = [data for data in results[0].datas if data._name == \"S&P 500\"][0]\n",
    "# benchmark_returns = pd.Series(benchmark_returns.close.array, index=benchmark_returns.datetime.array).pct_change().dropna()\n",
    "\n",
    "# cerebro = bt.Cerebro()\n",
    "# cerebro.broker.setcommission(commission=0.001)  # Example: 0.1% commission\n",
    "# # Add the PyFolio Analyzer\n",
    "# cerebro.addanalyzer(btanalyzers.PyFolio, _name='pyfolio')\n",
    "# benchmark = bt.feeds.YahooFinanceData(dataname=\"^GSPC\", \\\n",
    "#     fromdate=your_start_date, todate=your_end_date)  # S&P 500 from Yahoo Finance\n",
    "# cerebro.adddata(benchmark, name=\"S&P 500\")\n",
    "\n",
    "# for ticker, data_df in your_data_dict.items():\n",
    "#     data = PandasPredictions(dataname=data_df, name=ticker)\n",
    "#     cerebro.adddata(data)\n",
    "\n",
    "# cerebro.addstrategy(TradeAndRebalanceStrategy)\n",
    "# results = cerebro.run()\n",
    "\n",
    "# # Performance Analysis\n",
    "# returns, positions, transactions, gross_lev = results[0].analyzers.pyfolio.get_pf_items()\n",
    "# # pf.create_full_tearsheet(returns)\n",
    "# pf.create_simple_tear_sheet(returns)\n",
    "\n",
    "\n",
    "# min_date = preds.index.get_level_values(1).min()\n",
    "# max_date = preds.index.get_level_values(1).max()\n",
    "\n",
    "# import yfinance as yf\n",
    "\n",
    "# # Fetch S&P 500 data using yfinance\n",
    "# def fetch_data(ticker, start_date, end_date):\n",
    "#     df = yf.download(ticker, start=start_date, end=end_date)\n",
    "#     return df\n",
    "\n",
    "# # Your given dates\n",
    "# start_date = preds.index.get_level_values(1).min()\n",
    "# end_date = preds.index.get_level_values(1).max()\n",
    "\n",
    "# # Fetch the benchmark data\n",
    "# sp500_data = fetch_data('^GSPC', start_date, end_date)\n",
    "\n",
    "# # Convert it into Backtrader format\n",
    "# benchmark = bt.feeds.PandasData(dataname=sp500_data, name=\"S&P 500\")\n",
    "\n",
    "# cerebro = bt.Cerebro()\n",
    "# cerebro.broker.setcommission(commission=0.001)\n",
    "# cerebro.addanalyzer(btanalyzers.PyFolio, _name='pyfolio')\n",
    "\n",
    "# # Add the benchmark data\n",
    "# cerebro.adddata(benchmark)\n",
    "\n",
    "# for ticker, data_df in your_data_dict.items():\n",
    "#     data = PandasPredictions(dataname=data_df, name=ticker)\n",
    "#     cerebro.adddata(data)\n",
    "\n",
    "# cerebro.addstrategy(TradeAndRebalanceStrategy)\n",
    "# results = cerebro.run()\n",
    "\n",
    "# # Performance Analysis\n",
    "# returns, positions, transactions, gross_lev = results[0].analyzers.pyfolio.get_pf_items()\n",
    "# pf.create_simple_tear_sheet(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import backtrader as bt\n",
    "import backtrader.analyzers as btanalyzers\n",
    "from pypfopt import EfficientFrontier, expected_returns, risk_models\n",
    "import pyfolio as pf\n",
    "import yfinance as yf\n",
    "\n",
    "# Data Class for Predictions\n",
    "class PandasPredictions(bt.feeds.PandasData):\n",
    "    lines = ('signal',)\n",
    "    params = (\n",
    "        ('signal', -1),\n",
    "        ('open', 'FEATURE_open'),\n",
    "        ('high', 'FEATURE_high'),\n",
    "        ('low', 'FEATURE_low'),\n",
    "        ('close', 'FEATURE_close'),\n",
    "        ('volume', 'FEATURE_volume')\n",
    "    )\n",
    "\n",
    "# Function to optimize weights using PyPortfolioOpt\n",
    "def optimize_weights(datas):\n",
    "    prices = {}\n",
    "    \n",
    "    for data in datas:\n",
    "        s = pd.Series(data.close.array, index=data.datetime.array, name=data._name)\n",
    "        prices[data._name] = s\n",
    "\n",
    "    df = pd.DataFrame(index=prices[next(iter(prices))].index)\n",
    "\n",
    "    for ticker, s in prices.items():\n",
    "        df = df.merge(s, left_index=True, right_index=True, how='left').rename(columns={s.name: ticker})\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    mu = expected_returns.mean_historical_return(df)\n",
    "    S = risk_models.sample_cov(df)\n",
    "    ef = EfficientFrontier(mu, S, solver=\"SCS\", verbose=True)\n",
    "    weights = ef.max_sharpe(risk_free_rate=0.005)\n",
    "    return ef.clean_weights()\n",
    "\n",
    "# Strategy Class\n",
    "class TradeAndRebalanceStrategy(bt.Strategy):\n",
    "    lines = ('benchmark',)  # Added benchmark line here\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rebalance_days = 0\n",
    "        self.max_loss = -0.15\n",
    "        self.start_cash = self.broker.get_cash()\n",
    "        self.benchmark_data = self.getdatabyname(\"S&P 500\")\n",
    "\n",
    "    def log(self, txt, dt=None):\n",
    "        ''' Logging function for the strategy. It logs the date and the message provided. '''\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print(f\"{dt.isoformat()}, {txt}\")\n",
    "\n",
    "    def next(self):\n",
    "        # Update benchmark line at each step\n",
    "        self.lines.benchmark[0] = self.benchmark_data.close[0] \n",
    "\n",
    "        if (self.broker.get_cash() - self.start_cash) / self.start_cash <= self.max_loss:\n",
    "            return\n",
    "        \n",
    "        for data in self.datas:\n",
    "            if data._name == \"S&P 500\":  # Skip the benchmark\n",
    "                continue\n",
    "            if data.signal[0] == 1:\n",
    "                self.buy(data)\n",
    "            elif data.signal[0] == -1:\n",
    "                self.sell(data)\n",
    "\n",
    "        if self.rebalance_days == 0:\n",
    "            weights = optimize_weights([data for data in self.datas if data._name != \"S&P 500\"])\n",
    "            for asset, weight in weights.items():\n",
    "                if weight > 0.30:\n",
    "                    weights[asset] = 0.30\n",
    "            \n",
    "            for data in self.datas:\n",
    "                if data._name == \"S&P 500\":\n",
    "                    continue\n",
    "                if data._name in weights:\n",
    "                    self.order_target_percent(data, target=weights[data._name])\n",
    "                else:\n",
    "                    self.close(data)\n",
    "            self.rebalance_days = 20\n",
    "        else:\n",
    "            self.rebalance_days -= 1\n",
    "\n",
    "        benchmark_return = (self.benchmark_data.close[0] - self.benchmark_data.close[-1]) / self.benchmark_data.close[-1]\n",
    "        self.log(f\"Benchmark Return: {benchmark_return * 100:.2f}%\")\n",
    "\n",
    "# Fetch S&P 500 data using yfinance\n",
    "def fetch_data(ticker, start_date, end_date):\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return df\n",
    "\n",
    "# Assume preds is defined somewhere earlier in your code\n",
    "start_date = preds.index.get_level_values(1).min()\n",
    "end_date = preds.index.get_level_values(1).max()\n",
    "sp500_data = fetch_data('^GSPC', start_date, end_date)\n",
    "\n",
    "# Convert it into Backtrader format\n",
    "benchmark = bt.feeds.PandasData(dataname=sp500_data, name=\"S&P 500\")\n",
    "\n",
    "cerebro = bt.Cerebro()\n",
    "cerebro.broker.setcommission(commission=0.001)\n",
    "cerebro.addanalyzer(btanalyzers.PyFolio, _name='pyfolio')\n",
    "cerebro.adddata(benchmark)\n",
    "\n",
    "your_data_dict = {ticker: preds.xs(ticker) for ticker in preds.index.get_level_values(0).unique()}\n",
    "for ticker, data_df in your_data_dict.items():\n",
    "    data = PandasPredictions(dataname=data_df, name=ticker)\n",
    "    cerebro.adddata(data)\n",
    "\n",
    "cerebro.addstrategy(TradeAndRebalanceStrategy)\n",
    "results = cerebro.run()\n",
    "\n",
    "# Performance Analysis\n",
    "returns, positions, transactions, gross_lev = results[0].analyzers.pyfolio.get_pf_items()\n",
    "\n",
    "# Calculate benchmark returns and add to returns DataFrame\n",
    "benchmark_rets = sp500_data['Adj Close'].pct_change().dropna()\n",
    "benchmark_rets.index = benchmark_rets.index.tz_localize(None)  # Remove timezone info\n",
    "returns.index = returns.index.tz_localize(None)  # Ensure the strategy returns index is also tz-naive\n",
    "returns = pd.concat([returns, benchmark_rets], axis=1)\n",
    "returns.columns = ['Strategy', 'Benchmark']\n",
    "\n",
    "# Pass benchmark_rets to create_full_tear_sheet\n",
    "pf.create_simple_tear_sheet(returns['Strategy'], benchmark_rets=returns['Benchmark'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
