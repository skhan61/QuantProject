{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 101 Formulaic Alphas\n",
    "- Based on [101 Formulaic Alphas](https://arxiv.org/pdf/1601.00991.pdf), Zura Kakushadze, arxiv, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "top = 10  # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:18.430405Z",
     "iopub.status.busy": "2023-09-18T15:14:18.430336Z",
     "iopub.status.idle": "2023-09-18T15:14:18.564801Z",
     "shell.execute_reply": "2023-09-18T15:14:18.564458Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:18.566457Z",
     "iopub.status.busy": "2023-09-18T15:14:18.566300Z",
     "iopub.status.idle": "2023-09-18T15:14:18.933797Z",
     "shell.execute_reply": "2023-09-18T15:14:18.933428Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection \\\n",
    "        import mutual_info_regression\n",
    "        \n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:18.935497Z",
     "iopub.status.busy": "2023-09-18T15:14:18.935386Z",
     "iopub.status.idle": "2023-09-18T15:14:18.937150Z",
     "shell.execute_reply": "2023-09-18T15:14:18.936889Z"
    }
   },
   "outputs": [],
   "source": [
    "idx= pd.IndexSlice\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:18.938627Z",
     "iopub.status.busy": "2023-09-18T15:14:18.938440Z",
     "iopub.status.idle": "2023-09-18T15:14:19.112260Z",
     "shell.execute_reply": "2023-09-18T15:14:19.111857Z"
    }
   },
   "outputs": [],
   "source": [
    "ohlcv = ['open', 'high', 'low', 'close', 'volume', 'market_cap']\n",
    "\n",
    "DATA_STORE = Path('/home/sayem/Desktop/Project/data/assets.h5')\n",
    "\n",
    "lock_path = \"/tmp/assets_h5_file.lock\"  # Choose a path for the lock file\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "top = 500\n",
    "\n",
    "with FileLock(lock_path):\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        data = (store[f'data/top{top}_dataset']\n",
    "                .loc[:, ohlcv + ['ret_01d', 'sector', 'ret_fwd_01d']]\n",
    "                .rename(columns={'ret_01d': 'returns'})\n",
    "                .sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:19.114022Z",
     "iopub.status.busy": "2023-09-18T15:14:19.113942Z",
     "iopub.status.idle": "2023-09-18T15:14:19.115564Z",
     "shell.execute_reply": "2023-09-18T15:14:19.115342Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Meta Labeling\n",
    "# data_ohlcv = data[['open', 'high', 'low', 'close', 'volume']].copy()\n",
    "# T = data['ret_fwd_01d'].std()\n",
    "# data['primary_target'] = (data['ret_fwd_01d'].abs() > T).astype(int)\n",
    "\n",
    "# data['secondary_target'] = np.where(data['primary_target'] == 1, \n",
    "#                                     np.where(data['ret_fwd_01d'] > 0, 1, -1),\n",
    "#                                     np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:19.117035Z",
     "iopub.status.busy": "2023-09-18T15:14:19.116841Z",
     "iopub.status.idle": "2023-09-18T15:14:19.281738Z",
     "shell.execute_reply": "2023-09-18T15:14:19.281501Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def compute_targets(data):\n",
    "#     # Primary model's target\n",
    "#     threshold = 0.01  # Threshold for returns; for demonstration purposes\n",
    "#     data['primary_target'] = np.where(data['ret_fwd_01d'].abs() > threshold, 1, 0)\n",
    "\n",
    "#     # Secondary model's target\n",
    "#     data['secondary_target'] = np.where(data['ret_fwd_01d'] > 0, 1, -1)\n",
    "#     data['secondary_target'] = data['secondary_target'] * data['primary_target']\n",
    "\n",
    "#     return data\n",
    "\n",
    "# def plot_targets(data, ticker, start_date, end_date):\n",
    "#     # Filter data for the given ticker and time frame\n",
    "#     data_filtered = compute_targets(data.loc[ticker].loc[start_date:end_date])\n",
    "\n",
    "#     # Set up the plot\n",
    "#     fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "#     # Plotting primary target\n",
    "#     line1, = ax1.step(data_filtered.index, data_filtered['primary_target'], \\\n",
    "#                       label='Primary Target', where='mid', color='blue')\n",
    "\n",
    "#     # Right axis for secondary target\n",
    "#     ax2 = ax1.twinx()\n",
    "#     line2, = ax2.step(data_filtered.index, data_filtered['secondary_target'], \\\n",
    "#                       color='red', label='Secondary Target', where='mid', linestyle='--')\n",
    "    \n",
    "#     ax1.set_title(f'Targets for {ticker} ({start_date} to {end_date})', fontsize=16)\n",
    "#     ax1.set_xlabel('Date', fontsize=14)\n",
    "    \n",
    "#     # Left axis for primary target\n",
    "#     ax1.set_ylim(0, 1)\n",
    "#     ax1.set_yticks([0, 1])\n",
    "#     ax1.set_yticklabels(['No Bet', 'Bet'], fontsize=12)\n",
    "\n",
    "#     # Right axis for secondary target\n",
    "#     ax2.set_ylim(-1, 1)\n",
    "#     ax2.set_yticks([-1, 0, 1])\n",
    "#     ax2.set_yticklabels(['Short', '', 'Long'], fontsize=12)\n",
    "\n",
    "#     # Adjusting legend\n",
    "#     lines = [line1, line2]\n",
    "#     labels = [line.get_label() for line in lines]\n",
    "#     ax1.legend(lines, labels, loc='upper left')\n",
    "\n",
    "#     # Pad the spacing between the two y-axes\n",
    "#     ax2.spines['right'].set_position(('outward', 60))  \n",
    "#     ax2.yaxis.set_ticks_position('right')\n",
    "#     ax2.yaxis.set_label_position('right')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # To use the function:\n",
    "# plot_targets(data, 'AA', '2013-01-01', '2013-01-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:30.601202Z",
     "iopub.status.busy": "2023-09-18T15:14:30.601128Z",
     "iopub.status.idle": "2023-09-18T15:14:31.302027Z",
     "shell.execute_reply": "2023-09-18T15:14:31.301622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate rolling average volumes\n",
    "data['adv10'] = data.groupby('ticker').rolling(10).volume.mean().reset_index(0, drop=True)\n",
    "data['adv20'] = data.groupby('ticker').rolling(20).volume.mean().reset_index(0, drop=True)\n",
    "data['adv50'] = data.groupby('ticker').rolling(50).volume.mean().reset_index(0, drop=True)\n",
    "data['adv81'] = data.groupby('ticker').rolling(81).volume.mean().reset_index(0, drop=True)\n",
    "data['adv150'] = data.groupby('ticker').rolling(150).volume.mean().reset_index(0, drop=True)\n",
    "adv180 = data['adv180'] = data.groupby('ticker').rolling(180).volume.mean().reset_index(0, drop=True)\n",
    "\n",
    "# Ensure no duplicate indices\n",
    "data = data[~data.index.duplicated(keep='first')]\n",
    "\n",
    "# Unstacking once\n",
    "o = data.open.unstack('ticker')\n",
    "h = data.high.unstack('ticker')\n",
    "l = data.low.unstack('ticker')\n",
    "c = data.close.unstack('ticker')\n",
    "v = data.volume.unstack('ticker')\n",
    "r = data.returns.unstack('ticker')\n",
    "sector = data.sector.unstack('ticker')\n",
    "market_cap = data.market_cap.unstack('ticker')\n",
    "\n",
    "# Compute vwap only once\n",
    "vwap = (o + h + l + c) / 4\n",
    "\n",
    "# Assemble the datasets dictionary\n",
    "datasets = {\n",
    "    'o': o,\n",
    "    'h': h,\n",
    "    'l': l,\n",
    "    'c': c,\n",
    "    'v': v,\n",
    "    'r': r,\n",
    "    'adv10': data['adv10'].unstack('ticker'),\n",
    "    'adv20': data['adv20'].unstack('ticker'),\n",
    "    'adv50': data['adv50'].unstack('ticker'),\n",
    "    'adv81': data['adv81'].unstack('ticker'),\n",
    "    'adv150': data['adv150'].unstack('ticker'),\n",
    "    'adv180': data['adv180'].unstack('ticker'),\n",
    "    'vwap': vwap,\n",
    "    'sector': sector,\n",
    "    'market_cap': market_cap\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:14:31.303765Z",
     "iopub.status.busy": "2023-09-18T15:14:31.303694Z",
     "iopub.status.idle": "2023-09-18T15:14:31.306120Z",
     "shell.execute_reply": "2023-09-18T15:14:31.305888Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def apply_alpha_function(alpha_number, datasets):\n",
    "    \"\"\"\n",
    "    Apply the given alpha function (by number) to the datasets.\n",
    "\n",
    "    Args:\n",
    "    - alpha_number (int): The number of the alpha function to apply (e.g., 1, 2, 3...)\n",
    "    - datasets (dict): A dictionary containing the available datasets.\n",
    "\n",
    "    Returns:\n",
    "    - The result of the alpha function applied to the datasets.\n",
    "    \"\"\"\n",
    "    # Get the alpha function based on the number\n",
    "    alpha_func_name = f'alpha{alpha_number:03}'\n",
    "    if alpha_func_name not in globals():\n",
    "        raise ValueError(f\"No function named {alpha_func_name} found!\")\n",
    "    alpha_func = globals()[alpha_func_name]  # Fetch directly from globals dictionary\n",
    "\n",
    "    # Get the list of arguments that the function expects\n",
    "    expected_args = inspect.signature(alpha_func).parameters.keys()\n",
    "\n",
    "    # Filter the datasets to only include the expected arguments\n",
    "    filtered_datasets = {k: v for k, v in datasets.items() if k in expected_args}\n",
    "\n",
    "    # Apply the function using ** to unpack the dictionary into function arguments\n",
    "    return alpha_func(**filtered_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 001 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 002 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 003 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 004 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 005 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 006 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 007 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 008 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 009 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 010 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 011 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 012 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 013 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 014 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 015 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 016 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 017 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 018 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 019 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 020 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 021 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 022 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 023 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 024 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 025 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 026 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 027 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 028 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 029 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 030 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 031 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 032 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 033 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 034 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 035 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 036 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 037 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 038 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 039 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 040 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 041 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 042 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 043 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 044 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 045 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 046 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 047 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 048 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 049 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 050 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 051 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 052 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 053 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 054 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 055 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 056 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 057 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 058 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 059. Check the logs for details.\n",
      "Alpha 060 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 061 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 062 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 063. Check the logs for details.\n",
      "Alpha 064 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 065 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 066 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 067. Check the logs for details.\n",
      "Alpha 068 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 069. Check the logs for details.\n",
      "Error applying Alpha 070. Check the logs for details.\n",
      "Alpha 071 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 072 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 073 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 074. Check the logs for details.\n",
      "Alpha 075 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 076. Check the logs for details.\n",
      "Alpha 077 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 078 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 079. Check the logs for details.\n",
      "Error applying Alpha 080. Check the logs for details.\n",
      "Alpha 081 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 082 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 083. Check the logs for details.\n",
      "Alpha 084 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 085 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 086 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 087. Check the logs for details.\n",
      "Alpha 088 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 089. Check the logs for details.\n",
      "Error applying Alpha 090. Check the logs for details.\n",
      "Error applying Alpha 091. Check the logs for details.\n",
      "Alpha 092 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 093. Check the logs for details.\n",
      "Alpha 094 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 095 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 096. Check the logs for details.\n",
      "Alpha 097 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 098 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Alpha 099 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "Error applying Alpha 100. Check the logs for details.\n",
      "Alpha 101 applied successfully and stored in 'alphas_p' DataFrame!\n",
      "84 out of 101 alphas have been added to the 'alphas_p' DataFrame.\n",
      "Alphas with errors: 59, 63, 67, 69, 70, 74, 76, 79, 80, 83, 87, 89, 90, 91, 93, 96, 100\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os  \n",
    "from formulaic_alphas import *\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"alpha_errors.log\", level=logging.ERROR)\n",
    "\n",
    "# [Your dataset definitions here...]\n",
    "\n",
    "alphas_p = data[['returns', 'ret_fwd_01d']].copy()\n",
    "error_alphas = []\n",
    "\n",
    "def apply_alpha_and_store(alpha):\n",
    "    try:\n",
    "        result = apply_alpha_function(alpha, datasets)\n",
    "        return alpha, result, None  # None indicates no error\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error applying Alpha {alpha:03}: {str(e)}\")\n",
    "        return alpha, None, str(e)  # Return the error message\n",
    "\n",
    "success_count = 0\n",
    "\n",
    "# Number of processes to use for parallelization\n",
    "num_processes = min(15, os.cpu_count())  # Adjust this as needed\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "    # Use the executor to map the function over the alphas\n",
    "    for alpha, result, error in executor.map(apply_alpha_and_store, range(1, 102)):\n",
    "        if error is None:\n",
    "            alphas_p[f'alpha_{alpha:03}'] = result\n",
    "            success_count += 1\n",
    "            print(f\"Alpha {alpha:03} applied successfully and stored in 'alphas_p' DataFrame!\")\n",
    "        else:\n",
    "            error_alphas.append(alpha)\n",
    "            print(f\"Error applying Alpha {alpha:03}. Check the logs for details.\")\n",
    "\n",
    "# Print the summary\n",
    "print(f\"{success_count} out of 101 alphas have been added to the 'alphas_p' DataFrame.\")\n",
    "if error_alphas:\n",
    "    print(f\"Alphas with errors: {', '.join(map(str, error_alphas))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:41:59.750363Z",
     "iopub.status.busy": "2023-09-18T15:41:59.750175Z",
     "iopub.status.idle": "2023-09-18T15:45:26.568013Z",
     "shell.execute_reply": "2023-09-18T15:45:26.567691Z"
    }
   },
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# import os  \n",
    "# from formulaic_alphas import *\n",
    "\n",
    "# alphas_p = data[['returns', 'ret_fwd_01d']].copy()\n",
    "\n",
    "# def apply_alpha_and_store(alpha):\n",
    "#     try:\n",
    "#         result = apply_alpha_function(alpha, datasets)\n",
    "#         return alpha, result, None  # None indicates no error\n",
    "#     except Exception as e:\n",
    "#         return alpha, None, str(e)  # Return the error message\n",
    "\n",
    "# success_count = 0\n",
    "\n",
    "# # Number of processes to use for parallelization\n",
    "# num_processes = min(15, os.cpu_count())  # Adjust this as needed\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "#     # Use the executor to map the function over the alphas\n",
    "#     for alpha, result, error in executor.map(apply_alpha_and_store, range(1, 102)):\n",
    "#         if error is None:\n",
    "#             alphas_p[f'alpha_{alpha:03}'] = result\n",
    "#             success_count += 1\n",
    "#             print(f\"Alpha {alpha:03} applied successfully and stored in 'alphas_p' DataFrame!\")\n",
    "#         else:\n",
    "#             print(f\"Error applying Alpha {alpha:03}: {error}\")\n",
    "\n",
    "# # Print the summary\n",
    "# print(f\"{success_count} out of 101 alphas have been added to the 'alphas_p' DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:45:26.569536Z",
     "iopub.status.busy": "2023-09-18T15:45:26.569460Z",
     "iopub.status.idle": "2023-09-18T15:45:31.082657Z",
     "shell.execute_reply": "2023-09-18T15:45:31.082295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data memory before optimization: 901.43 MB\n",
      "Data memory after optimization: 376.09 MB\n",
      "Reduced by: 58.28%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from utils import optimize_dataframe\n",
    "\n",
    "df_optimized = optimize_dataframe(alphas_p.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:45:31.084069Z",
     "iopub.status.busy": "2023-09-18T15:45:31.084000Z",
     "iopub.status.idle": "2023-09-18T15:45:31.315882Z",
     "shell.execute_reply": "2023-09-18T15:45:31.315471Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put(f'factor/top{top}_dataset_alpha101', \\\n",
    "        df_optimized, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-18T15:45:31.317592Z",
     "iopub.status.busy": "2023-09-18T15:45:31.317520Z",
     "iopub.status.idle": "2023-09-18T15:45:31.323696Z",
     "shell.execute_reply": "2023-09-18T15:45:31.323439Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import clear_large_vars\n",
    "clear_large_vars(threshold_size_in_MB=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
