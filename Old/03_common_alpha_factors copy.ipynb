{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Alpha Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:12:02.851606Z",
     "start_time": "2021-04-16T16:12:02.849869Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:12:03.608774Z",
     "start_time": "2021-04-16T16:12:02.854679Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from sklearn.preprocessing import scale\n",
    "import talib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:12:03.611665Z",
     "start_time": "2021-04-16T16:12:03.609797Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "idx = pd.IndexSlice\n",
    "deciles = np.arange(.1, 1, .1).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:12:03.937536Z",
     "start_time": "2021-04-16T16:12:03.612758Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_STORE = Path('/home/sayem/Desktop/Project/data/assets.h5')\n",
    "\n",
    "lock_path = \"/tmp/assets_h5_file.lock\"  # Choose a path for the lock file\n",
    "\n",
    "top = 250\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "with FileLock(lock_path):\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        data = store[f'data/top{top}_dataset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 671463 entries, ('AA', Timestamp('2013-01-03 00:00:00')) to ('ZTS', Timestamp('2023-08-11 00:00:00'))\n",
      "Data columns (total 31 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   open                671463 non-null  float32\n",
      " 1   high                671463 non-null  float32\n",
      " 2   low                 671463 non-null  float32\n",
      " 3   close               671463 non-null  float32\n",
      " 4   volume              671463 non-null  float64\n",
      " 5   market_cap          671463 non-null  float64\n",
      " 6   sector              671463 non-null  float32\n",
      " 7   ret_frac_order      671463 non-null  float32\n",
      " 8   ret_01d             671463 non-null  float32\n",
      " 9   ret_02d             671463 non-null  float32\n",
      " 10  ret_03d             671463 non-null  float32\n",
      " 11  ret_04d             671463 non-null  float32\n",
      " 12  ret_05d             671463 non-null  float32\n",
      " 13  ret_10d             671463 non-null  float32\n",
      " 14  ret_21d             671463 non-null  float32\n",
      " 15  ret_42d             671463 non-null  float32\n",
      " 16  ret_63d             671463 non-null  float32\n",
      " 17  ret_126d            671463 non-null  float32\n",
      " 18  ret_252d            671463 non-null  float32\n",
      " 19  ret_fwd_frac_order  671463 non-null  float32\n",
      " 20  ret_fwd_01d         671463 non-null  float32\n",
      " 21  ret_fwd_02d         671463 non-null  float32\n",
      " 22  ret_fwd_03d         671463 non-null  float32\n",
      " 23  ret_fwd_04d         671463 non-null  float32\n",
      " 24  ret_fwd_05d         671463 non-null  float32\n",
      " 25  ret_fwd_10d         671463 non-null  float32\n",
      " 26  ret_fwd_21d         671463 non-null  float32\n",
      " 27  ret_fwd_42d         671463 non-null  float32\n",
      " 28  ret_fwd_63d         671463 non-null  float32\n",
      " 29  ret_fwd_126d        671463 non-null  float32\n",
      " 30  ret_fwd_252d        671463 non-null  float32\n",
      "dtypes: float32(29), float64(2)\n",
      "memory usage: 87.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# # with pd.HDFStore(DATA_STORE) as store:\n",
    "# #     store.put('factors/common', df_optimized)\n",
    "# data.rename(columns={'market cap': \\\n",
    "#     'market_cap'}, inplace=True)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TA-Lib: Function Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List number of available functions by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:12:03.941120Z",
     "start_time": "2021-04-16T16:12:03.938923Z"
    }
   },
   "outputs": [],
   "source": [
    "function_groups = ['Overlap Studies',\n",
    "                   'Momentum Indicators',\n",
    "                   'Volume Indicators',\n",
    "                   'Volatility Indicators',\n",
    "                   'Price Transform',\n",
    "                   'Cycle Indicators',\n",
    "                   'Pattern Recognition',\n",
    "                   'Statistic Functions',\n",
    "                   'Math Transform',\n",
    "                   'Math Operators']\n",
    "\n",
    "talib_grps = talib.get_function_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 indicators added successfully.\n",
      "0 indicators failed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import talib.abstract as ta\n",
    "import logging\n",
    "import talib\n",
    "import numpy as np\n",
    "\n",
    "# The 5 days can be seen as a very short-term trend.\n",
    "# The 21 days can be used to determine the medium-term trend.\n",
    "# The 63 days can be viewed as a longer-term trend.\n",
    "\n",
    "def compute_talib_indicators(df, function_groups, timeperiods=[5, 21, 63]):\n",
    "    \"\"\"\n",
    "    Compute indicators for the specified function groups using TA-Lib's Abstract API.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns 'open', 'high', 'low', 'close', and optionally 'volume'.\n",
    "    - function_groups: List of function groups to compute.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Organize data in the format needed for TA-Lib Abstract API\n",
    "    inputs = {\n",
    "        'open': df['open'].astype(float).values,\n",
    "        'high': df['high'].astype(float).values,\n",
    "        'low': df['low'].astype(float).values,\n",
    "        'close': df['close'].astype(float).values,\n",
    "        'volume': df['volume'].astype(float).values if \\\n",
    "            'volume' in df.columns else np.random.random(len(df))\n",
    "    }\n",
    "\n",
    "    talib_functions = {k: v for k, v in talib.get_function_groups().items() if k in function_groups}\n",
    "\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    \n",
    "    for group, indicators in talib_functions.items():\n",
    "        for indicator_name in indicators:\n",
    "            for timeperiod in timeperiods:\n",
    "                try:\n",
    "                    indicator_func = ta.Function(indicator_name)\n",
    "\n",
    "                    parameters = {}\n",
    "                    if 'timeperiod' in indicator_func.parameters:\n",
    "                        parameters['timeperiod'] = timeperiod\n",
    "\n",
    "                    if indicator_name == 'MAVP':\n",
    "                        inputs['periods'] = np.full(len(df), timeperiod, dtype=np.float64)\n",
    "\n",
    "                    outputs = indicator_func(inputs, **parameters)\n",
    "\n",
    "                    # For multi-output functions, like 'BBANDS'\n",
    "                    if isinstance(outputs, (list, tuple)):\n",
    "                        for i, out_name in enumerate(indicator_func.output_names):\n",
    "                            col_name = f\"{out_name}_{timeperiod}\".upper()\n",
    "                            df[col_name] = outputs[i]\n",
    "                    else:\n",
    "                        col_name = f\"{indicator_name}_{timeperiod}\".upper()\n",
    "                        df[col_name] = outputs\n",
    "\n",
    "                    success_count += 1\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"{indicator_name}_{timeperiod} failed due to: {str(e)}\")\n",
    "                    failure_count += 1\n",
    "\n",
    "    print(f\"{success_count} indicators added successfully.\")\n",
    "    print(f\"{failure_count} indicators failed.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "function_groups = [\n",
    "    'Overlap Studies',\n",
    "    'Momentum Indicators',\n",
    "    'Volume Indicators',\n",
    "    'Volatility Indicators',\n",
    "    'Price Transform',\n",
    "    'Cycle Indicators',\n",
    "    'Pattern Recognition',\n",
    "    'Statistic Functions',\n",
    "    'Math Transform',\n",
    "    'Math Operators'\n",
    "]\n",
    "\n",
    "# Assuming you've read your dataframe into a variable named 'data'\n",
    "data_ta = compute_talib_indicators(data.copy(), function_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 671463 entries, ('AA', Timestamp('2013-01-03 00:00:00')) to ('ZTS', Timestamp('2023-08-11 00:00:00'))\n",
      "Columns: 526 entries, open to OBV_63\n",
      "dtypes: float32(29), float64(299), int32(198)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "data_ta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data memory before optimization: 2115.85 MB\n",
      "Data memory after optimization: 1414.01 MB\n",
      "Reduced by: 33.17%\n"
     ]
    }
   ],
   "source": [
    "from utils import optimize_dataframe\n",
    "data_ta = optimize_dataframe(data_ta.copy())\n",
    "data_ta.to_hdf(DATA_STORE, f'factor/top{top}_dataset_with_TA', \\\n",
    "    format='table', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 671463 entries, ('AA', Timestamp('2013-01-03 00:00:00')) to ('ZTS', Timestamp('2023-08-11 00:00:00'))\n",
      "Columns: 520 entries, open to OBV_63\n",
      "dtypes: float32(291), float64(31), int32(198)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "data_ta.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rolling Factor Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:13:42.828286Z",
     "start_time": "2021-04-16T16:13:41.429910Z"
    }
   },
   "outputs": [],
   "source": [
    "factor_data = (web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 'famafrench', \n",
    "                              start=2005)[0].rename(columns={'Mkt-RF': 'MARKET'}))\n",
    "factor_data.index.names = ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:13:42.832172Z",
     "start_time": "2021-04-16T16:13:42.829107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MARKET', 'SMB', 'HML', 'RMW', 'CMA'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors = factor_data.columns[:-1]\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:15:05.025886Z",
     "start_time": "2021-04-16T16:13:42.833266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "63\n",
      "252\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "# ret = f'ret_{t:02}d'\n",
    "ret = 'ret_frac_order'\n",
    "windows = [21, 63, 252]\n",
    "for window in windows:\n",
    "    print(window)\n",
    "    betas = []\n",
    "    for ticker, df in data.groupby('ticker', group_keys=False):\n",
    "        model_data = df[[ret]].merge(factor_data, on='date').dropna()\n",
    "        model_data[ret] -= model_data.RF\n",
    "\n",
    "        rolling_ols = RollingOLS(endog=model_data[ret], \n",
    "                                 exog=sm.add_constant(model_data[factors]), window=window)\n",
    "        factor_model = rolling_ols.fit(params_only=True).params.rename(columns={'const':'ALPHA'})\n",
    "        result = factor_model.assign(ticker=ticker).set_index('ticker', append=True).swaplevel()\n",
    "        betas.append(result)\n",
    "    betas = pd.concat(betas).rename(columns=lambda x: f'{x}_{window:02}')\n",
    "    data = data.join(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Size proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:15:05.552334Z",
     "start_time": "2021-04-16T16:15:05.026793Z"
    }
   },
   "outputs": [],
   "source": [
    "by_ticker = data.groupby('ticker', group_keys=False)\n",
    "data['size_factor'] = by_ticker.close.apply(lambda x: x.fillna(method='bfill').div(x.iloc[0]))\n",
    "data['size_proxy'] = data['market_cap'].mul(data.size_factor).div(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data memory before optimization: 1140.39 MB\n",
      "Data memory after optimization: 859.23 MB\n",
      "Reduced by: 24.66%\n"
     ]
    }
   ],
   "source": [
    "from utils import optimize_dataframe\n",
    "data = optimize_dataframe(data.copy())\n",
    "data.to_hdf(DATA_STORE, \\\n",
    "    f'factor/top{top}_dataset_with_rolling_beta_size_proxy', \\\n",
    "        format='table', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4094783 entries, ('AA', Timestamp('2013-01-03 00:00:00')) to ('ZTS', Timestamp('2023-08-11 00:00:00'))\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   open                float32\n",
      " 1   high                float32\n",
      " 2   low                 float32\n",
      " 3   close               float32\n",
      " 4   volume              float64\n",
      " 5   market_cap          float64\n",
      " 6   sector              float32\n",
      " 7   ret_frac_order      float32\n",
      " 8   ret_01d             float32\n",
      " 9   ret_02d             float32\n",
      " 10  ret_03d             float32\n",
      " 11  ret_04d             float32\n",
      " 12  ret_05d             float32\n",
      " 13  ret_10d             float32\n",
      " 14  ret_21d             float32\n",
      " 15  ret_42d             float32\n",
      " 16  ret_63d             float32\n",
      " 17  ret_126d            float32\n",
      " 18  ret_252d            float32\n",
      " 19  ret_fwd_frac_order  float32\n",
      " 20  ret_fwd_01d         float32\n",
      " 21  ret_fwd_02d         float32\n",
      " 22  ret_fwd_03d         float32\n",
      " 23  ret_fwd_04d         float32\n",
      " 24  ret_fwd_05d         float32\n",
      " 25  ret_fwd_10d         float32\n",
      " 26  ret_fwd_21d         float32\n",
      " 27  ret_fwd_42d         float32\n",
      " 28  ret_fwd_63d         float32\n",
      " 29  ret_fwd_126d        float32\n",
      " 30  ret_fwd_252d        float32\n",
      " 31  ALPHA_21            float32\n",
      " 32  MARKET_21           float32\n",
      " 33  SMB_21              float32\n",
      " 34  HML_21              float32\n",
      " 35  RMW_21              float32\n",
      " 36  CMA_21              float32\n",
      " 37  ALPHA_63            float32\n",
      " 38  MARKET_63           float32\n",
      " 39  SMB_63              float32\n",
      " 40  HML_63              float32\n",
      " 41  RMW_63              float32\n",
      " 42  CMA_63              float32\n",
      " 43  ALPHA_252           float32\n",
      " 44  MARKET_252          float32\n",
      " 45  SMB_252             float32\n",
      " 46  HML_252             float32\n",
      " 47  RMW_252             float32\n",
      " 48  CMA_252             float32\n",
      " 49  size_factor         float32\n",
      " 50  size_proxy          float64\n",
      "dtypes: float32(48), float64(3)\n",
      "memory usage: 859.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Project/03_common_alpha_factors.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/03_common_alpha_factors.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m STOP\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import clear_large_vars\n",
    "clear_large_vars(threshold_size_in_MB=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to your data store\n",
    "DATA_STORE = Path('/home/sayem/Desktop/Project/data/assets.h5')\n",
    "CHUNK_SIZE = 10 ** 5  # Define the chunk size based on available memory\n",
    "\n",
    "# Load the secondary dataset just once for efficiency\n",
    "beta_size_data = pd.read_hdf(DATA_STORE, 'data/top500_dataset_with_rolling_beta_size_proxy')\n",
    "\n",
    "# Read the primary dataset in chunks\n",
    "ta_data_chunks = pd.read_hdf(DATA_STORE, 'data/top500_dataset_with_TA', chunksize=CHUNK_SIZE)\n",
    "\n",
    "# Prepare an empty list to collect processed chunks\n",
    "processed_data_list = []\n",
    "\n",
    "# # Display the initial sizes of datasets\n",
    "# print(f\"Initial shape of beta_size_data: {beta_size_data.shape}\")\n",
    "# print(f\"Initial shape of each ta_data_chunks: {CHUNK_SIZE} rows\")\n",
    "\n",
    "for chunk in ta_data_chunks:\n",
    "    \n",
    "    # Calculate common columns count for this chunk\n",
    "    common_columns_count = len(set(beta_size_data.columns).intersection(set(chunk.columns)))\n",
    "    \n",
    "    # Indicate progress\n",
    "    start_val = (chunk.index.get_level_values('ticker')[0], chunk.index.get_level_values('date')[0])\n",
    "    end_val = (chunk.index.get_level_values('ticker')[-1], chunk.index.get_level_values('date')[-1])\n",
    "    print(f\"Processing rows from {start_val} to {end_val}...\")\n",
    "    \n",
    "    # Extract unique tickers and dates from the current chunk\n",
    "    tickers_in_chunk = chunk.index.get_level_values('ticker').unique()\n",
    "    dates_in_chunk = chunk.index.get_level_values('date').unique()\n",
    "    \n",
    "    # Filter beta_size_data based on tickers and dates in the current chunk\n",
    "    filtered_beta_size_data = beta_size_data[\n",
    "        beta_size_data.index.get_level_values('ticker').isin(tickers_in_chunk) &\n",
    "        beta_size_data.index.get_level_values('date').isin(dates_in_chunk)\n",
    "    ]\n",
    "    \n",
    "    # Merge chunk with filtered data\n",
    "    merged_chunk = chunk.merge(\n",
    "        filtered_beta_size_data, left_index=True, right_index=True, how='inner', suffixes=('', '_y')\n",
    "    )\n",
    "    \n",
    "    # Drop \"_y\" suffixed columns (as they come from the secondary dataset)\n",
    "    merged_chunk = merged_chunk.drop(columns=[col for col in merged_chunk if col.endswith('_y')])\n",
    "\n",
    "    processed_data_list.append(merged_chunk)\n",
    "\n",
    "# Concatenate all processed chunks\n",
    "final_data = pd.concat(processed_data_list)\n",
    "\n",
    "print(f\"Shape of the final combined data: {final_data.shape}\")\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def static_columns(df):\n",
    "    \"\"\"Generator that yields static columns of the dataframe.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            yield col\n",
    "\n",
    "def optimize_dataframe(df):\n",
    "    print(f\"Initial dataframe memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Remove static columns\n",
    "    cols_to_drop = list(static_columns(df))\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Memory after removing static columns: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Convert float64 to float32\n",
    "    float64_cols = df.select_dtypes(include=['float64']).columns\n",
    "    df[float64_cols] = df[float64_cols].apply(pd.to_numeric, downcast='float')\n",
    "    print(f\"Memory after converting float64 columns: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Convert int64 to int32 (or smaller)\n",
    "    int64_cols = df.select_dtypes(include=['int64']).columns\n",
    "    df[int64_cols] = df[int64_cols].apply(pd.to_numeric, downcast='integer')\n",
    "    print(f\"Memory after converting int64 columns: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    print(f\"Memory after filling NaN values: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    gc.collect()  # Explicitly run the garbage collector\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "optimized_df = optimize_dataframe(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def efficient_save_to_hdf(dataframe, file_path, key, chunk_size=10**2, complevel=5, \n",
    "                          complib='zlib', overwrite_existing_key=True):\n",
    "\n",
    "    with pd.HDFStore(file_path, mode='a', complevel=complevel, complib=complib) as store:\n",
    "        if overwrite_existing_key and key in store:\n",
    "            del store[key]\n",
    "        # Chunk-wise conversion and append to HDFStore\n",
    "        for i in range(0, len(dataframe), chunk_size):\n",
    "            chunk = dataframe.iloc[i:i+chunk_size].copy()\n",
    "            \n",
    "            print(f\"Appending chunk {i} to {i+chunk_size}\")\n",
    "            store.append(key, chunk, format='table', data_columns=True)\n",
    "    \n",
    "    print(f\"Data saved to {file_path} under key {key}\")\n",
    "# Use the function\n",
    "key = 'factors/common'\n",
    "efficient_save_to_hdf(optimized_df, DATA_STORE, \\\n",
    "    key, overwrite_existing_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clear_large_vars\n",
    "clear_large_vars(threshold_size_in_MB=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
