{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Filter out warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Index and deciles for data slicing\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths to the downloaded datasets, model, and hyperparameters\n",
    "data_dir = Path('data/')\n",
    "model_dir = Path('model/')\n",
    "best_hyperparams_dir = Path('best_hyperparams/')\n",
    "study_dir = Path('study/')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_hyperparams_dir.mkdir(parents=True, exist_ok=True)\n",
    "study_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# from utils import rank_stocks_and_quantile\n",
    "# # UNSEEN_KEY = '/data/YEAR_20220803_20230803'\n",
    "# top = 250  # parameters -> papermill\n",
    "# DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "# with pd.HDFStore(DATA_STORE) as store:\n",
    "#     # unseen = store[UNSEEN_KEY]\n",
    "#     print(store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process Large Financial Datasets from HDF5 Format.\n",
    "\n",
    "This script reads, processes, and normalizes financial datasets stored in an HDF5 format.\n",
    "The primary processing steps involve converting data types, handling infinite values, and\n",
    "scaling the dataset. The MinMaxScaler, computed from the entire dataset, is employed for normalization.\n",
    "Once data processing is complete, stocks are ranked, and quantiles are determined in post-processing.\n",
    "\n",
    "Attributes:\n",
    "    - top (int): Number of top stocks to consider.\n",
    "    - DATA_STORE (Path): Path to the HDF5 file containing the datasets.\n",
    "    - dataset_keys (list of str): Keys identifying which datasets to process in the HDF5 store.\n",
    "    - target_string (str): Target column identifier for post-processing.\n",
    "    - CHUNK_SIZE (int): Size of chunks in which data is read and processed.\n",
    "\n",
    "Functions:\n",
    "    - convert_dtype(chunk, feature_columns, dtype='float32'): Converts dtype of specified columns in a chunk.\n",
    "    - handle_infinite_values(chunk, feature_columns): Handles infinite values in a chunk.\n",
    "    - process_chunk(chunk, feature_columns, scaler=None): Process a single chunk with optional normalization.\n",
    "\n",
    "Workflow:\n",
    "    1. Set parameters and paths.\n",
    "    2. Define utility functions.\n",
    "    3. Identify features and target columns from the first chunk.\n",
    "    4. Determine the MinMaxScaler using all chunks in the dataset.\n",
    "    5. Process and concatenate chunks to form the dataset.\n",
    "    6. Rank stocks and compute quantiles in post-processing.\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils import rank_stocks_and_quantile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Parameters and data paths\n",
    "top = 250\n",
    "DATA_STORE = Path(f'data/{top}_dataset.h5')\n",
    "dataset_keys = [\n",
    "    '/data/YEAR_20200930_20220802',\n",
    "    # '/data/YEAR_20181024_20200929',\n",
    "    # '/data/YEAR_20161116_20181023',\n",
    "    # '/data/YEAR_20141210_20161115'\n",
    "]\n",
    "target_string = 'TARGET_ret_fwd'\n",
    "CHUNK_SIZE = 50000\n",
    "\n",
    "def convert_dtype(chunk, feature_columns, dtype='float32'):\n",
    "    \"\"\"Converts the datatype of the specified columns.\"\"\"\n",
    "    chunk[feature_columns] = chunk[feature_columns].astype(dtype)\n",
    "    return chunk\n",
    "\n",
    "def handle_infinite_values(chunk, feature_columns):\n",
    "    \"\"\"Handle infinite values by replacing them with the maximum finite value.\"\"\"\n",
    "    max_val = np.finfo('float32').max\n",
    "    chunk[feature_columns] = chunk[feature_columns].replace([np.inf, -np.inf], max_val)\n",
    "    return chunk\n",
    "\n",
    "def process_chunk(chunk, feature_columns, scaler=None):\n",
    "    \"\"\"Process a single chunk of data.\"\"\"\n",
    "    chunk = convert_dtype(chunk, feature_columns)\n",
    "    chunk = handle_infinite_values(chunk, feature_columns)\n",
    "    \n",
    "    # Normalize with scaler if provided\n",
    "    if scaler:\n",
    "        chunk[feature_columns] = scaler.transform(chunk[feature_columns])\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "# Identify features and targets based on the first chunk\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    first_chunk = store.select(dataset_keys[0], stop=CHUNK_SIZE)\n",
    "    features = [col for col in first_chunk.columns if col.startswith('FEATURE_')]\n",
    "    target = [col for col in first_chunk.columns if col.startswith('TARGET_')]\n",
    "\n",
    "# Determine the scaler using the entire dataset for the identified features\n",
    "scaler = MinMaxScaler()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            # Convert dtype and handle infinite values\n",
    "            chunk = convert_dtype(chunk, features)\n",
    "            chunk = handle_infinite_values(chunk, features)\n",
    "            scaler.partial_fit(chunk[features])\n",
    "\n",
    "# Process and concatenate chunks\n",
    "dataset = pd.DataFrame()\n",
    "for key in dataset_keys:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for chunk in store.select(key, chunksize=CHUNK_SIZE):\n",
    "            processed_chunk = process_chunk(chunk, features, scaler)\n",
    "            dataset = pd.concat([dataset, processed_chunk], ignore_index=False)\n",
    "            del processed_chunk\n",
    "            gc.collect()\n",
    "\n",
    "# Post-processing steps\n",
    "dataset = rank_stocks_and_quantile(dataset, target_substring=target_string)\n",
    "dataset.index.set_levels(dataset.index.levels[0].tz_localize(None), level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.head(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = None  # If you have a predefined value, set it here; otherwise, it gets calculated automatically.\n",
    "\n",
    "def pad_sequence(inputs, padding_value=-1, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([input.shape[0] for input in inputs])\n",
    "    padded_inputs = []\n",
    "    masks = []\n",
    "    for input in inputs:\n",
    "        pad_len = max_len - input.shape[0]\n",
    "        padded_input = F.pad(input, (0, 0, 0, pad_len), value=padding_value)\n",
    "        mask = torch.ones((input.shape[0], 1), dtype=torch.float)\n",
    "        masks.append(\n",
    "            torch.cat((mask, torch.zeros((pad_len, 1), dtype=torch.float)), dim=0)\n",
    "        )\n",
    "        padded_inputs.append(padded_input)\n",
    "    return torch.stack(padded_inputs), torch.stack(masks)\n",
    "\n",
    "def convert_to_torch(timestamp, data):\n",
    "    feature_names = [col for col in data.columns if col.startswith('FEATURE_')]\n",
    "    target_names = [col for col in data.columns if col.startswith('TARGET_')]\n",
    "    \n",
    "    inputs = torch.from_numpy(\n",
    "                data[feature_names].values.astype(np.float32))\n",
    "    labels = torch.from_numpy(\n",
    "                data[target_names].values.astype(np.float32))\n",
    "\n",
    "    padded_inputs, masks_inputs = pad_sequence(\n",
    "            [inputs], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "    padded_labels, masks_labels = pad_sequence(\n",
    "            [labels], padding_value=PADDING_VALUE, max_len=MAX_LEN)\n",
    "\n",
    "    return {\n",
    "        timestamp: (\n",
    "            padded_inputs,\n",
    "            padded_labels,\n",
    "            masks_inputs\n",
    "        )\n",
    "    }\n",
    "\n",
    "def get_era2data(df):\n",
    "    # Group by the Timestamp index (level=0)\n",
    "    res = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "        delayed(convert_to_torch)(timestamp, data)\n",
    "        for timestamp, data in tqdm(df.groupby(level=0)))\n",
    "    \n",
    "    era2data = {}\n",
    "    for r in tqdm(res):\n",
    "        era2data.update(r)\n",
    "    return era2data\n",
    "\n",
    "# Assuming your DataFrame is named \"dataset\"\n",
    "timestamp2data_dataset = get_era2data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "\n",
    "# PADDING_VALUE = -1\n",
    "# MAX_LEN = 500\n",
    "# FEATURE_DIM = len(features)\n",
    "# HIDDEN_DIM = 128\n",
    "# OUTPUT_DIM = len(target)\n",
    "# NUM_HEADS = 2\n",
    "# NUM_LAYERS = 2\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# from model import Transformer\n",
    "\n",
    "# def test_model():\n",
    "\n",
    "#     inputs = [\n",
    "#         torch.randint(0, 4, (5, FEATURE_DIM)).float(),\n",
    "#         torch.randint(0, 4, (3, FEATURE_DIM)).float(),\n",
    "#     ]\n",
    "#     labels = [\n",
    "#         torch.randint(0, 2, (5, OUTPUT_DIM)).float(),\n",
    "#         torch.randint(0, 2, (3, OUTPUT_DIM)).float(),\n",
    "#     ]\n",
    "\n",
    "#     padded_inputs, masks_inputs = pad_sequence(inputs, \\\n",
    "#                                                padding_value=0, max_len=MAX_LEN)\n",
    "#     padded_labels, masks_labels = pad_sequence(labels, \\\n",
    "#                                                padding_value=0, max_len=MAX_LEN)\n",
    "\n",
    "#     transformer = Transformer(\n",
    "#         input_dim=FEATURE_DIM,\n",
    "#         d_model=HIDDEN_DIM,\n",
    "#         output_dim=OUTPUT_DIM,\n",
    "#         num_heads=NUM_HEADS,\n",
    "#         num_layers=NUM_LAYERS,\n",
    "#         max_len=MAX_LEN,\n",
    "#     )\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = transformer(padded_inputs, masks_inputs)\n",
    "\n",
    "#     assert torch.isnan(outputs).sum() == 0\n",
    "#     assert outputs.shape[:2] == padded_inputs.shape[:2]\n",
    "#     assert outputs.shape[-1] == len(target)\n",
    "\n",
    "#     print(\"Input Shape (padded):\", padded_inputs.shape)\n",
    "#     print(\"Output Shape (padded):\", outputs.shape)\n",
    "\n",
    "#     del transformer\n",
    "#     del inputs, labels\n",
    "#     del padded_inputs, masks_inputs, padded_labels, masks_labels\n",
    "#     del outputs\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "# test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch \n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = 500\n",
    "FEATURE_DIM = len(features)\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1 # len(target[0])\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 128)        # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(128, output_dim) # Output layer\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x * mask  # This ensures that the outputs for padded positions are zero\n",
    "\n",
    "def test_simple_model():\n",
    "    inputs = [\n",
    "        torch.randint(0, 4, (5, FEATURE_DIM)).float(),\n",
    "        torch.randint(0, 4, (3, FEATURE_DIM)).float(),\n",
    "    ]\n",
    "\n",
    "    # Padding sequences to have the same length for batch processing\n",
    "    padded_inputs, masks_inputs = pad_sequence(inputs)\n",
    "\n",
    "    model = SimpleNN(FEATURE_DIM, OUTPUT_DIM)\n",
    "    outputs = model(padded_inputs, masks_inputs)\n",
    "\n",
    "    print(\"Input Shape:\", padded_inputs.shape)\n",
    "    print(\"Output Shape:\", outputs.shape)\n",
    "\n",
    "test_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearsonr in torch differentiable\n",
    "def pearsonr(x, y):\n",
    "    mx = x.mean()\n",
    "    my = y.mean()\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = torch.sum(xm * ym)\n",
    "    r_den = torch.sqrt(torch.sum(xm ** 2) * torch.sum(ym ** 2))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                padded_inputs=None, target_weight_softmax=None):\n",
    "    # print(\"Outputs shape:\", outputs.shape)\n",
    "    # print(\"Padded labels shape:\", padded_labels.shape)\n",
    "    # MSE on all targets; additionally, on primary target\n",
    "    if target_weight_softmax is not None:\n",
    "        _mse = criterion(\n",
    "            outputs * masks_inputs * target_weight_softmax,\n",
    "            padded_labels * masks_inputs * target_weight_softmax\n",
    "        ) * 0.1\n",
    "\n",
    "    else:\n",
    "        _mse = criterion(outputs * masks_inputs, padded_labels * masks_inputs) * 0.1\n",
    "\n",
    "    _mse += criterion(outputs[:, 0] * masks_inputs, padded_labels[:, 0] * masks_inputs)\n",
    "\n",
    "    # Corr with only primary target; adjust as needed\n",
    "    corr = pearsonr(\n",
    "        outputs[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "        padded_labels[0][:, 0][masks_inputs.view(-1).nonzero()].view(-1, 1),\n",
    "    )\n",
    "\n",
    "    loss = _mse - corr #+ some_complex_constraints\n",
    "    return loss, _mse, corr\n",
    "\n",
    "# Training loop\n",
    "def train_on_batch(model, criterion, optimizer, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    # print(padded_inputs.shape)\n",
    "    # print(padded_labels.shape)\n",
    "    # print(masks_inputs.shape)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(padded_inputs / 4.0, masks_inputs)\n",
    "    # print(\"Outputs shape:\", outputs.shape)\n",
    "    # print(\"Padded labels shape:\", padded_labels.shape)\n",
    "\n",
    "\n",
    "    target_weight_softmax = None\n",
    "    #random_weights = torch.rand(padded_labels.shape[-1], device=device)\n",
    "    #target_weight_softmax = F.softmax(random_weights)\n",
    "\n",
    "    loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs, \\\n",
    "                                       target_weight_softmax=target_weight_softmax)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), _mse.item(), _corr.item()\n",
    "\n",
    "\n",
    "def evaluate_on_batch(transformer, criterion, batch):\n",
    "\n",
    "    padded_inputs = batch[0].to(device=device)\n",
    "    padded_labels = batch[1].to(device=device)\n",
    "    masks_inputs = batch[2].to(device=device)\n",
    "\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(padded_inputs / 4.0, masks_inputs)\n",
    "        # print(outputs)\n",
    "        loss, _mse, _corr = calculate_loss(outputs, criterion, padded_labels, masks_inputs)\n",
    "        \n",
    "        # Convert outputs to numpy\n",
    "        preds = outputs[0][masks_inputs.view(-1).nonzero()].squeeze(1).cpu().numpy()\n",
    "        # print(preds)\n",
    "\n",
    "    return loss.item(), _mse.item(), _corr.item(), preds\n",
    "\n",
    "\n",
    "def metrics_on_batch(era_scores):\n",
    "    era_scores = pd.Series(era_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_correlation = np.mean(era_scores)\n",
    "    std_deviation = np.std(era_scores)\n",
    "    sharpe_ratio = mean_correlation / std_deviation\n",
    "    max_dd = (era_scores.cummax() - era_scores).max() # from calculate_metrics\n",
    "\n",
    "    # Smart Sharpe: Modified Sharpe ratio that also considers the instability of scores over time,\n",
    "    # penalizing models with high score instability even if their mean score is high\n",
    "    smart_sharpe = mean_correlation / (std_deviation + np.std(era_scores.diff()))\n",
    "    \n",
    "    # Autocorrelation: Measure of the correlation of the series with a lagged version of itself\n",
    "    autocorrelation = era_scores.autocorr()\n",
    "\n",
    "    metrics = pd.Series({\n",
    "        'mean_correlation': mean_correlation,\n",
    "        'std_deviation': std_deviation,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'smart_sharpe': smart_sharpe,\n",
    "        'autocorrelation': autocorrelation,\n",
    "        'max_dd': max_dd, # added from calculate_metrics\n",
    "        'min_correlation': era_scores.min(), # added from calculate_metrics\n",
    "        'max_correlation': era_scores.max(), # added from calculate_metrics\n",
    "    })\n",
    "\n",
    "    # Cleanup\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, \\\n",
    "                num_epochs, patience, train_loader, val_loader, is_lr_scheduler=True):\n",
    "    best_loss = float('inf')\n",
    "    best_corr = None\n",
    "    best_model = None\n",
    "    best_outputs = None\n",
    "    no_improve_epoch = 0\n",
    "\n",
    "    epoch_progress = tqdm(range(num_epochs), desc=\"Epochs\", position=0, leave=False)\n",
    "\n",
    "    for epoch in epoch_progress:\n",
    "        total_loss = []\n",
    "        total_corr = []\n",
    "\n",
    "        # Training\n",
    "        for era_num in tqdm(train_loader, desc=\"Training\", leave=False, position=1):\n",
    "            batch = train_loader[era_num]\n",
    "            loss, _mse, _corr = train_on_batch(model, criterion, optimizer, batch)\n",
    "            total_loss.append(loss)\n",
    "            total_corr.append(_corr)\n",
    "\n",
    "        # Adjust learning rate if is_lr_scheduler is True\n",
    "        if is_lr_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_total_loss = []\n",
    "        val_total_corr = []\n",
    "        val_total_outputs = {}\n",
    "        with torch.no_grad():\n",
    "            for era_num in tqdm(val_loader, desc=\"Validation\", leave=False, position=2):\n",
    "                batch = val_loader[era_num]\n",
    "                loss, _mse, _corr, outputs = evaluate_on_batch(model, criterion, batch)\n",
    "                # print(outputs)\n",
    "                val_total_loss.append(loss)\n",
    "                val_total_corr.append(_corr)\n",
    "                val_total_outputs[era_num] = outputs\n",
    "\n",
    "        # Early stopping check\n",
    "        val_loss = np.mean(val_total_loss)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_corr = val_total_corr.copy()\n",
    "            best_model = model.state_dict().copy()\n",
    "            best_outputs = val_total_outputs.copy()\n",
    "            no_improve_epoch = 0\n",
    "        else:\n",
    "            no_improve_epoch += 1\n",
    "            if no_improve_epoch >= patience:\n",
    "                epoch_progress.set_description(f'Early stopping at epoch {epoch+1}')\n",
    "                epoch_progress.refresh()\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        _ = gc.collect()\n",
    "\n",
    "    # # Save the best model state\n",
    "    # torch.save(best_model, data_dir / \"model_best.pth\")\n",
    "\n",
    "    return model, best_corr, best_outputs # best_outputs for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "PADDING_VALUE = -1\n",
    "MAX_LEN = 500\n",
    "FEATURE_DIM = len(features)\n",
    "print(FEATURE_DIM)\n",
    "OUTPUT_DIM = 1 # len(target[0])\n",
    "\n",
    "\n",
    "# Suggesting parameters\n",
    "train_length_multiplier = 10 # trial.suggest_int('train_length_multiplier', 8, 12)\n",
    "val_period_length = 21 # trial.suggest_categorical('val_period_length', [5, 10, 21])\n",
    "lookahead = 1 # trial.suggest_categorical('lookahead', [1, 5, 21])\n",
    "\n",
    "# Instantiate CV object with the suggested parameters\n",
    "cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length=int(21 * train_length_multiplier),\n",
    "                                        test_period_length=val_period_length, lookahead=lookahead,\n",
    "                                        date_idx='date')\n",
    "\n",
    "hidden_dim_1 = 128 # trial.suggest_int(\"hidden_dim_1\", 128, 512)\n",
    "hidden_dim_2 = 64 # trial.suggest_int(\"hidden_dim_2\", 64, 256)\n",
    "lr = 1e-5 # trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "model = SimpleNN(input_dim=FEATURE_DIM, output_dim=OUTPUT_DIM).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "sharpe_ratios = []\n",
    "NUM_EPOCHS = 1\n",
    "PATIENCE = 5\n",
    "for train_idx, val_idx in cv:\n",
    "    # print(train_idx)\n",
    "    train_data = dataset.iloc[train_idx]\n",
    "    val_data = dataset.iloc[val_idx]\n",
    "\n",
    "    era2data_train = get_era2data(train_data)\n",
    "    era2data_validation = get_era2data(val_data)\n",
    "\n",
    "    model, best_corr, outputs = train_model(model, criterion, optimizer, scheduler,\n",
    "                                            NUM_EPOCHS, PATIENCE, era2data_train, \n",
    "                                            era2data_validation, is_lr_scheduler=True)\n",
    "\n",
    "    # metrics = metrics_on_batch(best_corr)\n",
    "    # sharpe_ratios.append(metrics['sharpe_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Constants and hyperparameters\n",
    "NUM_EPOCHS = 1\n",
    "PATIENCE = 5\n",
    "FEATURE_DIM = len(features)\n",
    "OUTPUT_DIM = 1\n",
    "MODEL_DIR = \"/home/sayem/Desktop/Project/models\"  # Model directory\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def objective(trial, dataset=dataset):\n",
    "    print(f\"\\n--- Starting Trial: {trial.number + 1} ---\")\n",
    "\n",
    "    # Suggesting parameters\n",
    "    train_length_multiplier = trial.suggest_int('train_length_multiplier', 8, 12)\n",
    "    val_period_length = trial.suggest_categorical('val_period_length', [5, 10, 21])\n",
    "    lookahead = trial.suggest_categorical('lookahead', [1, 5, 21])\n",
    "\n",
    "    # Instantiate CV object with the suggested parameters\n",
    "    cv = CustomBackwardMultipleTimeSeriesCV(dataset, train_period_length=int(21 * train_length_multiplier),\n",
    "                                            test_period_length=val_period_length, lookahead=lookahead,\n",
    "                                            date_idx='date')\n",
    "\n",
    "    hidden_dim_1 = trial.suggest_int(\"hidden_dim_1\", 128, 512)\n",
    "    hidden_dim_2 = trial.suggest_int(\"hidden_dim_2\", 64, 256)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = SimpleNN(input_dim=FEATURE_DIM, output_dim=OUTPUT_DIM).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "    sharpe_ratios = []\n",
    "\n",
    "    for train_idx, test_idx in cv:\n",
    "        train_data = dataset.iloc[train_idx]\n",
    "        test_data = dataset.iloc[test_idx]\n",
    "\n",
    "        era2data_train = get_era2data(train_data)\n",
    "        era2data_validation = get_era2data(test_data)\n",
    "\n",
    "        model, best_corr, outputs = train_model(model, criterion, optimizer, scheduler,\n",
    "                                                NUM_EPOCHS, PATIENCE, era2data_train, \n",
    "                                                era2data_validation, is_lr_scheduler=True)\n",
    "\n",
    "        metrics = metrics_on_batch(best_corr)\n",
    "        sharpe_ratios.append(metrics['sharpe_ratio'])\n",
    "\n",
    "    return -np.mean(sharpe_ratios)\n",
    "\n",
    "def callback(study, trial):\n",
    "    print(f\"\\n--- Trial {trial.number + 1} finished ---\")\n",
    "    print(f\"Value: {trial.value} and parameters: {trial.params}\")\n",
    "    \n",
    "    # Ensure there is at least one successful trial before trying to get the best trial\n",
    "    if len(study.trials_dataframe(attrs=(\"state\",))) > 0 and any(study.trials_dataframe(attrs=(\"state\",))[\"state\"] == \"COMPLETE\"):\n",
    "        print(f\"Best is trial {study.best_trial.number} with value: {study.best_trial.value}\\n\")\n",
    "    else:\n",
    "        print(\"No successful trials yet.\\n\")\n",
    "    \n",
    "    if study.best_trial and study.best_trial.number == trial.number:\n",
    "        best_model = trial.user_attrs.get(\"model\", None)\n",
    "        \n",
    "        # Constructing filename dynamically\n",
    "        top = trial.params[\"hidden_dim_1\"]\n",
    "        model_name = \"SimpleNN\"  # assuming you're using SimpleNN for now\n",
    "        lookahead = trial.params[\"lookahead\"]\n",
    "        filename = f\"{top}_{model_name}_TARGET_ret_fwd_{lookahead}d_rank_quantiled.pkl\"\n",
    "        file_path = os.path.join(MODEL_DIR, filename)\n",
    "        \n",
    "        # Saving model state dict\n",
    "        if best_model is not None:\n",
    "            torch.save(best_model.state_dict(), file_path)\n",
    "        # Save parameters if needed\n",
    "        with open(os.path.join(MODEL_DIR, f\"{filename}_params.json\"), 'w') as f:\n",
    "            json.dump(trial.params, f)\n",
    "\n",
    "study = optuna.create_study(study_name='Maximizing the Sharpe', direction='minimize',\n",
    "                            storage=f'sqlite:///{study_dir}/study.db', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=25, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
