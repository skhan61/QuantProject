{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "top = 250  # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection \\\n",
    "        import mutual_info_regression\n",
    "        \n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 250 dataset\n",
    "\n",
    "DATA_STORE = Path(f'/home/sayem/Desktop/Project/data/{top}_dataset.h5')\n",
    "\n",
    "lock_path = \"/tmp/assets_h5_file.lock\"  # Choose a path for the lock file\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "with FileLock(lock_path):\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        result = store['/data/YEAR_20220906_20230811']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from utils import optimize_dataframe\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_correlation(data1, data2):\n",
    "    n = len(data1)\n",
    "    mean_x = np.mean(data1)\n",
    "    mean_y = np.mean(data2)\n",
    "    \n",
    "    num = np.sum((data1 - mean_x) * (data2 - mean_y))\n",
    "    den = np.sqrt(np.sum((data1 - mean_x)**2) * np.sum((data2 - mean_y)**2))\n",
    "    \n",
    "    if den == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return num / den\n",
    "\n",
    "def calculate_ic(dataframe, target_column, target_ranked, n_jobs=-1):\n",
    "    # Exclude the target column from the feature list\n",
    "    features = [col for col in dataframe.columns.tolist() if col != target_column]\n",
    "    correlations = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_correlation)(dataframe[column].values, target_ranked) for column in features\n",
    "    )\n",
    "    ic_original = pd.Series(dict(zip(features, correlations))).sort_values(ascending=False)\n",
    "    return ic_original\n",
    "\n",
    "def calculate_ic_batched(dataframe, target_column, batch_size=50, corr_threshold=0.5, n_jobs=-1):\n",
    "    df_ranked = dataframe.rank()\n",
    "    target_ranked = df_ranked[target_column].values\n",
    "    columns = [col for col in dataframe.columns.tolist() if col != target_column]\n",
    "    \n",
    "\n",
    "    ic_aggregated = pd.Series(dtype=float)\n",
    "\n",
    "    num_batches = len(columns) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        print(f\"Processing batch {i+1}/{num_batches}...\")\n",
    "        start_col = i * batch_size\n",
    "        end_col = start_col + batch_size\n",
    "\n",
    "        subset_cols = columns[start_col:end_col]\n",
    "        subset = df_ranked[subset_cols]\n",
    "        # ic_original = calculate_ic(subset, target_ranked, n_jobs=n_jobs)\n",
    "        ic_original = calculate_ic(subset, target_column, target_ranked, n_jobs=n_jobs)\n",
    "        ic_aggregated = ic_aggregated.add(ic_original, fill_value=0)\n",
    "\n",
    "    ic_aggregated = ic_aggregated.sort_values(ascending=False)\n",
    "\n",
    "    correlation_matrix = df_ranked[ic_aggregated.index].corr()\n",
    "    dropped_features = set()\n",
    "    for col in ic_aggregated.index:\n",
    "        if col not in dropped_features:\n",
    "            correlated_features = correlation_matrix[col][(correlation_matrix[col].abs() \\\n",
    "                > corr_threshold) & (correlation_matrix[col].index != col)].index\n",
    "            for feature in correlated_features:\n",
    "                if ic_aggregated[col] < ic_aggregated[feature]:\n",
    "                    dropped_features.add(col)\n",
    "                else:\n",
    "                    dropped_features.add(feature)\n",
    "\n",
    "    ic_reduced = ic_aggregated.drop(labels=dropped_features)\n",
    "    columns_to_include = ic_reduced.index.tolist() + [target_column, 'FEATURE_open', 'FEATURE_high', \\\n",
    "        'FEATURE_low', 'FEATURE_volume', 'FEATURE_close']\n",
    "    reduced_dataframe = dataframe[columns_to_include]\n",
    "\n",
    "    selected_correlation_matrix = \\\n",
    "        correlation_matrix.loc[ic_reduced.index, ic_reduced.index]\n",
    "    \n",
    "    return reduced_dataframe, ic_reduced, selected_correlation_matrix\n",
    "\n",
    "\n",
    "    # return reduced_dataframe\n",
    "\n",
    "# Optimize memory and clean dataframe\n",
    "data = optimize_dataframe(result)\n",
    "\n",
    "# Drop duplicated rows from the data\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop duplicated columns\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "del result\n",
    "\n",
    "TARGET = 'TARGET_ret_fwd_frac_order' # RET_FWD_FRAC_ORDER'\n",
    "reduced_dataframe, \\\n",
    "    selected_ics, \\\n",
    "    selected_corr_matrix = calculate_ic_batched(data, TARGET, batch_size=100)\n",
    "\n",
    "# Find duplicate columns\n",
    "duplicated_cols = reduced_dataframe.columns[reduced_dataframe.columns.duplicated()].to_list()\n",
    "\n",
    "# Drop duplicate columns\n",
    "reduced_dataframe = reduced_dataframe.loc[:, ~reduced_dataframe.columns.duplicated()]\n",
    "\n",
    "print(f\"Removed duplicate columns: {duplicated_cols}\")\n",
    "print(reduced_dataframe.shape)\n",
    "\n",
    "del data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_to_hdf\n",
    "\n",
    "FILE_PATH = f\"/home/sayem/Desktop/Project/data/{top}_dataset.h5\"\n",
    "# Define key name\n",
    "\n",
    "FILE_NAME_PREFIX = f'/data/ic_based_reduced_features_YEAR'\n",
    "\n",
    "key = save_to_hdf(reduced_dataframe, FILE_PATH, FILE_NAME_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"File key: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming selected_corr_matrix is the variable containing your correlation matrix\n",
    "# selected_corr_matrix = your_correlation_matrix_here\n",
    "\n",
    "# Define the color map\n",
    "cmap = sns.diverging_palette(10, 220, as_cmap=True)\n",
    "\n",
    "# Create the clustermap\n",
    "g = sns.clustermap(selected_corr_matrix, cmap=cmap, figsize=(30, 30))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the Series for better visualization\n",
    "sorted_ics = selected_ics.sort_values()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 20))  # Adjust the size as needed\n",
    "\n",
    "sns.barplot(x=sorted_ics, y=sorted_ics.index, palette=\"coolwarm\")\n",
    "\n",
    "plt.xlabel('Information Coefficient (IC)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Information Coefficient (IC) of Selected Features')\n",
    "\n",
    "# Annotate each bar with the actual IC value\n",
    "for index, value in enumerate(sorted_ics):\n",
    "    plt.text(value, index, f'{value:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_time_series_cv_splits(dataframe, train_period_length=21*3, \\\n",
    "#     test_period_length=21, lookahead=1, date_idx='date'):\n",
    "#     \"\"\"\n",
    "#     Plots the train and validation windows for time series cross-validation.\n",
    "    \n",
    "#     Parameters:\n",
    "#         dataframe: The data to be split.\n",
    "#         train_period_length: The number of business days in the training set.\n",
    "#         test_period_length: The number of business days in the validation set.\n",
    "#         lookahead: The gap between training and validation sets.\n",
    "#         date_idx: The name of the date index in the dataframe.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     cv_splits = CustomBackwardMultipleTimeSeriesCV(dataframe, \\\n",
    "#         train_period_length, test_period_length, lookahead, date_idx)\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    \n",
    "#     for i, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "#         # Get the date ranges\n",
    "#         train_dates = dataframe.index.get_level_values(date_idx)[train_idx]\n",
    "#         test_dates = dataframe.index.get_level_values(date_idx)[test_idx]\n",
    "\n",
    "#         ax.plot([train_dates[0], train_dates[-1]], [i, i], color='blue', label='Training' if i == 0 else \"\")\n",
    "#         ax.plot([test_dates[0], test_dates[-1]], [i, i], color='red', label='Validation' if i == 0 else \"\")\n",
    "#         ax.scatter([train_dates[0], train_dates[-1]], [i, i], color='blue')\n",
    "#         ax.scatter([test_dates[0], test_dates[-1]], [i, i], color='red')\n",
    "        \n",
    "#         # Draw vertical dashed lines to represent the lookahead gap for every split\n",
    "#         gap_start = train_dates[-1]\n",
    "#         gap_end = test_dates[0]\n",
    "#         ax.axvline(x=gap_start, color='green', linestyle='--', alpha=0.6)\n",
    "#         ax.axvline(x=gap_end, color='green', linestyle='--', alpha=0.6)\n",
    "\n",
    "#         # Shade the lookahead gap\n",
    "#         ax.axvspan(gap_start, gap_end, facecolor='gray', alpha=0.2)\n",
    "        \n",
    "#     ax.set_xlabel('Date')\n",
    "#     ax.set_ylabel('Iteration')\n",
    "#     ax.set_title('Backward-Rolling Time Series CV Splits with Lookahead Gap')\n",
    "#     ax.legend(loc='best')\n",
    "#     plt.gca().invert_yaxis()  # invert y-axis to have the most recent data at the top\n",
    "#     plt.show()\n",
    "\n",
    "# # Now call the function to plot\n",
    "# plot_time_series_cv_splits(reduced_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
