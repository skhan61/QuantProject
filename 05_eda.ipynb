{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection \\\n",
    "        import mutual_info_regression\n",
    "        \n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define file path\n",
    "FILE_PATH = \"/home/sayem/Desktop/Project/data/dataset.h5\"\n",
    "\n",
    "# FILE_NAME = '/data/reduced_2022-07-21_to_2022-07-29'\n",
    "\n",
    "FILE_NAME = '/data/2020-01-02_to_2022-12-30'\n",
    "\n",
    "# Read the dataset using Dask\n",
    "data = dd.read_hdf(FILE_PATH, FILE_NAME)\n",
    "\n",
    "# Compute the result (this will load data into memory)\n",
    "result = data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 4191516 entries, ('AA', Timestamp('2020-01-02 00:00:00')) to ('ZTS', Timestamp('2022-12-30 00:00:00'))\n",
      "Columns: 624 entries, OPEN to ALPHA_101\n",
      "dtypes: float32(384), float64(33), int32(198), int8(9)\n",
      "memory usage: 10.2+ GB\n"
     ]
    }
   ],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from joblib import Parallel, delayed\n",
    "# from utils import optimize_dataframe\n",
    "\n",
    "# def compute_correlation(data1, data2):\n",
    "#     if np.std(data1) == 0 or np.std(data2) == 0:\n",
    "#         return np.nan\n",
    "#     return np.corrcoef(data1, data2)[0, 1]\n",
    "\n",
    "# def calculate_ic(dataframe, target_ranked, n_jobs=-1):\n",
    "#     features = dataframe.columns.tolist()\n",
    "#     correlations = Parallel(n_jobs=n_jobs)(\n",
    "#         delayed(compute_correlation)(dataframe[column].values, target_ranked) for column in features\n",
    "#     )\n",
    "#     ic_original = pd.Series(dict(zip(features, correlations))).sort_values(ascending=False)\n",
    "#     return ic_original\n",
    "\n",
    "# def calculate_ic_batched(dataframe, target_column, batch_size=50, corr_threshold=0.5, n_jobs=-1):\n",
    "#     df_ranked = dataframe.rank()\n",
    "#     target_ranked = df_ranked[target_column].values\n",
    "#     columns = dataframe.columns.tolist()\n",
    "\n",
    "#     ic_aggregated = pd.Series(dtype=float)\n",
    "\n",
    "#     num_batches = len(columns) // batch_size + 1\n",
    "#     for i in range(num_batches):\n",
    "#         print(f\"Processing batch {i+1}/{num_batches}...\")\n",
    "#         start_col = i * batch_size\n",
    "#         end_col = start_col + batch_size\n",
    "\n",
    "#         subset_cols = columns[start_col:end_col]\n",
    "#         subset = df_ranked[subset_cols]\n",
    "#         ic_original = calculate_ic(subset, target_ranked, n_jobs=n_jobs)\n",
    "#         ic_aggregated = ic_aggregated.add(ic_original, fill_value=0)\n",
    "\n",
    "#     ic_aggregated = ic_aggregated.sort_values(ascending=False)\n",
    "\n",
    "#     correlation_matrix = df_ranked[ic_aggregated.index].corr()\n",
    "#     dropped_features = set()\n",
    "#     for col in ic_aggregated.index:\n",
    "#         if col not in dropped_features:\n",
    "#             correlated_features = correlation_matrix[col][(correlation_matrix[col].abs() \\\n",
    "#                 > corr_threshold) & (correlation_matrix[col].index != col)].index\n",
    "#             for feature in correlated_features:\n",
    "#                 if ic_aggregated[col] < ic_aggregated[feature]:\n",
    "#                     dropped_features.add(col)\n",
    "#                 else:\n",
    "#                     dropped_features.add(feature)\n",
    "\n",
    "#     ic_reduced = ic_aggregated.drop(labels=dropped_features)\n",
    "#     columns_to_include = ic_reduced.index.tolist() + [target_column, \\\n",
    "#         'OPEN', 'HIGH', 'LOW', 'VOLUME', 'CLOSE']\n",
    "#     reduced_dataframe = dataframe[columns_to_include]\n",
    "\n",
    "#     return reduced_dataframe\n",
    "\n",
    "# # Optimize memory and clean dataframe\n",
    "# data = optimize_dataframe(result)\n",
    "\n",
    "# # Drop duplicated rows from the data\n",
    "# data.drop_duplicates(inplace=True)\n",
    "\n",
    "# # Drop duplicated columns\n",
    "# data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "# del result\n",
    "\n",
    "# TARGET = 'RET_FWD_FRAC_ORDER'\n",
    "# reduced_dataframe = calculate_ic_batched(data, TARGET)\n",
    "\n",
    "# # Find duplicate columns\n",
    "# duplicated_cols = reduced_dataframe.columns[reduced_dataframe.columns.duplicated()].to_list()\n",
    "\n",
    "# # Drop duplicate columns\n",
    "# reduced_dataframe = reduced_dataframe.loc[:, ~reduced_dataframe.columns.duplicated()]\n",
    "\n",
    "# print(f\"Removed duplicate columns: {duplicated_cols}\")\n",
    "\n",
    "# print(reduced_dataframe.shape)\n",
    "\n",
    "# del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data memory before optimization: 10413.14 MB\n",
      "Data memory after optimization: 10317.20 MB\n",
      "Reduced by: 0.92%\n",
      "Processing batch 1/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "/tmp/ipykernel_23275/917238105.py:12: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/7...\n",
      "Processing batch 3/7...\n",
      "Processing batch 4/7...\n",
      "Processing batch 5/7...\n",
      "Processing batch 6/7...\n",
      "Processing batch 7/7...\n",
      "Removed duplicate columns: ['RET_FWD_FRAC_ORDER', 'VOLUME']\n",
      "(4191516, 128)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from utils import optimize_dataframe\n",
    "from numba import jit\n",
    "\n",
    "# Using Numba's JIT compiler to speed up the compute_correlation function\n",
    "@jit(nopython=True)\n",
    "def compute_correlation(data1, data2):\n",
    "    if np.std(data1) == 0 or np.std(data2) == 0:\n",
    "        return np.nan\n",
    "    return np.corrcoef(data1, data2)[0, 1]\n",
    "\n",
    "def calculate_ic(dataframe, target_ranked, n_jobs=-1):\n",
    "    features = dataframe.columns.tolist()\n",
    "    correlations = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_correlation)(dataframe[column].values, \\\n",
    "            target_ranked) for column in features\n",
    "    )\n",
    "    ic_original = pd.Series(dict(zip(features, correlations))).sort_values(ascending=False)\n",
    "    return ic_original\n",
    "\n",
    "def calculate_ic_batched(dataframe, target_column, batch_size=50, \\\n",
    "    corr_threshold=0.5, n_jobs=-1):\n",
    "    df_ranked = dataframe.rank()\n",
    "    target_ranked = df_ranked[target_column].values\n",
    "    columns = dataframe.columns.tolist()\n",
    "\n",
    "    ic_aggregated = pd.Series(dtype=float)\n",
    "\n",
    "    num_batches = len(columns) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        print(f\"Processing batch {i+1}/{num_batches}...\")\n",
    "        start_col = i * batch_size\n",
    "        end_col = start_col + batch_size\n",
    "\n",
    "        subset_cols = columns[start_col:end_col]\n",
    "        subset = df_ranked[subset_cols]\n",
    "        ic_original = calculate_ic(subset, target_ranked, n_jobs=n_jobs)\n",
    "        ic_aggregated = ic_aggregated.add(ic_original, fill_value=0)\n",
    "\n",
    "    ic_aggregated = ic_aggregated.sort_values(ascending=False)\n",
    "\n",
    "    correlation_matrix = df_ranked[ic_aggregated.index].corr()\n",
    "    dropped_features = set()\n",
    "    for col in ic_aggregated.index:\n",
    "        if col not in dropped_features:\n",
    "            correlated_features = correlation_matrix[col][(correlation_matrix[col].abs() \\\n",
    "                > corr_threshold) & (correlation_matrix[col].index != col)].index\n",
    "            for feature in correlated_features:\n",
    "                if ic_aggregated[col] < ic_aggregated[feature]:\n",
    "                    dropped_features.add(col)\n",
    "                else:\n",
    "                    dropped_features.add(feature)\n",
    "\n",
    "    ic_reduced = ic_aggregated.drop(labels=dropped_features)\n",
    "    columns_to_include = ic_reduced.index.tolist() + [target_column, \\\n",
    "        'OPEN', 'HIGH', 'LOW', 'VOLUME', 'CLOSE']\n",
    "    reduced_dataframe = dataframe[columns_to_include]\n",
    "\n",
    "    return reduced_dataframe\n",
    "\n",
    "# Optimize memory and clean dataframe\n",
    "data = optimize_dataframe(result)\n",
    "\n",
    "# Drop duplicated rows from the data\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop duplicated columns\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "del result\n",
    "\n",
    "TARGET = 'RET_FWD_FRAC_ORDER'\n",
    "reduced_dataframe = calculate_ic_batched(data, TARGET, batch_size=100)  # Increased batch size for potential speedup\n",
    "\n",
    "# Find duplicate columns\n",
    "duplicated_cols = reduced_dataframe.columns[reduced_dataframe.columns.duplicated()].to_list()\n",
    "\n",
    "# Drop duplicate columns\n",
    "reduced_dataframe = reduced_dataframe.loc[:, ~reduced_dataframe.columns.duplicated()]\n",
    "\n",
    "print(f\"Removed duplicate columns: {duplicated_cols}\")\n",
    "print(reduced_dataframe.shape)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sayem/Desktop/Project/05_eda.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cv_splits \u001b[39m=\u001b[39m CustomBackwardMultipleTimeSeriesCV(reduced_dataframe, \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         train_period_length\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, test_period_length\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, lookahead\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_idx, test_idx \u001b[39min\u001b[39;00m cv_splits:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Here you can use train_idx and test_idx to get the training and validation data respectively\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_data \u001b[39m=\u001b[39m reduced_data[train_idx]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(train_data\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# val_data = reduced_dataframe[test_idx]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sayem/Desktop/Project/05_eda.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# # Your training and validation code goes here...\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced_data' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import CustomBackwardMultipleTimeSeriesCV\n",
    "\n",
    "cv_splits = CustomBackwardMultipleTimeSeriesCV(reduced_dataframe, \\\n",
    "        train_period_length=5, test_period_length=3, lookahead=1)\n",
    "\n",
    "for train_idx, test_idx in cv_splits:\n",
    "    # Here you can use train_idx and test_idx to get the training and validation data respectively\n",
    "    train_data = reduced_data[train_idx]\n",
    "    print(train_data.shape)\n",
    "    # val_data = reduced_dataframe[test_idx]\n",
    "    # # Your training and validation code goes here...\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_series_cv_splits(dataframe, train_period_length=142, \\\n",
    "    test_period_length=21, lookahead=10, date_idx='date'):\n",
    "    \"\"\"\n",
    "    Plots the train and validation windows for time series cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: The data to be split.\n",
    "        train_period_length: The number of business days in the training set.\n",
    "        test_period_length: The number of business days in the validation set.\n",
    "        lookahead: The gap between training and validation sets.\n",
    "        date_idx: The name of the date index in the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    cv_splits = CustomBackwardMultipleTimeSeriesCV(dataframe, \\\n",
    "        train_period_length, test_period_length, lookahead, date_idx)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(cv_splits):\n",
    "        # Get the date ranges\n",
    "        train_dates = dataframe.index.get_level_values(date_idx)[train_idx]\n",
    "        test_dates = dataframe.index.get_level_values(date_idx)[test_idx]\n",
    "\n",
    "        ax.plot([train_dates[0], train_dates[-1]], [i, i], color='blue', label='Training' if i == 0 else \"\")\n",
    "        ax.plot([test_dates[0], test_dates[-1]], [i, i], color='red', label='Validation' if i == 0 else \"\")\n",
    "        ax.scatter([train_dates[0], train_dates[-1]], [i, i], color='blue')\n",
    "        ax.scatter([test_dates[0], test_dates[-1]], [i, i], color='red')\n",
    "        \n",
    "        # Draw vertical dashed lines to represent the lookahead gap for every split\n",
    "        gap_start = train_dates[-1]\n",
    "        gap_end = test_dates[0]\n",
    "        ax.axvline(x=gap_start, color='green', linestyle='--', alpha=0.6)\n",
    "        ax.axvline(x=gap_end, color='green', linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Shade the lookahead gap\n",
    "        ax.axvspan(gap_start, gap_end, facecolor='gray', alpha=0.2)\n",
    "        \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Iteration')\n",
    "    ax.set_title('Backward-Rolling Time Series CV Splits with Lookahead Gap')\n",
    "    ax.legend(loc='best')\n",
    "    plt.gca().invert_yaxis()  # invert y-axis to have the most recent data at the top\n",
    "    plt.show()\n",
    "\n",
    "# Now call the function to plot\n",
    "plot_time_series_cv_splits(reduced_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
